[{"path":"index.html","id":"preamble","chapter":"1 Preamble","heading":"1 Preamble","text":"-progress version Data Management Large-Scale Education Research. completed, book published CRC Press. see previous version material, please visit website.results educational research studies accurate data used produce . - Aleata Hubbard (2017)","code":""},{"path":"index.html","id":"introduction","chapter":"1 Preamble","heading":"1.1 Introduction","text":"2013, without knowing term research data management existed, accepted position prevention science research center. job coordinate collection management data federally funded randomized controlled trial efficacy studies taking place K-12 schools, along team investigators, research staff, part-time data collectors, graduate students. experience analyzing working education data, .e. ECLS-K, experience running research grants, collecting original data, managing research data, excited learn.time position learned plan, schedule, track data collection activities, create data capture tools, organize document data inputs, produce usable data outputs; didn’t learn things formal training. books, courses, workshops learned . learned colleagues large amount trial error. Since , met investigators, data managers, project coordinators education research, realize common method learning data management—mentoring “winging ”. learning data management informal methods helps us get , ramifications unstandardized system felt project team future data users.","code":""},{"path":"index.html","id":"why-this-book","chapter":"1 Preamble","heading":"1.2 Why this book","text":"Research data management becoming complicated. collecting data, sometimes novel ways, using complex technologies, increasing visibility work push data sharing open science practices (Briney 2015; Nelson 2022). Ad hoc data management practices may worked us past, now others need understand processes well, requiring researchers thoughtful planning data management routines.","code":""},{"path":"index.html","id":"lack-of-training-resources-and-standards","chapter":"1 Preamble","heading":"1.2.1 Lack of training, resources, and standards","text":"order implement thoughtful standardized data management practices, researchers need training. Yet clear lack data management training higher education. survey 274 psychology researchers, Borghi Van Gulick (2021) found 33% respondents learned data management college level coursework, 64% learned collaborators, 52% learned self-education. survey 202 education researchers (PIs Co-PIs), Ceviren Logan (2022) found 60% respondents reported formal training data management, yet across eight different data management practices, respondents responsible data management activities anywhere 25-50% time. Similarly, survey 150 graduate students school education, asked needed training research data management, average overall score scale 1 100 80, overall confidence managing data score 40 (Zhou, Xu, Kogut 2023). Furthermore, training exist, usually provided university library systems, material either discipline agnostic STEM focused, leaving gap training apply skills field education unique issues, particularly around working human subjects data (Nichols Hess Thielen 2017).Without training, resources formal support systems next best option learning best practices. Within university systems, addition providing periodic training, research data librarians provide data management planning consultation researchers teams. also wealth existing research data management books manuals written broad audiences link book. However, education researchers starting put excellent resources (Neild, Robinson, Agufa 2022; Reynolds, Schatschneider, Logan 2022), still find dearth practical guides researchers refer building data management workflow field education, especially working large-scale longitudinal research grants many moving pieces. Researchers often collecting data real-world environments, school systems, keeping data secure reliable deliberate orderly way can overwhelming.Last, unfortunately, fields research, psychology, appear banding together develop standards around structure document data (Kline 2018), field education yet develop agreed upon rules things data documentation data formats. lack standards leads inconsistencies quality usability data products across field (Borghi Van Gulick 2022).","code":""},{"path":"index.html","id":"consequences","chapter":"1 Preamble","heading":"1.2.2 Consequences","text":"lack training data management practices absence agreed upon standards field education leads consequences. Implementing subpar inconsistent data management practices, typically resulting frustration time lost, also potential devastating, resulting analyzing erroneous data even unusable lost data. review 1,082 retracted publications journal PubMed 2013-2016, authors found 32% retractions due data management errors (Campos-Varela Ruano-Raviña 2019). 2013 study surveying 360 graduate students data management practices, 14% students indicated recollect data previously collected find file file corrupted, 17% students said lost file unable recollect (Doucette Fyfe 2013). study 488 researchers published psychology journal 2010 2018, Kovacs, et al. (2021) asked respondents data management mistakes found serious data management mistakes reported led range consequences including time loss, frustration, even erroneous conclusions.Poor data management can even prevent researchers implementing good open science practices. waves 1 2 Open Scholarship Survey collected Center Open Science, team found education researchers surveyed currently publicly sharing research data, approximately 15% mentioned “nervous mistakes” reason sharing (Beaudry et al. 2022). Similarly, surveying 780 researchers field psychology, researchers found 38% respondents agreed “fear discovery errors data” posed barrier data sharing (Houtkoop et al. 2018).well-known replication crisis another reason concerned data management. Failure implement practices quality documentation standardization practices (among many reasons), resulted one study finding across 1,500 researchers surveyed, 70% tried failed reproduce another researcher’s study (Baker 2016).","code":""},{"path":"index.html","id":"about-this-book","chapter":"1 Preamble","heading":"1.3 About this book","text":"field whole may agreed upon guidelines data management, still practices proven result secure, reproducible, reliable data. hope book can foundation help researchers think build quality, standardized data management workflow works team projects. suggested title book, content designed specifically help teams navigate complicated workflows associated large-scale research, randomized controlled trial studies, ultimately practices applicable research project, matter scale.book viewed handbook referenced regularly necessarily meant read entirety one sitting. perusing entire book better understand entire research data life cycle helpful, book also intended chapters referenced needed ready start planning specific phase project.","code":""},{"path":"index.html","id":"what-this-book-will-cover","chapter":"1 Preamble","heading":"1.3.1 What this book will cover","text":"book begins, like many books subject area, describing research life cycle data management fits within larger picture. remaining chapters organized phase life cycle, examples best practices provided phase. Considerations whether implement, integrate practices workflow discussed.","code":""},{"path":"index.html","id":"what-this-book-will-not-cover","chapter":"1 Preamble","heading":"1.3.2 What this book will not cover","text":"important also point book cover. book intended tool agnostic provide suggestions anyone can use, matter tools work , especially comes data cleaning. Therefore, might mention options tools can use different tasks, advocate specific tools.also specific coding practices actual syntax included book. honest, many ways feel actual “data cleaning” phase data management easiest phase implement, long implement good practices point. , book introduces practices phases leading data cleaning prepare data minimal cleaning. said, provide examples expect see data cleaning process, just provide steps specific software system. beyond scope book.book also talk analysis preparing data analysis means data imputation, removal legitimate outliers, calculating analysis specific variables. Written perspective data manager, end goal data management build datasets general data sharing. means cover practices keep data complete true, usable form, future researcher analyze way works best .","code":""},{"path":"index.html","id":"who-this-book-is-for","chapter":"1 Preamble","heading":"1.4 Who this book is for","text":"book anyone involved research study involving original data collection. particular, book focuses quantitative, observational data collection, although think many practices covered can also apply qualitative data collection well. book also applies team member, ranging PIs, data managers, project staff, students, contractual data collectors. contents book useful anyone may part planning, collecting, organizing research study data.","code":""},{"path":"index.html","id":"final-note","chapter":"1 Preamble","heading":"1.5 Final note","text":"Planning implementing new data management practices top planning implementation entire research grant can feel overwhelming. However, idea book find practices work team implement consistently. teams may look like implementing just suggestions mentioned others may involve implementing suggestions. Improving data management workflow process becomes easier time practices become part normal routine. point may even find enjoy working data management processes start see benefits implementation!","code":""},{"path":"index.html","id":"acknowledgements","chapter":"1 Preamble","heading":"1.6 Acknowledgements","text":"book compilation lessons learned personal experiences data manager, knowledge collected existing books papers (many written librarians involved open science movement), well advice stories collected interviews researchers work data. want clear formally study research data management, unlike research data librarians experts content. Much book based lessons learned firsthand experience book attempt hopefully save others making mistakes personally made seen others make. can emphasize enough work university opportunity consult librarian project, absolutely !said, long list people like acknowledge contributions book supporting process.many people graciously allowed interview current data management practices. Mary McCraken, Ryan Estrellado, Kim Manturuk, Beth Chance, Jessica Logan, Rebecca Schmidt, Sara Hart, Kerry Shea. interviews integral supplementing personal knowledge broader experience others field. Yet, affirmed yes, data management hard, especially context complicated study designs work education research, everyone works field wishes better training, support systems, standards existed. Thank everyone gave hour time share experiences knowledge! also give special thank Jessica Logan first person met appreciates things data management much , since interview, provided invaluable support working book.also want thank everyone took time read provide feedback chapters book . includes Meghan Harris, Alexis Swanz, Allyson Hanson. revisions insight helped make cohesive useful book!special thank Keith Herman well. Many years ago suggested write book titled Data Management Large-Scale Education Research, based everything ’ve learned experience data manager. time considered suggestion fun impossible idea. Yet sitting idea back head several years, realized idea actually far-fetched. Thank Keith believing something didn’t even know possible.Much appreciation Wendy Reinke well. Although may know , first person learned research data management practices . Joining project already created documentation tracking systems first glimpse building tools help manage data love research data management grew experience.want say thank POWER Data Management Issues Education Research Hub. Regularly meeting group data managers, researchers, students, professors last two years amazing source support learning greatly increased understanding data management.Last, thank Josh fully supporting decision write book Fox reason remember step away computer time time fun.","code":""},{"path":"rdm.html","id":"rdm","chapter":"2 Research Data Management Overview","heading":"2 Research Data Management Overview","text":"","code":""},{"path":"rdm.html","id":"what-is-research-data-management","chapter":"2 Research Data Management Overview","heading":"2.1 What is research data management?","text":"Research data management (RDM) involves organization, storage, preservation, dissemination research study data (Bordelon 2023). Research study data includes materials generated collected throughout research process (National Endowment Humanities 2018). can imagine, broad definition includes much just management digital datasets. also includes physical files, documentation, artifacts, recordings, . RDM substantial undertaking begins long data ever collected, planning phase, continues well research project ends archiving phase.","code":""},{"path":"rdm.html","id":"standards","chapter":"2 Research Data Management Overview","heading":"2.2 Standards","text":"Data management standards refer rules data collected, formatted, described, shared (Borghi Van Gulick 2022; Koos 2023). Implementing standards things variables collected named, items common measures shared, data formatted documented, leads findable usable data within fields provides added benefit allowing researchers integrate datasets without painstaking work normalize data.fields adopted standards across research life cycle, CDISC standards used clinical researchers (CDISC 2023), fields adopted standards specifically around metadata, TEI standards used digital humanities (Burnard 2014) ISO 19115 standard used geospatial data (Michener 2015), grassroots efforts, fields psychology developing standards things data formatting documentation (Kline 2018) based FAIR principles inspired BIDS standard (BIDS-Contributors 2022). Yet, common knowledge currently agreed-upon norms field education research (Institute Education Sciences n.d.; Logan Hart 2023). rules collect, format, document data often left individual team, long external compliance requirements met (Tenopir et al. 2016). However, growing interest open science practices expanding requirements federally funded research make data publicly available (Holdren 2013), data repositories likely begin play stronger role promoting standards around data formats documentation (Borghi Van Gulick 2022).","code":""},{"path":"rdm.html","id":"why-care-about-research-data-management","chapter":"2 Research Data Management Overview","heading":"2.3 Why care about research data management?","text":"Without current agreed-upon standards field, important research teams develop data management standards apply within across projects. Developing internal standards, implemented reproducible data management workflow, allows practices implemented consistently fidelity. external pressures personal reasons care developing research data management standards projects.","code":""},{"path":"rdm.html","id":"external-reasons","chapter":"2 Research Data Management Overview","heading":"2.3.1 External Reasons","text":"Funder compliance: researcher applying federal funding required submit data management plan (see Chapter 4) along grant proposal (Holdren 2013; Nelson 2022). contents plans may vary slightly across agencies shared purpose documents facilitate good data management practices mandate open sharing data maximize scientific outputs benefits society. Along mandatory data sharing policy, comes incentive manage data purposes data sharing (Borghi Van Gulick 2022).Funder compliance: researcher applying federal funding required submit data management plan (see Chapter 4) along grant proposal (Holdren 2013; Nelson 2022). contents plans may vary slightly across agencies shared purpose documents facilitate good data management practices mandate open sharing data maximize scientific outputs benefits society. Along mandatory data sharing policy, comes incentive manage data purposes data sharing (Borghi Van Gulick 2022).Journal compliance: Depending journal publish , providing open access data associated publication may requirement (see PLOS ONE (https://journals.plos.org/plosone/) AMPPS (https://www.psychologicalscience.org/publications/ampps) examples). , along data sharing, comes incentive manage data thoughtful, responsible, organized way.Journal compliance: Depending journal publish , providing open access data associated publication may requirement (see PLOS ONE (https://journals.plos.org/plosone/) AMPPS (https://www.psychologicalscience.org/publications/ampps) examples). , along data sharing, comes incentive manage data thoughtful, responsible, organized way.Compliance mandates: required submit research project Institutional Review Board (see Chapter 10), board review monitor data management practices. Concerned welfare, rights, privacy research participants, IRB rules data managed stored securely (Filip 2023). Data sharing legal agreements research partners also need monitored honored. Additionally, organization may institutional data policies mandate data must cared secured.Compliance mandates: required submit research project Institutional Review Board (see Chapter 10), board review monitor data management practices. Concerned welfare, rights, privacy research participants, IRB rules data managed stored securely (Filip 2023). Data sharing legal agreements research partners also need monitored honored. Additionally, organization may institutional data policies mandate data must cared secured.Open science practices: growing interest open science practices, sharing well-managed documented data helps build trust research process (Renbarger et al. 2022). Sharing data curated reproducible way “strong indicator fellow researchers rigor, trustworthiness, transparency scientific research” (Alston Rick 2021, 2). also allows others replicate learn work, validate results strengthen evidence, well potentially catch errors work, preventing decisions made based incorrect data. Sharing data sufficient documentation standardized metadata can also lead collaboration greater impact collaborators able access understand data ease (Borghi Van Gulick 2022; Cowles n.d.; Eaker 2016).Open science practices: growing interest open science practices, sharing well-managed documented data helps build trust research process (Renbarger et al. 2022). Sharing data curated reproducible way “strong indicator fellow researchers rigor, trustworthiness, transparency scientific research” (Alston Rick 2021, 2). also allows others replicate learn work, validate results strengthen evidence, well potentially catch errors work, preventing decisions made based incorrect data. Sharing data sufficient documentation standardized metadata can also lead collaboration greater impact collaborators able access understand data ease (Borghi Van Gulick 2022; Cowles n.d.; Eaker 2016).Data management matter ethics: education research often collecting data human participants, result, data management ethical issue. important consider environmental, social, cultural, historical, political context data collecting (Alexander 2023). managing data often making human decisions collect, organize, clean data, process need aware biases may affect equitable representation datasets. instance, word question family relationships, order choose present categories gender, choose collapse categories race data cleaning process just examples ways may potentially bias datasets based personal lenses (Mathematica n.d.). \nprocess de-identifying data data sharing also becomes ethical issue. Protection participant identities always equally distributed across dataset. may easy scrub identifying information majority participants, individuals represent smaller numbers data may still identifiable important consider security implications participants (McKay Bowen Snoke 2023). \nLast, collecting data human participants means people giving time energy entrusting us information. Implementing poor data management leads irrelevant, unusable, lost data huge disservice research participants erodes trust research process. responsibility well-designed research studies quality data management data sharing practices ensure participant data remains secure, usable, true efforts lead maximum societal benefits (Feeney, Kopper, Sautmann 2022).Data management matter ethics: education research often collecting data human participants, result, data management ethical issue. important consider environmental, social, cultural, historical, political context data collecting (Alexander 2023). managing data often making human decisions collect, organize, clean data, process need aware biases may affect equitable representation datasets. instance, word question family relationships, order choose present categories gender, choose collapse categories race data cleaning process just examples ways may potentially bias datasets based personal lenses (Mathematica n.d.). \nprocess de-identifying data data sharing also becomes ethical issue. Protection participant identities always equally distributed across dataset. may easy scrub identifying information majority participants, individuals represent smaller numbers data may still identifiable important consider security implications participants (McKay Bowen Snoke 2023). \nLast, collecting data human participants means people giving time energy entrusting us information. Implementing poor data management leads irrelevant, unusable, lost data huge disservice research participants erodes trust research process. responsibility well-designed research studies quality data management data sharing practices ensure participant data remains secure, usable, true efforts lead maximum societal benefits (Feeney, Kopper, Sautmann 2022).","code":""},{"path":"rdm.html","id":"personal-reasons","chapter":"2 Research Data Management Overview","heading":"2.3.2 Personal reasons","text":"Even never plan share data outside research group, still many compelling reasons manage data reproducible standardized way.Reduces data curation debt: Taking time plan implement quality data management entire research study reduces data curation debt caused suboptimal data management practices (Butters, Wilson, Burton 2020). poorly collected, managed, documented data may make data unusable, either permanently errors corrected. Decreasing removing debt reduces time, energy, resources spent possibly recollecting data scrambling end study get data acceptable standards.Reduces data curation debt: Taking time plan implement quality data management entire research study reduces data curation debt caused suboptimal data management practices (Butters, Wilson, Burton 2020). poorly collected, managed, documented data may make data unusable, either permanently errors corrected. Decreasing removing debt reduces time, energy, resources spent possibly recollecting data scrambling end study get data acceptable standards.Facilitates use data: Every member research team able find understand project data documentation huge benefit. allows easy use re-use data, hastens efforts like publication process (Markowetz 2015). search around numbers consented participants asking version data use allows team spend time analyzing less time playing detective.Facilitates use data: Every member research team able find understand project data documentation huge benefit. allows easy use re-use data, hastens efforts like publication process (Markowetz 2015). search around numbers consented participants asking version data use allows team spend time analyzing less time playing detective.Encourages validation: Implementing reproducible data management practices encourages allows team internally replicate validate processes ensure outputs accurate.Encourages validation: Implementing reproducible data management practices encourages allows team internally replicate validate processes ensure outputs accurate.Improves continuity: Data management practices documentation ensures fidelity implementation project. includes implementing practices consistently longitudinal project, consistently across sites. also improves project continuity staff turnover. thoroughly documented procedures allows new staff pick right former staff member left implement project fidelity (Borghi Van Gulick 2021; Cowles n.d.). Furthermore, good data management enables continuity handing projects collaborators picking projects long hiatus (Markowetz 2015).Improves continuity: Data management practices documentation ensures fidelity implementation project. includes implementing practices consistently longitudinal project, consistently across sites. also improves project continuity staff turnover. thoroughly documented procedures allows new staff pick right former staff member left implement project fidelity (Borghi Van Gulick 2021; Cowles n.d.). Furthermore, good data management enables continuity handing projects collaborators picking projects long hiatus (Markowetz 2015).Increases efficiency: Documenting automating data management tasks reduces duplication efforts repeating tasks, especially longitudinal studies.Increases efficiency: Documenting automating data management tasks reduces duplication efforts repeating tasks, especially longitudinal studies.Upholds research integrity: Errors come many forms, humans technology(Kovacs, Hoekstra, Aczel 2021; Strand n.d.). ’ve seen evidence papers cited retracted “unreliable data” blog Retraction Watch (https://retractionwatch.com/). Implementing quality control procedures reduces chances errors occurring allows confidence data. Without implementing practices, research findings include extra noise, missing data, erroneous misleading results.Upholds research integrity: Errors come many forms, humans technology(Kovacs, Hoekstra, Aczel 2021; Strand n.d.). ’ve seen evidence papers cited retracted “unreliable data” blog Retraction Watch (https://retractionwatch.com/). Implementing quality control procedures reduces chances errors occurring allows confidence data. Without implementing practices, research findings include extra noise, missing data, erroneous misleading results.Improves data security: Quality data management practices reduce risk lost stolen data, risk data becoming corrupted inaccessible, risk breaking confidentiality agreements.Improves data security: Quality data management practices reduce risk lost stolen data, risk data becoming corrupted inaccessible, risk breaking confidentiality agreements.","code":""},{"path":"rdm.html","id":"existing-frameworks","chapter":"2 Research Data Management Overview","heading":"2.4 Existing Frameworks","text":"Data management live space alone. co-exists frameworks impact data managed important familiar provide foundation build data management structures.","code":""},{"path":"rdm.html","id":"fair","chapter":"2 Research Data Management Overview","heading":"2.4.1 FAIR","text":"2016, FAIR Principles (GO FAIR n.d.) published Scientific Data, outlining four guiding principles scientific data management stewardship. principles created improve support reuse scholarly data, specifically ability machines access read data, foundation digital data publicly shared (Wilkinson et al. 2016). principles :F: FindableAll data findable persistent identifier thorough, searchable metadata. practices aid long-term discovery information provide registered citations.: AccessibleUsers able access data. can mean data available repository request system. minimum, user able access metadata, even actual data available.: InteroperableYour data metadata use standardized vocabularies well formats. humans machines able read interpret data. Software licenses pose barrier usage. Data available open formats can accessed software (e.g., CSV, TXT, DAT).R: ReusableIn order provide context reuse data, metadata give insight data provenance, providing project description, overview data workflow, well authors cite appropriate attribution. also clear licensing data use.","code":""},{"path":"rdm.html","id":"seer","chapter":"2 Research Data Management Overview","heading":"2.4.2 SEER","text":"addition FAIR principles, SEER principles, developed 2018 Institute Education Sciences (IES), provide Standards Excellence Education Research (Institute Education Sciences n.d.c). principles broadly cover entire life cycle research study, provide context good data management within education research study. SEER principles include:Pre-register studiesMake findings, methods, data openIdentify interventions’ core componentsDocument treatment implementation contrastAnalyze interventions’ costsFocus meaningful outcomesFacilitate generalization study findingsSupport scaling promising results","code":""},{"path":"rdm.html","id":"open-science","chapter":"2 Research Data Management Overview","heading":"2.4.3 Open Science","text":"concept Open Science pushed quality data management forefront, bringing visibility cause, well advances practices urgency implement . Open Science aims make scientific research dissemination accessible , making need good data management practices absolutely necessary. Open science advocates transparent reproducible practices means open data, open analysis, open materials, preregistration, open access (Dijk, Schatschneider, Hart 2021). Organizations Center Open Science (https://www.cos.io), become well-known proponents open science, offering open science framework (OSF) (Foster Deardorff 2017) tool promote open science entire research life cycle. Furthermore, many education funders aligned fundee requirements open science practices, openly sharing study data pre-registration study methods (Institute Education Sciences n.d.b).","code":""},{"path":"rdm.html","id":"terminology","chapter":"2 Research Data Management Overview","heading":"2.5 Terminology","text":"moving forward book important shared understanding terminology used. Many concepts education research synonymous terms can used interchangeably. Across different institutions, researchers may use terms. Please review Glossary gain better understanding various terms used throughout book.","code":""},{"path":"rdm.html","id":"the-research-life-cycle","chapter":"2 Research Data Management Overview","heading":"2.6 The Research Life Cycle","text":"remainder book organized chapters provide best practices phase research data life cycle. imperative understand research life cycle order see flow data project, well see everything project connected. phases skipped, whole project suffer.can see Figure 2.1, throughout project, data management roles project coordination roles work parallel collaboratively. teams may made people different members, either way, workflows must happen must work together.\nFigure 2.1: research project life cycle\nLet’s walk chart.typical study first begin generating ideas, deciding want study., likely, look grant funding implement study. two paths begin diverge. team applying federal funding, proposal budget created project management track, supplemental required data management plan (see Chapter 4) created data track. , may people working pieces.Next, grant awarded, project team begin planning things hiring, recruitment, data collection, implement intervention. time, working data team begin plan specifically implement 2-5 page data management plan submitted funder start putting necessary structures place.planning complete, team moves cycle data collection. called cycle study longitudinal, every step occur cyclically. one phase data collection wraps , team re-enters cycle next phase data collection, data collection complete entire project.\ndata management project management team begin cycle starting documentation. can see phase occurs collaboratively denoted double outline. teams begin developing documentation data dictionaries standard operating procedures.\ndocumentation started, teams collaboratively begin create necessary data collection instruments. instruments created input documentation. phase teams may also develop participant tracking database.\nNext, project management team moves data collection phase. addition actual data collection, may also involve preliminary activities recruitment consenting participants, well hiring training data collectors. point, data management team just provides support needed.\ndata collected, project team track data collected participant tracking database. data management team collaborate project management team help troubleshoot anything related actual tracking database issues discovered data tracking.\nNext, data collected, teams move data capture phase. teams actively retrieving converting data. electronic data may look like downloading data platform data sent team via secure transfer. physical data, may look like teams entering paper data database. Oftentimes, collaborative effort project management team data team.\ndata captured, needs stored. data team may charge setting monitoring storage efforts, project team may ones actively retrieving storing data.\nNext teams move cleaning validation phase. time data team reviewing data cleaning plans, writing data cleaning scripts, actively cleaning data recent data collection round.\nlast, data team version data updated errors found.\ndata management project management team begin cycle starting documentation. can see phase occurs collaboratively denoted double outline. teams begin developing documentation data dictionaries standard operating procedures.documentation started, teams collaboratively begin create necessary data collection instruments. instruments created input documentation. phase teams may also develop participant tracking database.Next, project management team moves data collection phase. addition actual data collection, may also involve preliminary activities recruitment consenting participants, well hiring training data collectors. point, data management team just provides support needed.data collected, project team track data collected participant tracking database. data management team collaborate project management team help troubleshoot anything related actual tracking database issues discovered data tracking.Next, data collected, teams move data capture phase. teams actively retrieving converting data. electronic data may look like downloading data platform data sent team via secure transfer. physical data, may look like teams entering paper data database. Oftentimes, collaborative effort project management team data team.data captured, needs stored. data team may charge setting monitoring storage efforts, project team may ones actively retrieving storing data.Next teams move cleaning validation phase. time data team reviewing data cleaning plans, writing data cleaning scripts, actively cleaning data recent data collection round.last, data team version data updated errors found.teams move active data collection phase data collection project complete. time project team begins analyzing study data working publications well final grant reports. able organized processes implemented data collection cycle. Since data managed cleaned throughout, data ready analysis soon data collection complete. , project team analyzing data, data team additional preparation archive data public sharing.Last, grant closing , team submits data public sharing.work remaining chapters book, chart guide navigating phase practices fits larger picture.","code":""},{"path":"structure.html","id":"structure","chapter":"3 Data Structure","heading":"3 Data Structure","text":"data management made just , data, need basic understanding data looks like. Understanding basic structure data helps us write Data Management Plan, organize data management process, create data dictionaries, build data collection tools, clean data, ways allow us analyzable data.","code":""},{"path":"structure.html","id":"basics-of-a-dataset","chapter":"3 Data Structure","heading":"3.1 Basics of a dataset","text":"education research, data often collected internally team using instrument questionnaire, observation, interview, assessment. However, data may also collected external entities, districts, states, agencies.data come many forms (e.g., video, transcripts, documents, files), represented text, numbers, multimedia (USGS n.d.). world quantitative education research, often working digital data form dataset, structured collection data. datasets organized rectangular format allow data machine-readable. Even qualitative research, often wrangling data format analyzable allows categorization.rectangular (also called tabular) datasets made columns rows.\nFigure 3.1: Basic format dataset\n","code":""},{"path":"structure.html","id":"columns","chapter":"3 Data Structure","heading":"3.1.1 Columns","text":"columns dataset consist one following types variables:Variables collect (instrument external source)Variables create/add (e.g., cohort, intervention, time, derivations)Unless data collected anonymously, every dataset must also following:One variables unique identifiers, sometimes called primary keys. variables uniquely define rows dataset (.e. help identify duplicate rows), also allow link data contain identifiers (example link student data). important make sure variables consistently formatted across files (e.g., always character variables).plan link datasets across entities (e.g., link teachers schools students teachers) also need secondary unique identifiers dataset (also called foreign keys) allow link across datasets.talk creating identification variables Chapter 9.Column attributesIt important know variables following attributes:Unique names (variable name dataset can repeat). talk variable naming discuss style guides (see Chapter 8).measurement type (e.g., numeric, character, date) can also narrowly defined needed (e.g., continuous, categorical)Acceptable values (e.g., yes/) expected ranges (e.g., 1-25 2021-08-01 2021-12-15). Anything outside acceptable values ranges considered error.Labels, descriptions variable represents. may label variable creator assigns (e.g., “Treatment condition”) may actual wording item (e.g., “enjoy pizza?”).","code":""},{"path":"structure.html","id":"rows","chapter":"3 Data Structure","heading":"3.1.2 Rows","text":"rows dataset aligned participants cases data. Participants data may students, teachers, schools, locations, forth. unique identifier variable mentioned denote row belongs participant.","code":""},{"path":"structure.html","id":"cells","chapter":"3 Data Structure","heading":"3.1.3 Cells","text":"cells observations associated participant. Cells made key/value pairs, created intersection column row. Consider example collect survey students. dataset, row made unique student study, column item survey, cell contains value/observation corresponds row/column pair (participant question).\nFigure 3.2: Representation cell value\n","code":""},{"path":"structure.html","id":"dataset-organization-rules","chapter":"3 Data Structure","heading":"3.2 Dataset organization rules","text":"order dataset machine-readable analyzable, adhere set structural rules (Broman Woo 2018; Wickham 2014).first rule data make rectangle. first row data variable names (use one row ). remaining data made values cells.\nFigure 3.3: comparison non-rectangular rectangular data\ncolumns adhere variable type.\nexample, numeric variable, age, add cell value text, variable longer adheres variable type. Machines now read variable text.\nexample, numeric variable, age, add cell value text, variable longer adheres variable type. Machines now read variable text.\nFigure 3.4: comparison variables adhering adhering data type\nvariable collect one piece information. variable contains one piece information may following issues:\nlose granularity information (e.g., location = Los Angeles, CA less granular city variable state variable separately)\nvariable may become unanalyzable (e.g., variable value 220/335 analyzable numeric variable). interested rate, can calculate rate variable value .657.\nmay lose variable type (e.g., want incident_rate variable numeric, assign value 220/335, variable longer numeric)\nlose granularity information (e.g., location = Los Angeles, CA less granular city variable state variable separately)variable may become unanalyzable (e.g., variable value 220/335 analyzable numeric variable). interested rate, can calculate rate variable value .657.may lose variable type (e.g., want incident_rate variable numeric, assign value 220/335, variable longer numeric)\nFigure 3.5: comparison two things measured one variable two things measured across two variables\ncell values explicit. means cells filled physical value.\nConsider cell value empty\nvalue actually missing, can either leave cells blank fill pre-determined missing values (e.g., -99). See Chapter 8 ideas.\ncell left empty “implied” value , cells filled actual data\nvalue cell “implied” 0, fill cells 0\n\nConsider cell value empty\nvalue actually missing, can either leave cells blank fill pre-determined missing values (e.g., -99). See Chapter 8 ideas.\ncell left empty “implied” value , cells filled actual data\nvalue cell “implied” 0, fill cells 0\nvalue actually missing, can either leave cells blank fill pre-determined missing values (e.g., -99). See Chapter 8 ideas.cell left empty “implied” value , cells filled actual dataIf value cell “implied” 0, fill cells 0\nFigure 3.6: comparison variables empty cells variables empty cells\nvalues implied using color coding\nwant indicate information, add indicator variable rather cell coloring\nwant indicate information, add indicator variable rather cell coloring\nFigure 3.7: comparison variables implicit values variables explicit values\ndata contain duplicate rows. want duplicate rows measurement collected participant, time period. Different types duplicate rows can occur:\ntrue duplicate row entire row duplicated (row values every variable). may happen someone enters form twice.\nunique identifier duplicated row values may may across variables. happen one three reasons:\ninstrument accidentally collected participant collection period. type duplicate need remedied.\nunique identifier entered incorrectly. case don’t actually duplicate, just incorrect unique identifier. error need remedied.\none variable used identify unique participants row actually duplicate.\nTake example student id class id. Multiple unique identifiers may used data collected participants multiple locations treated unique data. case, data truly duplicate combined identifiers unique.\nAnother example data organized long format (discussed Section 3.3.2). case unique study identifiers may repeat data repeat form time period data.\n\n\ntrue duplicate row entire row duplicated (row values every variable). may happen someone enters form twice.unique identifier duplicated row values may may across variables. happen one three reasons:\ninstrument accidentally collected participant collection period. type duplicate need remedied.\nunique identifier entered incorrectly. case don’t actually duplicate, just incorrect unique identifier. error need remedied.\none variable used identify unique participants row actually duplicate.\nTake example student id class id. Multiple unique identifiers may used data collected participants multiple locations treated unique data. case, data truly duplicate combined identifiers unique.\nAnother example data organized long format (discussed Section 3.3.2). case unique study identifiers may repeat data repeat form time period data.\n\ninstrument accidentally collected participant collection period. type duplicate need remedied.unique identifier entered incorrectly. case don’t actually duplicate, just incorrect unique identifier. error need remedied.one variable used identify unique participants row actually duplicate.\nTake example student id class id. Multiple unique identifiers may used data collected participants multiple locations treated unique data. case, data truly duplicate combined identifiers unique.\nAnother example data organized long format (discussed Section 3.3.2). case unique study identifiers may repeat data repeat form time period data.\nTake example student id class id. Multiple unique identifiers may used data collected participants multiple locations treated unique data. case, data truly duplicate combined identifiers unique.Another example data organized long format (discussed Section 3.3.2). case unique study identifiers may repeat data repeat form time period data.\nFigure 3.8: comparison data duplicate cases data duplicate cases\n","code":""},{"path":"structure.html","id":"linking-data","chapter":"3 Data Structure","heading":"3.3 Linking data","text":"now talking one, standalone dataset. However, likely research project made multiple datasets, collected different participants, variety instruments, possibly across different time points. point likely need link datasets together.order think link data, need discuss two things: data structure database design.","code":""},{"path":"structure.html","id":"database","chapter":"3 Data Structure","heading":"3.3.1 Database design","text":"database “organized collection data stored multiple datasets” (USGS n.d.). Sometimes database actually housed database software system (SQLite FileMaker), times loosely using term database simply define linking disparate datasets together stored individually file system. matter storage system, general concepts applicable.database terminology, dataset considered “table”. table primary key identifies unique entries within table table can connected primary foreign keys. linking tables creates relational database talk structure discuss participant data tracking (see Chapter 9).Let’s take simplest example, primary keys data. collected two pieces data students (survey assessment) one time period. Figure 3.9 shows variables collected instrument table can linked together primary key (circled yellow).\nFigure 3.9: Linking data primary keys\nHowever, often collecting data across different forms, also collecting nested data across different participants (e.g., students, nested classrooms, nested schools, ). Let’s take another example collected data three instruments, student assessment, teacher survey, school intake form. Figure 3.10 shows variables exist dataset (primary keys still circled yellow) table can linked together foreign key (circled blue).\nFigure 3.10: Linking data foreign keys\ncan imagine, add forms, begin collect data across time, database structure begins become even complex. Figure 3.11 another example collected two forms students (survey assessment), two forms teachers (survey observation), one form schools (intake form). linking structure begins look complex, see can still link data primary foreign keys. Forms within participants can linked primary keys, forms across participants can linked foreign keys.\nFigure 3.11: Linking data primary foreign keys\n","code":""},{"path":"structure.html","id":"datastructure","chapter":"3 Data Structure","heading":"3.3.2 Data structure","text":"comes time link data, two ways often think linking structuring data, wide long.","code":""},{"path":"structure.html","id":"wide-format","chapter":"3 Data Structure","heading":"3.3.2.1 Wide format","text":"structure data wide format, data collected unique participant one row. Participants duplicated data format.type format can used following situations:link forms within timeTo link forms across timeTo link forms across participantsThe easiest scenario think format repeated measure data. collect survey participants wave 1 2, waves data row (joined together unique ID) wave data collection appended variable name create unique variable names. dive deeper different types joins Chapter 13.Note \nimportant note , data unique identifiers (primary /foreign keys), case anonymous data, unable merge data wide format.\nFigure 3.12: Data structured wide format\n","code":""},{"path":"structure.html","id":"long-format","chapter":"3 Data Structure","heading":"3.3.2.2 Long format","text":"education research, long data mostly used specific way structure data collected time. long data participant can repeat dataset., straight forward way think repeated measure data, row new time point participant. instead merging forms unique id, stack forms top , often called appending data. Rows stacked top one another variables aligned variable name. Now instead linking data id, data now “linked” variable names. important variable names types stay identical time order structure work.scenario, longer add data collection wave variable names. However, need add time period variable denote wave associated row data.\nFigure 3.13: Data structured long format\n","code":""},{"path":"structure.html","id":"choosing-wide-vs-long","chapter":"3 Data Structure","heading":"3.3.2.3 Choosing wide vs long","text":"different reasons constructing data one way another. may store share data one format, restructure data another format comes time analysis.Storing data long format usually considered efficient, potentially requiring less memory. However, comes time analysis, specific data structures may required. example, repeated measure procedures typically require data wide format, unit analysis subject. mixed model procedures typically required data long format, unit analysis measurement subject (Grace-Martin 2013). review decision making around data structure Chapter 13.","code":""},{"path":"structure.html","id":"file-types","chapter":"3 Data Structure","heading":"3.4 File types","text":"rectangular datasets can saved variety file types. common file types education research include interoperable formats CSV, TXT, TSV, proprietary formats XLSX, SAV (SPSS), DTA (Stata).save files, file size. number columns well number rows dataset contribute file size. Just get feel size files might , small datasets (example 5 columns <100 rows) may less 100 KB. Datasets several hundred variables several thousand cases may start 1,000-5,000 KB range. type file use also changes size data. Saving data format contains embedded metadata (variable value labels), .sav file, greatly increase file size. talk pros cons different file formats Chapter 14.","code":""},{"path":"dmp.html","id":"dmp","chapter":"4 Data Management Plan","heading":"4 Data Management Plan","text":"\nFigure 4.1: Data management plan research project life cycle\n","code":""},{"path":"dmp.html","id":"history-and-purpose","chapter":"4 Data Management Plan","heading":"4.1 History and purpose","text":"Since 2013, even earlier National Science Foundation, federal agencies education researchers work required data management plan (DMP) part funding application (Holdren 2013). focus plans mostly future outcome data sharing, data management plan means ensuring researchers thoughtfully plan research study result data can shared confidence, free errors, uncertainty, violations confidentiality. President Obama’s May 2013 Executive Order declared “default state new modernized government information resources shall open machine readable” (White House 2013). August 2022, Office Science Technology Policy (OSTP) doubled data sharing policy issued memorandum stating federal agencies must update public access policies later December 31, 2025, make federally funded publications supporting data accessible public embargo release (Nelson 2022). Even sooner , organizations like National Institutes Health (NIH) mandated grant applicants, beginning January 2023, must submit plan managing sharing project data (National Institutes Health n.d.c). National Science Foundation (NSF) also released version 2.0 public access plan February 2023, describing agency plans ensure scientific data, funded NSF associated peer-reviewed publications, publicly shared (National Science Foundation 2023).Note \nlast year, agencies begun revising phrase “data management plan” include word “sharing” better represent shifting emphasis sharing publicly funded data. example, NIH now uses term Data Management Sharing Plan (DMS Plan), Institute Education Sciences (IES) chosen use term Data Sharing Management Plan (DSMP). sake simplicity, term DMP used throughout book generally represent plans, matter precise name, across federal agencies.","code":""},{"path":"dmp.html","id":"why-are-dmps-important","chapter":"4 Data Management Plan","heading":"4.1.1 Why are DMPs important?","text":"Funding agencies see DMPs important maximizing scientific outputs investments increasing transparency. Mandating data sharing federally funded projects leads many benefits including accelerating discovery, greater collaboration, building trust among data creators users. addition benefits viewed funders, intrinsic benefits come write data management plan. thoughtfully plan transparency plan leads better data management. Knowing eventually sharing data documentation others outside team can motivate researchers think hard organize data management practices way produce data trust share outside world (Center Open Science n.d.).","code":""},{"path":"dmp.html","id":"what-is-it","chapter":"4 Data Management Plan","heading":"4.2 What is it?","text":"Generally, data management plan supplemental 2-5 page document, submitted grant application, contains details plan store, manage, share research data products. funders DMPs part scoring process, reviewed panel program officer. funders may provide feedback ask revisions believe plan /budget associated costs adequate.","code":""},{"path":"dmp.html","id":"what-to-include","chapter":"4 Data Management Plan","heading":"4.2.1 What to include?","text":"include DMP varies across funding agencies landscape requirements currently evolving. check funding agency’s site specific DMP requirements submitting proposal. said typically 10 common categories covered data management plan (Center Open Science n.d.; Gonzales, Carson, Holmes 2022; ICPSR n.d.; Michener 2015) review .Roles responsibilities\nstaff roles management long-term preservation data?\nensures accessibility, reliability, quality data?\nplan core team member leaves project institution?\nstaff roles management long-term preservation data?ensures accessibility, reliability, quality data?plan core team member leaves project institution?Types amount data\ndata captured? (e.g., surveys, assessments, observations)\ndata item-level? Summary scores? Metadata ?\nDatasets project may need shared different ways due legal, ethical, technical reasons.\n\nshare raw data clean data?\nexpected number files? Expected number rows/cases file?\ndata captured? (e.g., surveys, assessments, observations)data item-level? Summary scores? Metadata ?\nDatasets project may need shared different ways due legal, ethical, technical reasons.\nDatasets project may need shared different ways due legal, ethical, technical reasons.share raw data clean data?expected number files? Expected number rows/cases file?Format data\ndata electronic format?\nprovided non-proprietary format? (e.g., CSV)\none format provided? (e.g., SAV CSV)\ntools needed manipulate shared data?\ncode available support reproducibility?\ndata electronic format?provided non-proprietary format? (e.g., CSV)one format provided? (e.g., SAV CSV)tools needed manipulate shared data?code available support reproducibility?Documentation\ndocumentation share? (Consider project-level, dataset-level, variable-level documentation)\nmetadata create?\nformat documentation ? (e.g., XML, CSV, PDF)\nsupplemental documents plan include sharing data? (e.g., consort diagrams, data collection instruments, consent forms)\ndocumentation share? (Consider project-level, dataset-level, variable-level documentation)metadata create?format documentation ? (e.g., XML, CSV, PDF)supplemental documents plan include sharing data? (e.g., consort diagrams, data collection instruments, consent forms)Standards\nplan use standards things metadata, data formatting, terminology, persistent identifiers (PIDs)?\nplan use standards things metadata, data formatting, terminology, persistent identifiers (PIDs)?Method data sharing\nshare data? (Many agencies now requiring applicants name specific data repository section)\ndata restricted data enclave required?\ndata use agreement required?\nlicense data?\ndata persistent unique identifiers?\nshare data? (Many agencies now requiring applicants name specific data repository section)data restricted data enclave required?data use agreement required?license data?data persistent unique identifiers?Circumstances preventing data sharing\ndata covered FERPA/HIPAA doesn’t allow data sharing?\nwork partners allow share data? (e.g., School districts, tribal regulations)\nworking proprietary data?\ndata covered FERPA/HIPAA doesn’t allow data sharing?work partners allow share data? (e.g., School districts, tribal regulations)working proprietary data?Privacy rights participants\nprevent disclosure personally identifiable information share data? anonymize data (applicable)?\nparticipants sign informed consent agreements? consent communicate participant data expected used shared?\nprevent disclosure personally identifiable information share data? anonymize data (applicable)?participants sign informed consent agreements? consent communicate participant data expected used shared?Data security\nmaintain participant privacy confidentiality project?\nprevent unauthorized access data?\nConsider IRB requirements .\nmaintain participant privacy confidentiality project?prevent unauthorized access data?Consider IRB requirements .Schedule data sharing\nshare study data long?\nshare study data long?Pre-registration (less commonly required)\npre-register study?\npre-register study?, specifics included category vary funder. sites visit learn four common federal education research funder DMP requirements.Institute Education Sciences 1 2National Institutes Health 3National Institute Justice 4National Science Foundation 5","code":""},{"path":"dmp.html","id":"getting-help","chapter":"4 Data Management Plan","heading":"4.3 Getting help","text":"Since DMPs written project funded, therefore additional staff members may hired, oftentimes investigators developing grant proposal ones write DMP. However, constructing DMP well worth time enlist help. existing data manager data team, certainly want consult writing plan ensure decisions feasible. work university system, research data librarians also excellent resources wealth knowledge writing comprehensive data management plans. last, plan share final data repository institutional archive want contact repository writing plan well. repository may requirements data must shared helpful outline guidelines data management plan time submission. can also specifically write name repository data management plan well. Last, may want obtain help colleagues. colleagues likely written DMPs many people willing share plans way help others better understand include.DMP living document can always update plan project completion. may helpful keep contact program officer regarding potential changes throughout project.looking guidance writing DMP, variety generic DMP templates different federal agencies available, well actual copies submitted DMPs researchers graciously make publicly available example purposes.Template Resources","code":""},{"path":"dmp.html","id":"budget","chapter":"4 Data Management Plan","heading":"4.4 Budgeting","text":"briefly mention , funding agencies acknowledge costs associated implementing data management plan allow explain costs budget narrative. Costs associated entire data life cycle considered may include costs associated data management personnel, specialized infrastructure, tools needed collect, enter, organize, document, store, share study data (UK Data Service 2022), well fees associated data preservation. Make sure review funder’s documentation information allowable costs(Samuel J. Wood Library n.d.) time frame incurring costs. Examples potential allowable costs include (National Institutes Health n.d.):Costs associated curating de-identifying dataCosts associated developing data documentationFees associated depositing data long-term sharing repositoryIt can difficult estimate costs everything associated vast landscape managing data. Luckily organizations developed resources aid estimating costs.Resources","code":""},{"path":"plan.html","id":"plan","chapter":"5 Planning Data Management","heading":"5 Planning Data Management","text":"\nFigure 5.1: Planning research project life cycle\nPlanning data management distinct 2-5 page data management plan (DMP) discussed Chapter 4. spending weeks, maybe months, meeting regularly team gathering information develop detailed instructions plan manage data according DMP. data management planning happens time project team planning project implementation (things like collect data, hire staff, supplies needed, recruit participants, communicate sites, etc). Team members PIs, project coordinators, data managers, may assisting planning processes.","code":""},{"path":"plan.html","id":"why-spend-time-on-planning","chapter":"5 Planning Data Management","heading":"5.1 Why spend time on planning?","text":"Funder required data management plans hopeful outlines future practices. However, broad theory behind DMPs actually prepare us complex implementation plans practice (Borycz 2021). Therefore, important spend time, project begins, planning preparing data management. upfront time investment sort slow science leads better data outcomes. Reproducibility begins planning phase. Taking time create, document, train staff data management standards project begins helps ensure processes implemented fidelity can replicated consistently throughout entire study.Planning day day management project data many benefits well. allows anticipate overcome barriers managing data, communication issues, training needs, potential tool issues. type planning also saves time long run, removing last minute scrambling can occur trying organize data end project. Last, type planning can mitigate errors. Viewing errors problems created poorly planned workflows, rather individual failures, helps us see data management planning can lead better data (Strand n.d.). data management planning can remove chances errors creeping data (Eaker 2016), can certainly reduce errors prevent “compounding time” (Alston Rick (2021), p.4 ).","code":""},{"path":"plan.html","id":"goals-of-planning","chapter":"5 Planning Data Management","heading":"5.2 Goals of planning","text":"planning phase include series regular meetings core decision makers. data management planning time several goals keep mind.Flesh project goals laid grant proposal (.e. data needs collected answer research questions)Finalize timeline goals (.e. data collected)Lay specific tasks needed accomplish goals come consensus regarding necessary data management decisions (.e. data collected, stored, managed, shared)Assign roles responsibilities (.e. responsible tasks)Make decisions around task management communication (.e. tasks monitored communication tracked)Make sure come every meeting agenda stay track take detailed notes. notes basis creating documentation (see Chapter 7). meeting notes can stored central location planning folder notes ordered date running document.Note \nplanning phase excellent time start setting project file structure file naming conventions according style guide (see Chapter 8) files organized, understandable, findable.end planning period, team clear plan project goals , goals accomplished, goals accomplished, charge completing tasks associated goals, additional resources needed accomplish goals.","code":""},{"path":"plan.html","id":"checklist","chapter":"5 Planning Data Management","heading":"5.3 Planning checklists","text":"Along existing data management plan, checklists great tools help guide discussions work planning process team. sample checklists, one phase research cycle. checklists can added amended brought planning meetings help team think various data management decisions need made phase research project.Planning checklistsRoles Responsibilities 19Task Management 20Documentation 21Data Collection 22Data Tracking 23Data Capture 24Data Storage Security 25Data Cleaning 26Data Sharing 27Note \nfirst time working book, checklists great way summarize content chapter. learn best practices phase, pull checklist specific chapter begin thinking practices feasible specific project.","code":""},{"path":"plan.html","id":"decision-making-process","chapter":"5 Planning Data Management","heading":"5.3.1 Decision-making process","text":"move remaining chapters book, begin learn recommended practices phase research cycle. Going checklist , can start fill practices work project phase study.decision-making process personalized. Borghi Van Gulick (Borghi Van Gulick 2022) view process series steps research team chooses, many possibilities chosen. Maybe won’t always able implement “best practices” can decide good enough team based motivations, incentives, needs, resources, skill set, rules regulations.example, one team may collect survey data paper participants young children, hand enter Excel tool access , double enter 20% don’t capacity enter . Another team may collect paper data collecting data field, hand enter data FileMaker tool team familiar , double enter 100% budget capacity .Figure 5.2 simplified example decision making process, based (Borghi Van Gulick 2022) flow chart. course real life often choosing many just two options!\nFigure 5.2: simplified decision-making process\n","code":""},{"path":"plan.html","id":"checklist-considerations","chapter":"5 Planning Data Management","heading":"5.3.2 Checklist considerations","text":"’s important consider team project unique work planning checklists. technique might work well one team, may work well another. Make sure consider following:external requirements\npractices align plan laid DMP? , may need revise DMP match new decisions - remember DMP living document.\npractices meet external compliance requirements Institutional Review Board, institutional policies, project partner requirements, government mandates?\npractices align plan laid DMP? , may need revise DMP match new decisions - remember DMP living document.practices meet external compliance requirements Institutional Review Board, institutional policies, project partner requirements, government mandates?skill set team\nskill set team align practices plan implement? additional training required?\nskill set team align practices plan implement? additional training required?available tools\ntools available team?\norganization allow use certain platforms data storage?\ncomplexity tools? additional training needed?\ntools available team?organization allow use certain platforms data storage?complexity tools? additional training needed?budget\nbudget implement practices want implement need plan something feasible?\nbudget implement practices want implement need plan something feasible?Complexity project\nsize project, amount types data collecting, number participants populations collecting data , sensitivity level data collecting, number sites collecting data , number partners decision makers working , factor data management planning\nsize project, amount types data collecting, number participants populations collecting data , sensitivity level data collecting, number sites collecting data , number partners decision makers working , factor data management planningShared investment\nentire team invested quality data management?\nentire team motivated adhere standards instructions laid data management planning? , safeguards can implement help prevent errors creeping data?\nentire team invested quality data management?entire team motivated adhere standards instructions laid data management planning? , safeguards can implement help prevent errors creeping data?","code":""},{"path":"plan.html","id":"data-management-workflow","chapter":"5 Planning Data Management","heading":"5.4 Data management workflow","text":"last step planning phase build workflows. Workflows allow data management seamlessly integrated data collection process. Often illustrated flow diagram, workflow series repeatable tasks help move stages research life cycle “organized efficient manner” (CSP Library Research n.d.). walk checklists, can begin enter decisions workflow diagram show actionable steps data management process. order steps follow general order data management life cycle (specifically data collection cycle). want workflow diagram every piece data collect. example, collect following three items , three workflow diagrams.Student online surveyStudent paper assessmentStudent district-level administrative dataYour diagrams include , , , task process. Adding details make process actionable (Borycz 2021). diagram can displayed format works can simple detailed want . template like one Figure 5.3 works well thinking high level workflows. Remember, repeatable process. diagram linear (steps laid chronological order expect happen), process repeated every time collect piece data.\nFigure 5.3: simple workflow template\nmight complete diagram student survey.\nFigure 5.4: Example student survey workflow\nformat truly matter. Figure 5.5 diagram student survey workflow , detailed added, time using swimlane template instead, lane displays tasks associated individual iterative processes occur within across lanes.\nFigure 5.5: Example student survey workflow using swimlane template\nworking data collection timeline (see Chapter 7) already created, can even build time workflow. Figure 5.6 another example survey workflow , time displayed using Gantt chart (Duru Kopper, n.d.) order better capture expected timeline.\nFigure 5.6: Example student survey workflow using Gantt chart\nworkflow diagrams excellent high level views process , can see unable put fine details visual. last step creating workflow put tasks (final decisions associated tasks) standard operating procedure (SOP). SOP add necessary details process. can also attach diagram addendum link SOPs diagrams ways reference. talk creating SOPs Chapter 7.","code":""},{"path":"plan.html","id":"benefits-to-visualizing-a-workflow","chapter":"5 Planning Data Management","heading":"5.4.1 Benefits to visualizing a workflow","text":"Visualizing decisions diagram format many benefits. First, allows team conceptualize specific tasks process, timing tasks occur, dependencies associated tasks. also allows team see roles responsibilities fit larger research process (Briney, Coates, Goben 2020). Showing data management integrated larger research workflow can help team members view data management part daily routine, rather “extra work” (Borghi Van Gulick 2022). last, reviewing workflows team allowing members provide feedback may help create buy-data management processes, potentially leading better adherence practices.","code":""},{"path":"plan.html","id":"workflow-considerations","chapter":"5 Planning Data Management","heading":"5.4.2 Workflow considerations","text":"Similar questions need consider reviewing planning checklists, also need evaluate following things developing personalized workflow (Hansen 2017).flow preserve integrity data? point might lose comprise data?point flow data handled securely? Someone gains access identifiable information access?flow accordance compliance requirements (IRB, FERPA, HIPAA, Institutional Data Policies, etc.)?flow feasible team (based size, skill level, motivation, etc.)?flow feasible budget available resources?flow feasible amount types data collecting?bottlenecks workflow? Areas resources training needed? areas tasks re-directed?","code":""},{"path":"plan.html","id":"task-management-systems","chapter":"5 Planning Data Management","heading":"5.5 Task management systems","text":"tools checklists, workflow diagrams, SOPs allow us document share processes, can tricky manage day day implementation processes. planning phase great time choose task management system (Gentzkow Shapiro 2014). Keeping track various deadlines communications across scattered sources can overwhelming using task management system may help remove ambiguity status task progress. Rather regularly check via email status updates reading various meeting notes learn decisions made, task management system allows assign tasks responsible parties, set deadlines based timelines, track progress, capture communication decisions one location.many existing tools allow teams assign track tasks, schedule meetings, track project timelines, document communication. Without endorsing particular product, project/task management tools know education research teams used include:TrelloSmartsheetTodoistMicrosoft PlannerNotionBasecampConfluenceAsanaOf course, processes ’ve discussed far, task management system useful team trained use , invested using , actually uses part daily routine. make sure consider choose tool, , right .","code":""},{"path":"roles.html","id":"roles","chapter":"6 Project Roles and Responsibilities","heading":"6 Project Roles and Responsibilities","text":"\nFigure 6.1: Planning research project life cycle\nPart DMP planning data management phase, noted previous chapters, include assigning roles responsibilities. terms data management, important assign document roles, just presume roles, many reasons including following (UK Data Service 2023c):allows team members begin standardizing workflowsWhen team members know exactly expected , keeps data secureCreating contingency plans staff can longer fulfill roles allows continuity practices","code":""},{"path":"roles.html","id":"typical-roles-in-a-research-project","chapter":"6 Project Roles and Responsibilities","heading":"6.1 Typical roles in a research project","text":"diving assign document roles project, important get understanding typical roles education research project team. team may lucky enough (multiple ) roles. times, just one person, Principal Investigator (PI), may take multiple roles. said, budget allows , highly recommend hiring individuals fill roles mentioned allow team members specialize excel area expertise. learning aspects project highly recommended create cohesive team works collaboratively, team members take many project roles can spread thin project goals may suffer.","code":""},{"path":"roles.html","id":"pi-and-co-pi","chapter":"6 Project Roles and Responsibilities","heading":"6.1.1 PI and Co-PI","text":"PIs (project directors), well Co-PIs, individuals prepare submit grant proposal responsible administration grant. often one PI project including least someone content area knowledge well methodologist. PIs Co-PIs varying levels involvement research projects typically, always, hands day day administration. Even tasks delegated research staff, PIs Co-PIs ultimately responsible Institutional Review Board (IRB) submissions meeting IRB requirements, well submitting MOUs, budgets, effort reporting, continuing review reports, final technical finding reports.","code":""},{"path":"roles.html","id":"project-coordinator","chapter":"6 Project Roles and Responsibilities","heading":"6.1.2 Project Coordinator","text":"project coordinator (project manager) essential member research team. name implies, person typically coordinates research activities ensures compliance agencies Institutional Review Board. Tasks may oversee include recruitment consenting participants, creation data collection materials, creation protocols, training data collectors, data collection scheduling, . project coordinator may also supervise many research team roles, research assistants.","code":""},{"path":"roles.html","id":"data-manager","chapter":"6 Project Roles and Responsibilities","heading":"6.1.3 Data Manager","text":"data manager also essential member team. person responsible organizing, cleaning, documenting, storing, dissemination research project data. team member works closely project coordinator, well PI, ensure data management considered throughout project life cycle. Tasks data manager may oversee include data storage, security access, building data collection tracking tools, cleaning validating data, data documentation, organizing data sharing purposes.role vital maintaining standardization data practices. budget hire full-time data manager, make sure assign someone team oversee flow data, ensuring throughout project, data documented, collected, entered, cleaned, stored consistently securely.","code":""},{"path":"roles.html","id":"project-team-members","chapter":"6 Project Roles and Responsibilities","heading":"6.1.4 Project Team Members","text":"role refers staff hired help implement research project may include full-time staff members, titles research project assistants instance, may include part-time graduate students. Project team members typically field, collecting data, may also assist areas preparing data collection materials assisting data management. Senior project team members may also assist implementing training acting data collection leads field.","code":""},{"path":"roles.html","id":"other-roles","chapter":"6 Project Roles and Responsibilities","heading":"6.1.5 Other Roles","text":"size research team roles exist dependent factors funding, type research study, intervention studied, organization specific research institution. teams may include additional roles, mentioned , research director, lab manager, software engineer, database manager, postdoc, analyst, statistician, administrative professional, hourly data collector, outreach coordinator, coach/interventionist, may assist research cycle ways. roles assist research data life cycle seen diagram . may path hidden diagram still happening, behind scenes, alongside process. Take instance, role coach implementing intervention studied. tasks aren’t shown original diagram work happening alongside data collection cycle.\nFigure 6.2: Life cycle diagram updated show hidden processes\n","code":""},{"path":"roles.html","id":"assigning-roles-and-responsibilities","chapter":"6 Project Roles and Responsibilities","heading":"6.2 Assigning roles and responsibilities","text":"Early project may start generally assign roles data management plan. Remember submitted DMP, often required state responsible activities data integrity security. , project funded start better idea goals budget, can flesh details roles. planning phase, using tools planning checklists help think specific responsibilities tasks associated role. assigning roles responsibilities, several factors consider (Valentine n.d.).Required skillsetIn assigning roles responsibilities, make sure consider skills needed successful position. example, considering role data manager responsibilities associated role, may look skill sets following buckets:Interpersonal skills (Detail-oriented, organized, good communicator)Domain skills (Experience working education data, understands data privacy - FERPA, HIPAA)Technical skills (Understanding database structure, experience building data pipelines, coding experience, specific software/tool experience)specific skills needed role depend project needs well skill sets members team.Training needsIn addition considering skills needed certain roles, also consider training needed fulfill assigned responsibilities. roles work data, training may include mandated courses program like Collaborative Institutional Training Initiative (CITI) may signing training use specific tool software. Make sure team members well-equipped perform responsibilities project begins.Estimated costsIf working roles responsibilities grant funded, grant budget already submitted. However, can still helpful thinking costs associated overall roles (based experience/skillset person filling role) even broken associated responsibilities (based things like percent effort time complete task). discrepancies original budget updated costs found, often funders allow PIs amend budgets.Assess equity responsibilitiesReview responsibilities allocated. Consider time needed complete tasks number responsibilities assigned team member. Make sure overloading one team member, reassign tasks needed.Contingency plansYou also begin thinking backup plans staff member leave project absent extended period time. may include cross training staff plan training replacement staff.","code":""},{"path":"roles.html","id":"roledoc","chapter":"6 Project Roles and Responsibilities","heading":"6.3 Documenting roles and responsibilities","text":"assigning roles responsibilities, decisions documented avoid ambiguity . documentation topic covered next chapter, think helpful break rules discuss just one document covering topic assigning roles.many reasons document staff roles responsibilities store information central, accessible location.allows team easily reference document see project team, roles play, contact questions regarding various project aspects (e.g., contact data storage access).new tasks arise, team members can refer document see best fitted assignment.Last, reviewing roles responsibilities document also helps clearly see responsibilities assigned assigned. reviewing document can make revisions responsibilities need added redistributed way.document can laid format conveys information clearly team. Figure 6.3 Figure 6.4 two example templates. Note templates list overarching responsibilities, specific steps associated tasks. Specific actionable steps laid process documentation standard operating procedures (see Chapter 7) names attached task.\nFigure 6.3: Roles responsibilities document organized role\n\nFigure 6.4: Roles responsibilities document organized phase\nSince one template creating roles responsibility document, can really add whatever information helps clearly convey assignments. additional columns may consider adding include:Links related standard operating procedures (e.g., building participant tracking database may link specific SOP lays steps building tool)Names staff members () assist also contribute responsibilityTiming responsibility (e.g., weekly, ongoing, month February)","code":""},{"path":"roles.html","id":"data-management-role","chapter":"6 Project Roles and Responsibilities","heading":"6.4 Data management role","text":"Like mentioned earlier, highly recommend hiring full-time data manager able budget allows team member narrow responsibilities implement tasks better precision. However, everyone capacity . , vitally important still assign data management responsibilities specific team members. choosing assign tasks , want consider several things skill set manage data, interest data management tasks, time commit data management. Oftentimes responsibility falls full-time project coordinator ones intimately familiar data, since full-time, able carve hours data management tasks. times may collaboration project coordinator another staff member, part-time graduate student (may technical skills terms data wrangling). matter assign roles , just ensure documented information disseminated team.","code":""},{"path":"document.html","id":"document","chapter":"7 Documentation","heading":"7 Documentation","text":"\nFigure 7.1: Data documentation research project life cycle\nDocumentation collection files contain procedural descriptive information team, project, workflows, data. Creating thorough documentation study equally important collecting data. Documentation serves many purposes including:Standardizing proceduresSecuring data protecting confidentialityTracking data provenanceDiscovering errorsEnabling reproducibilityEnsuring others use interpret data accuratelyProviding searchability metadataWe going cover four levels documentation chapter: team-level, project-level, dataset-level, variable-level. documentation discussed fall within eponymous phase research life cycle, documents created earlier later timing discussed section. project, actively using documents, format documents matter. Choose human-readable format works well team (e.g., Word, PDF, TXT, Google Doc, XLSX, HTML, OneNote, etc.). projects closing preparing share data, can consider, time, best make documents sustainable, interoperable, searchable. See Chapter 14 information.documents recommended help successfully run project. can create many documents wish. documents choose produce based best project team, well required funder (see Chapter 4) governing bodies Institutional Review Board. matter documents choose implement, important create templates documents implement consistently within, even across projects. Implementing documentation using templates, consistent formats fields, reduces duplication efforts (need reinvent wheel) allows team interpret document easily. documents best created team member directly oversees process sometimes may include collaborative effort (example project coordinator data manager may build documents together).type documentation discussed living document updated procedures change new information received. seen cyclical section Figure 7.1 , team members revisit documentation time new data collected, often needed, ensure documentation still aligns actual practices. changes made added documentation long periods time, find longer remember happened information lost. also important version documents along way staff know working recent version can see documents updated .Note \nCreating maintaining documents investment. Make sure account time expertise proposal budget (see Chapter 4). said, return investment well worth effort.","code":""},{"path":"document.html","id":"team-level","chapter":"7 Documentation","heading":"7.1 Team Level","text":"Team-level data management documentation typically contains data governance rules apply entire team, across projects. documents can amended time, started long apply grant, lab, center, institution formed.\nFigure 7.2: Team-level documentation research project life cycle\n","code":""},{"path":"document.html","id":"lab-manual","chapter":"7 Documentation","heading":"7.1.1 Lab manual","text":"lab manual, team handbook, creates common knowledge across team (Mehr 2020). provides staff consistent information lab culture—team works things . also sets expectations, provides guidelines, can even place passing along career advice (Aczel n.d.; Turing Way Community 2022). lab manual primarily consist administrative, procedural, interpersonal types information, can helpful include data management content, including general rules accessing, storing, sharing, working data securely ethically.Template Resources","code":""},{"path":"document.html","id":"wiki","chapter":"7 Documentation","heading":"7.1.2 Wiki","text":"wiki webpage allows users collaboratively edit manage content. can either created alongside lab manual alternative lab manual team wiki. Wikis can built housed many tools SharePoint, Teams, Notion, GitHub Open Science Foundation (OSF). lab wikis public (’ll see examples ), can restricted invited users . Wikis great way keep disparate documents pieces information, administrative data related purposes, organized central, accessible location. wiki can include links important documents, can also add text directly wiki describe certain procedures. Rather sending team members multiple different folders frequently requested information, can refer one wiki page.\nFigure 7.3: Example team wiki links frequently requested information\nNote \nProject-level wikis can also created useful centralizing frequently referenced information pertaining specific projects.Templates Resources","code":""},{"path":"document.html","id":"onboardingoffboarding","chapter":"7 Documentation","heading":"7.1.3 Onboarding/Offboarding","text":"onboarding checklists mostly consist non-data related, administrative information sign email get set laptop, also contain several data specific pieces information get new staff generally acclimated working data new role.Similarly, offboarding checklists contain lot procedural information returning equipment handing tasks, also contain information specific data management documentation help maintain data integrity security.Data related topics consider adding onboarding offboarding checklists included Figure 7.4.\nFigure 7.4: Sample data topics add onboarding offboarding checklists\nTemplate Resources","code":""},{"path":"document.html","id":"data-inventory","chapter":"7 Documentation","heading":"7.1.4 Data Inventory","text":"data inventory maps data sources collected research team (Salfen 2018). team grows, number data sources typically expands well. can helpful keep inventory data sources available team members use, well details data sources.Project associated data sourceDates data sources collectedStorage location data sourceDetails dataset (dataset contains, organized, questions can answered data)data sources related","code":""},{"path":"document.html","id":"data-governance-plan","chapter":"7 Documentation","heading":"7.1.5 Data governance plan","text":"data governance plan set formal guidelines working data within organization (NYU Web Communications n.d.). often used administrative offices industry, types plans can also beneficial research teams. plan broadly cover team members allowed work data, considering things access controls, research participant privacy, data destruction rules. Documenting information ensures cohesive understanding among team members regarding terms conditions project data use (CESSDA Training Team 2017). data governance plan can added lab manual, can created separate document team members can even sign (Filip 2023) check box acknowledging read understand plan.Ideas content include data governance plan included Figure 7.5.\nFigure 7.5: Example content include data governance plan\nTemplate Resources","code":""},{"path":"document.html","id":"styleshort","chapter":"7 Documentation","heading":"7.1.6 Style guide","text":"style guide set standards formatting information (“Style Guide” 2023). improves consistency shared understanding within across files projects. document includes conventions procedures variable naming, variable value coding, file naming, versioning, file structure, even coding practices. can created one large document separate files type procedure. highly recommend applying style guide consistently across projects, hence included team documentation. Since style guides important, many recommended practices cover, given document chapter. See Chapter 8 information.Template Resources","code":""},{"path":"document.html","id":"project-level","chapter":"7 Documentation","heading":"7.2 Project level","text":"Project-level documentation descriptive information project contained, well planning decisions process documentation specifically related project. , documents created documentation phase, documents data management plan (started project funded), checklists meeting notes (started planning phase), participant flow diagram (started data collected) begin points throughout cycle.","code":""},{"path":"document.html","id":"data-management-plan","chapter":"7 Documentation","heading":"7.2.1 Data management plan","text":"discussed Chapter 4, project federally funded likely data management plan required. project-level document created DMP phase, long project begins. However, DMP can continue modified throughout entire study. major changes made, may helpful reach program officer keep loop well.","code":""},{"path":"document.html","id":"checklists-and-meeting-notes","chapter":"7 Documentation","heading":"7.2.2 Checklists and meeting notes","text":"Checklists, discussed Chapter 5, documents created (copied existing sources) reviewed planning phase. Using checklists facilitates discussion allows team build cohesive understanding data managed throughout entire project. work checklists, decisions made documented meeting notes. planning phase complete, decisions formally documented applicable team, project, data, variable-level documents (e.g. research protocol, SOPs, style guide, roles responsibilities documents). Even beyond planning phase though, meeting decisions discussions continue documented meeting notes used update formal documentation needed.","code":""},{"path":"document.html","id":"roles-and-responsibilities-document","chapter":"7 Documentation","heading":"7.2.3 Roles and responsibilities document","text":"Using checklists reviewed planning phase, team begin assigning roles responsibilities project. planning documentation phase, designations formally documented shared team. Chapter 6 reviewed ways structure document. document created, make sure store central location easy referral update document needed.Templates Resources","code":""},{"path":"document.html","id":"protocol","chapter":"7 Documentation","heading":"7.2.4 Research protocol","text":"research protocol comprehensive project plan document describes , , , , study. Many decisions made data management plan reviewing planning checklists summarized document. submitting study Institutional Review Board, likely required submit document part application. research protocol assists board determining methods provide adequate protection human subjects. addition serving required purpose, research protocol also excellent document share along data time data sharing, excellent resource writing technical reports manuscripts. document provides context needed others effectively interpret use data. Make sure follow university’s specific template provided, common items typically included protocol provided Figure 7.6.\nFigure 7.6: Common research protocol elements\ncomes time deposit data repository, protocol can revised contain information helpful data end user, known limitations data. Content risks benefits participants might removed, numbers study sample count updated show final numbers. Additional supplemental information can also added needed.Template Resources","code":""},{"path":"document.html","id":"supplement","chapter":"7 Documentation","heading":"7.2.5 Supplemental documents","text":"series documents, can absolutely standalone documents, calling supplemental documents can added research protocol addendum point clarify specifics project.TimelineThe first supplemental document highly recommend creating visual representation data collection timeline. first create timelines based best estimates time take complete milestones, like documents ’ve discussed, can updated learn reality workload. document can helpful planning tool (project data teams) preparing times heavier lighter workloads, well excellent document share future data users better understand waves data collection. one format create document. Figure 7.7 example one way visualize data collection timeline.\nFigure 7.7: Example data collection timeline\nParticipant flow diagramA participant flow diagram displays movement participants study, assisting researchers better understanding milestones eligibility, enrollment, final sample counts. diagrams helpful assessing study attrition reasons missing data can described diagram (Nahmias et al. 2022). randomized controlled trial studies, visualizations formally referred CONSORT (Consolidated Standards Reporting Trials) diagrams, (Schulz et al. 2010) seen Figure 7.8 (“CONSORT 2010 Flow Diagram” n.d.). provide means understand participants randomized assigned treatment groups. can imagine though, diagram created least one wave data collected, must updated waves collected. participant tracking database, discuss Chapter 9, inform creation diagram.\nFigure 7.8: 2010 CONSORT flow diagram template\nInstrumentsActual copies instruments can included supplemental documentation. includes copies surveys, assessments, forms, forth. can also include technical documents associated instruments measures (.e. technical document assessment publication associated measure used). Sometimes researchers annotate instruments show items named coded.Flowchart data collection instruments/screenersYou can also include flowcharts participants provided assigned different instruments screeners help users better understand issues missing data (Tourangeau 2015).\nFigure 7.9: Flowchart ECLS-K:2011 kindergarten assessment\nConsent formsConsent forms(see Chapter 10) can also added addendum research protocols give insight information provided study participants.Related publicationsYou may also choose attach publications come data addendum protocol.","code":""},{"path":"document.html","id":"sop","chapter":"7 Documentation","heading":"7.2.6 Standard operating procedures","text":"research protocol provides summary information decisions procedures associated project, still need documents inform procedures actually implemented daily basis (NUCATS n.d.). Standard operating procedures (SOPs) provide set detailed instructions routine tasks decision making processes. recall Chapter 5, every step added data collection workflow added SOP details fleshed . SOP type data collecting (.e., survey, assessments, observations), also SOPs decisions processes need repeated reproducible manner followed specific way maintain compliance (Hollmann et al. 2020). Many decisions laid protocol detailed SOP. Examples data management procedures include SOP provided Figure 7.10. Additional project management tasks recruitment procedures, personnel training, data collection scheduling, -field data collection routines, also documented SOPs, ensuring fidelity implementation project procedures.\nFigure 7.10: Examples data management processes decisions develop SOP \naddition giving staff instruction perform tasks, SOPs also create transparency practices, allow continuity staff turnover go leave, create standardization procedures, last, SOP include versioning information, allow accurately report changes procedures throughout project. want create template used consistently across procedures, staff build SOPs.\nFigure 7.11: Standard operating procedure minimal template\ndeveloping SOP template, like one Figure 7.11, begin general information scope purpose procedure, well relevant tools, terminology, documentation. provides context user gives background use interpret SOP. next section SOP template, procedures, lists steps order. step provides name staff member/s associated activity ensure ambiguity. step detailed possible hand SOP new staff member background process confident can implement procedure little trouble. Specifics names files links locations, names contacts, methods communication (e.g., email vs instant message), forth included. Additions screenshots, links SOPs workflow diagrams, even links tutorials can also embedded. Last, time revisions made SOP, clarifying information update added revision section new version SOP saved. allows keep track changes made time, including made made .Template Resources","code":""},{"path":"document.html","id":"document-dataset","chapter":"7 Documentation","heading":"7.3 Dataset Level","text":"next type documentation applies solely datasets includes information data contain related. also captures things planned transformations data, potential issues aware , alterations data. addition helpful descriptive documentation, huge reason creating dataset documentation authenticity. Datasets go many iterations processing can result multiple versions dataset (CESSDA Training Team 2017; UK Data Service 2023b). Preserving data lineage tracking transformations errors found key ensuring know data come , processing already completed, using correct version data.dataset-level documentation created documentation phase talk timing review document.","code":""},{"path":"document.html","id":"readme","chapter":"7 Documentation","heading":"7.3.1 Readme","text":"Readme plain text document contains information files. stem field computer science now prevalent research world. documents way convey pertinent information collaborators simple, frills manner. Readmes can used many different ways cover three ways often used data management.conveying information colleagues\nexample study participant reaches project coordinator let know entered incorrect ID survey. project coordinator downloads raw data file cleaned data manager instance, also create file named “readme.txt” contains information saved alongside file raw data folder. way data manager goes retrieve file, see Readme included know review document first.\nexample study participant reaches project coordinator let know entered incorrect ID survey. project coordinator downloads raw data file cleaned data manager instance, also create file named “readme.txt” contains information saved alongside file raw data folder. way data manager goes retrieve file, see Readme included know review document first.conveying steps process (sometimes also called setup file)\nmay times specific data pipeline reporting process requires multiple steps, opening different files running different scripts. information can go SOP, programmatic type process completed using series scripts, might easiest put simple file named “readme_setup.txt” folder scripts someone can easily open file see need run.\nmay times specific data pipeline reporting process requires multiple steps, opening different files running different scripts. information can go SOP, programmatic type process completed using series scripts, might easiest put simple file named “readme_setup.txt” folder scripts someone can easily open file see need run.providing information set files directory\ncan helpful add Readmes top directories sharing data internally colleagues, sharing files external repository. can provide information datasets available directory pertinent information datasets, including datasets related can linked, information associated different versions, definitions common prefixes suffixes used datasets, instrument response rates. Figure 7.12 example readme can used describe data sources shared project repository (Neild, Robinson, Agufa 2022).\ncan helpful add Readmes top directories sharing data internally colleagues, sharing files external repository. can provide information datasets available directory pertinent information datasets, including datasets related can linked, information associated different versions, definitions common prefixes suffixes used datasets, instrument response rates. Figure 7.12 example readme can used describe data sources shared project repository (Neild, Robinson, Agufa 2022).\nFigure 7.12: Institute Education Sciences example Readme conveying information files directory\nTemplate Resources","code":"  - ID 5051 entered incorrectly. Should be 5015.\n  - ID 5089 completed the survey twice\n    - First survey is only partially completed  Step 1: Run the file 01_clean_data.R to clean the data  \n  Step 2: Run the file 02_check_errors.R to check for errors  \n  Step 3: Run the file 03_run_report.R to create report "},{"path":"document.html","id":"change","chapter":"7 Documentation","heading":"7.3.2 Changelog","text":"changelog record versions data code (UK Data Service 2023d; Wilson et al. 2017). automatic ways track data code programs Git GitHub, education field researchers often work human subjects identifiable data, users often keeping study data, active project, remote repository. Instead, data usually kept institution-approved storage location. Even storage location versioning Box SharePoint, unless users able add contextual messages changes made saving versions (like commit message Git), users still want keep changelog.changelog provides data lineage, allowing user understand data originated well transformations made data. also supports data confidence, allowing user understand version data currently using see recent versions created .simplest form changelog contain following:file name (versioned consistently)date file createdA description dataset (including changes made compared previous version)also include additional information made change link code used transform data.\nFigure 7.13: Example changelog clean student survey data file\nchangelogs likely created data capture data cleaning phases life cycle data transformations begin happening, can updated point needed.Template Resources","code":""},{"path":"document.html","id":"data-cleaning-plan","chapter":"7 Documentation","heading":"7.3.3 Data cleaning plan","text":"data cleaning plan written proposal outlining plan transform raw data clean, usable data. document contains code technical skills dependent. data cleaning plan created dataset plan collect (e.g., student survey, student assessment, teacher survey, district student demographic data). document lays intended transformations raw dataset, allows team member provide feedback data cleaning process.document can started documentation phase, likely continue updated throughout study, especially start digging collected raw data seeing additional transformations needed. Typically person responsible cleaning data write data cleaning plans, documents can brought planning meeting allowing team members, PIs, provide input plan. ensures everyone agrees transformations performed. finalized, data cleaning plan serves guide cleaning process. addition changelog, data cleaning plan (well syntax used) provides documentation necessary assess data provenance, historical record data file’s journey.writing data cleaning plans, can helpful team agreed upon general norms constitutes clean dataset help ensure datasets cleaned formatted consistently. standards can written stored central team project location referral used guide process write data cleaning plan. review types transformations consider adding type norms document Chapter 13.\nFigure 7.14: simplistic data cleaning plan\n","code":""},{"path":"document.html","id":"variable-level","chapter":"7 Documentation","heading":"7.4 Variable Level","text":"last category documentation variable-level documentation. think data management, think likely first type documentation pops people’s minds. documentation tells us pertinent information variables datasets: variable names, descriptions, types, allowable values. variable-level documentation often used interpret existing datasets, can also serve many vital purposes including guiding construction data collection instruments, assisting data cleaning, validating accuracy data (Lewis 2022a). discuss throughout chapters book.","code":""},{"path":"document.html","id":"document-dictionary","chapter":"7 Documentation","heading":"7.4.1 Data dictionary","text":"data dictionary rectangular formatted collection names, definitions, attributes variables dataset (Bordelon 2023; Gonzales, Carson, Holmes 2022; UC Merced Library n.d.). document useful created documentation phase used throughout study planning interpretation purposes (see Figure 7.15) (Lewis 2022a; Bochove, Alper, Gu n.d.).\nFigure 7.15: many uses data dictionary\ndocument structured similar dataset, variable names first row (Broman Woo 2018). several necessary fields include document, well several optional fields (see Figure 7.16) (Johns Hopkins Institute Clinical Translational Research 2020).\nFigure 7.16: Fields include data dictionary\n","code":""},{"path":"document.html","id":"creating-a-data-dictionary-for-an-original-data-source","chapter":"7 Documentation","heading":"7.4.1.1 Creating a data dictionary for an original data source","text":"collecting data original source, things helpful creating data dictionaries:style guide already created\ntalk style guides Chapter 8, document provide team project standards naming variables coding response values.\ntalk style guides Chapter 8, document provide team project standards naming variables coding response values.Documentation measures\ncollecting data using existing measures (.e. existing scales, existing standardized assessments), want collect documentation measures technical documents copies instruments. want documentation provide information :\nitems make measures/scales/assessment? exact wording items?\nitems coded? allowable values?\ncalculations/scoring/reverse coding needed?\nitems entered scoring program exported, variables exported?\n\nSee Figure 7.17 example information pulled publication using Connor Davidson Resilience Scale (CD-RISC) (Connor Davidson 2003).\ncollecting data using existing measures (.e. existing scales, existing standardized assessments), want collect documentation measures technical documents copies instruments. want documentation provide information :\nitems make measures/scales/assessment? exact wording items?\nitems coded? allowable values?\ncalculations/scoring/reverse coding needed?\nitems entered scoring program exported, variables exported?\nitems make measures/scales/assessment? exact wording items?items coded? allowable values?calculations/scoring/reverse coding needed?items entered scoring program exported, variables exported?See Figure 7.17 example information pulled publication using Connor Davidson Resilience Scale (CD-RISC) (Connor Davidson 2003).data element standards plan use\nSee Section 10.3.1 overview existing data element standards\nSee Section 10.3.1 overview existing data element standards\nFigure 7.17: Pulling relevant information Connor Davidson Resilience Scale (CD-RISC)\nbuild one data dictionary instrument plan collect (e.g., student survey data dictionary, teacher survey data dictionary, student assessment data dictionary). measures/items instrument included data dictionary.build data dictionary, consider following:Item names\nvariable names meeting requirements laid style guide?\nfield standards dictate item named?\nvariable names meeting requirements laid style guide?field standards dictate item named?Item wording\nitems come existing scale, item wording match wording scale documentation? plan reword item?\nfield standards dictate item worded?\nitems come existing scale, item wording match wording scale documentation? plan reword item?field standards dictate item worded?Item value codes categorical items\nitems come existing scale, value coding (numeric values assigned response options) align coding laid scale documentation?\nitems come existing scale, value coding align requirements style guide?\nfield standards dictate items values coded?\nitems come existing scale, value coding (numeric values assigned response options) align coding laid scale documentation?items come existing scale, value coding align requirements style guide?field standards dictate items values coded?Additional Items\nadditional items make final dataset? Consider items derived, collected metadata, added . included data dictionary.\nIdentifiers (unique participant ids, rater ids)\nGrouping variables (treatment, cohort)\nDerived variables\nincludes variables team derives (e.g., mean scores, reverse coded variables, variable checks) well variables derived scoring programs (e.g., percentile ranks, grade equivalent scores)\n\nMetadata (Variables tool collects IPAddress, completion, language)\n\nadditional items make final dataset? Consider items derived, collected metadata, added . included data dictionary.\nIdentifiers (unique participant ids, rater ids)\nGrouping variables (treatment, cohort)\nDerived variables\nincludes variables team derives (e.g., mean scores, reverse coded variables, variable checks) well variables derived scoring programs (e.g., percentile ranks, grade equivalent scores)\n\nMetadata (Variables tool collects IPAddress, completion, language)\nIdentifiers (unique participant ids, rater ids)Grouping variables (treatment, cohort)Derived variables\nincludes variables team derives (e.g., mean scores, reverse coded variables, variable checks) well variables derived scoring programs (e.g., percentile ranks, grade equivalent scores)\nincludes variables team derives (e.g., mean scores, reverse coded variables, variable checks) well variables derived scoring programs (e.g., percentile ranks, grade equivalent scores)Metadata (Variables tool collects IPAddress, completion, language)items removed public data sharing (.e., personally identifiable information)demonstration purposes , data dictionary Figure 7.18 uses items Patterns Adaptive Learning Scales (PALS) (Midgley 2000). actual research study dictionary likely include many items variety measures.\nFigure 7.18: Example student survey data dictionary\nlast step creating data dictionary, every document create documentation phase, review document/s team.everyone agreement variables named, values coded, variable types?everyone agreement gets item?team want adjust question/item wording?data dictionary include everything team plans collect? items missing?\nadditional items added instruments later time points, adding fields data dictionary, “time periods available”, can really helpful future users understanding items may missing data certain time points.\nadditional items added instruments later time points, adding fields data dictionary, “time periods available”, can really helpful future users understanding items may missing data certain time points.","code":""},{"path":"document.html","id":"creating-a-data-dictionary-from-an-existing-data-source","chapter":"7 Documentation","heading":"7.4.1.2 Creating a data dictionary from an existing data source","text":"research study data gathered original data collection methods. may collecting supplemental external data sources organizations like school districts state departments education. possible, start gathering information external data sources early , documentation phase, begin adding information project data dictionary. Starting process early help prepare future data capture cleaning processes.data source public, may able easily find codebooks data dictionaries data source. , download sample data learn variables exist source formatted.data source non-public, request documentation ahead time partner (see Section 11.3).However, possible may able access information acquire actual data data capture phase. documentation provided along data, begin reviewing data ensure documentation matches see data. Integrate information project data dictionary.documentation provided important review data begin collecting questions allow build data dictionary.variables represent?\nwording items?\nwording items?received items?values represent?\nseeing full range values/categorical options item? range larger seeing?\nvalues data don’t make sense item?\nseeing full range values/categorical options item? range larger seeing?values data don’t make sense item?data types items currently? types ?situations questions easily answered without documentation may require detective work.Contact person originally collected data learn instrument data.Contact person cleaned data (cleaned) see transformations completed raw data.Request access original instruments review exact question wording, item response options, skip patterns, etc.Ultimately end data dictionary structured similarly one . may add additional fields help keep track changes (e.g., column old variable name column new variable name), transformations section may become verbose values assigned previously may align values prefer based style guide existing measures. Otherwise, data dictionary still constructed manner mentioned .","code":""},{"path":"document.html","id":"time-well-spent","chapter":"7 Documentation","heading":"7.4.1.3 Time well spent","text":"process described section manual, time consuming process. intentional. Building data dictionary information seeking journey take time understand dataset, create standardization items, plan data transformations. Spending time manually creating document collecting data prevents many potential errors time lost fixing data future. absolutely ways can automate creation data dictionary using existing dataset, time can imagine useful clean dataset confidently already verified accurate ready shared. However, mentioned , data dictionary much document shared alongside public dataset. tool guiding many processes research data life cycle.Template Resources","code":""},{"path":"document.html","id":"codebook","chapter":"7 Documentation","heading":"7.4.2 Codebook","text":"Codebooks provide descriptive, variable-level information well univariate summary statistics allow users understand contents dataset without ever opening . Unlike data dictionary, codebook created data collected cleaned value lies data interpretation data validation.codebook contains information overlaps data dictionary, summary document actually exists dataset (ICPSR 2011).\nFigure 7.19: Codebook content overlaps unique data dictionary\nFigure 7.20 example codebook United States Department Health Human Services (2022).\nFigure 7.20: Example codebook SCOPE Coach Survey\naddition excellent resource users review data without ever opening file, document may also help catch errors data range unexpected values appear.can create separate codebooks per dataset contained one document, clickable table contents. Unlike data dictionary, recommend creating manually, codebook created automated process. Automating codebooks save tons time, also reduce errors made manual entry. can use many tools create codebooks, including point click statistical programs SPSS, little programming knowledge can flexibly design codebooks using programs like R SAS. example, R programming language many packages create export codebooks variety formats existing dataset just running functions (Lewis 2023).Last, may notice review codebooks, many start several pages text, usually containing information project. comes time share data, ’s common people combine information research protocol Readme files, codebooks, rather sharing separate documents.Template Resources","code":""},{"path":"document.html","id":"metadata","chapter":"7 Documentation","heading":"7.5 Metadata","text":"last type documentation discuss metadata, created “prepare archiving” phase. depositing data repository, submit two types documentation, human-readable documentation, includes documents ’ve previously discussed, metadata. Metadata, data data, documentation meant processed machines serves purpose making files searchable (CESSDA Training Team 2017; Danish National Forum Research Data Management n.d.). Metadata aids cataloging, citing, discovering, retrieving data creation critical step creating FAIR data (GO FAIR n.d.; Logan, Hart, Schatschneider 2021; UK Data Service 2023a).part, additional work needed create metadata depositing data repository. simply created part depositing process (CESSDA Training Team 2017; University Iowa Libraries 2023). deposit data, repository may fill form contains descriptive (description project files), administrative (licensing ownership well technical information), structural (relationships objects) metadata (Cofield 2023; Danish National Forum Research Data Management n.d.). information form become metadata. Figure 7.21 example intake form Figshare repository (https://figshare.com/).\nFigure 7.21: Example intake metadata form Figshare repository, captured January 13, 2023\ncommon metadata elements (Dahdul 2023; Hayslett 2022) included Figure 7.22.\nFigure 7.22: Common metadata elements\nDepending repository, minimum, enter basic project-level metadata similar , may required option enter comprehensive information, project-level information covered research protocol. may also option enter additional levels metadata help make level searchable, file-level variable-level metadata (Gilmore, Kennedy, Adolph 2018; ICPSR 2023; LDbase n.d.). information needed metadata can gathered documents ’ve discussed earlier chapter.entered form, repository converts entries human-readable machine-readable, searchable formats XML (ICPSR 2023) JSON-LD. can see metadata looks like humans submitted. Figure 7.23 example ICPSR Open displays metadata information project page (Page, Lenard, Keele 2020). Notice even option download XML formatted metadata files one two standards (see Section 7.5.1) want well.\nFigure 7.23: Example metadata displayed ICPSR Open project page\nways metadata can gathered well. instance, variable-level metadata, rather users input metadata, repositories may create metadata deposited statistical data files contain inherent metadata (variable types labels) deposited documentation data dictionaries codebooks (ICPSR 2023).repository provides limited forms metadata entry, can also choose increase searchability files creating machine-readable documents. several tools help users create machine-readable codebooks data dictionaries findable search engines Google Dataset Search (Arslan 2019; Buchanan et al. 2021; USGS 2021).","code":""},{"path":"document.html","id":"metastandards","chapter":"7 Documentation","heading":"7.5.1 Metadata standards","text":"Metadata standards, typically field specific, establish common way describe data improves data interoperability well ability users find, understand, use data. Metadata standards can applied several ways (Bolam 2022; DDI Alliance 2023a).Formats: machine-readable format metadata ?Schema: fields recommended verses mandatory project, dataset, variable level metadata?Controlled vocabularies: controlled list terms used index retrieve data.Many fields chosen metadata standards adhere . fields, like psychology (Kline 2018), developing metadata standards, including formats, schemas, vocabularies grounded FAIR principles Schema.org schema (Schema.org 2023). Yet, Institute Education Sciences recognizes currently agreed upon metadata standards field education (Institute Education Sciences n.d.).\nFigure 7.24: sampling field metadata standards\ncan helpful see standards differ well overlap. DDI Alliance (2023b) put together table Figure 7.25 instance, mapping DDI Elements (vocabularies) Dublin Core, two commonly used standards.\nFigure 7.25: comparison DDI Version 2 standards Dublin Core standards\ncan see metadata comparison actually looks like download Dublin Core DDI 2.5 XML format metadata files ICPSR Open project saw (Page, Lenard, Keele 2020). can start see differences similarities across standards.\nFigure 7.26: Metadata comparison AERA Open project\nplan archive data, first check repository see follow standards. example, OSF (Gueguen 2023) Figshare (Figshare 2023) repositories currently use DataCite schema , ICPSR uses DDI standard (ICPSR 2023). repository use certain standards, work ensure metadata adheres standards. repositories may even provide curation support free fee. mentioned earlier, depending repository, adding metadata project may require additional work part. repository may simply enter information form convert information .standards provided repository plan create metadata, can choose standard works . Oftentimes researchers may choose pick general standard DataCite Dublin Core (University Iowa Libraries 2023), field education, researchers least familiar DDI standard another good option. Remember, choose adhere standard, decision documented data management plan.","code":""},{"path":"document.html","id":"wrapping-it-up","chapter":"7 Documentation","heading":"7.6 Wrapping it up","text":"point head might spinning amount documents ’ve covered. ’s important understand document discussed provides unique meaningful purpose, don’t create every document listed. data management walk fine line creating sufficient documentation, spending working hours perfecting documenting every detail project. Choose documents help record structure processes best way project also giving grace stop documents “good enough”. document create well organized well maintained improve data management workflow, decrease errors, enhance understanding data.","code":""},{"path":"style.html","id":"style","chapter":"8 Style guide","heading":"8 Style guide","text":"\nFigure 8.1: Style guide research project life cycle\nstyle guide provides general agreed upon rules formatting information (“Style Guide” 2023). mentioned previous Chapter 7, style guides can created standardize procedures variable naming, variable value coding, file naming, file versioning, file structure, even coding practices.Style guides create standardization within across projects. benefits using consistently include:Creating interoperability: allows data easily combined compared across forms time.Improving interpretation: Consistent clear structure, naming, coding allows files variables findable understandable humans computers.Increasing reproducibility: organization file paths, file naming, variable naming constantly change undermines reproducibility data management analysis code written.Style guides can created individual projects, can also created team level, applied across projects. importantly, created project kicks can implement soon project begins. team-wide style guide already created, likely want create project-level style guide planning phase can begin setting directory structures file naming standards start creating saving project-related files.Style guides can housed one large document, table contents used reference section, can created separate documents. Either way, style guides stored central location easily accessible team members (team project wiki), team members trained, periodically retrained, style guide ensure adherence rules. team members consistently implementing style guide, benefits guide lost.remainder chapter, spend time reviewing good practices rules add style guides following purposes:Structuring directoriesNaming filesNaming variablesAssigning variable valuesStyling syntax filesWhile best practices provided , ultimately rules choose add style guide chosen based practices work best projects team. Whatever rules settle , write style guide everyone following rules within across projects.","code":""},{"path":"style.html","id":"general-good-practices","chapter":"8 Style guide","heading":"8.1 General good practices","text":"dive particular types style guides, things understand computers read names order understand “” behind practices.Avoid spaces.\napplications (like Windows) recognize spaces, command line operations operating systems still support best avoid together. Furthermore, can often break URL shared\nunderscore _ hyphen - good delimiters use place spaces\nworth noting _ can difficult read file paths shared links underlined denote path clickable (example sharing SharePoint link document)\n\napplications (like Windows) recognize spaces, command line operations operating systems still support best avoid together. Furthermore, can often break URL sharedThe underscore _ hyphen - good delimiters use place spaces\nworth noting _ can difficult read file paths shared links underlined denote path clickable (example sharing SharePoint link document)\nworth noting _ can difficult read file paths shared links underlined denote path clickable (example sharing SharePoint link document)exception _ -, avoid special characters\nExamples include limited ?, ., *, \\, /, +, ', &, \"\nComputers assign specific meaning many special characters\nExamples include limited ?, ., *, \\, /, +, ', &, \"Computers assign specific meaning many special charactersThere several existing naming conventions can choose add style guide. Different naming conventions may work better different purposes. Using conventions help consistent delimeters capitalization makes names human-readable also allows computer read search names easier.\nPascal case (ScaleSum)\nSnake case (scale_sum)\nCamel case (scaleSum)\nKebab case (scale-sum)\nTrain case (Scale-Sum)\nPascal case (ScaleSum)Snake case (scale_sum)Camel case (scaleSum)Kebab case (scale-sum)Train case (Scale-Sum)Character length matters. Computers unable read names surpass certain character length. applies file paths, file names, variable names. Considerations type limit reviewed .","code":""},{"path":"style.html","id":"directory-structure","chapter":"8 Style guide","heading":"8.2 Directory structure","text":"deciding structure project directories (organization operating systems folders files), several things want consider.structuring folders:First, consider organizing directory hierarchical folder structure clearly delineate segments projects improve searchability\nalternative using folder structure using metadata tagging organize search files (Cakici 2017; Fuchs Kuusniemi 2018; Krishna 2018)\nalternative using folder structure using metadata tagging organize search files (Cakici 2017; Fuchs Kuusniemi 2018; Krishna 2018)creating folder structure, strike balance deep shallow structure\nshallow leads many files one folder difficult sort \ndeep leads many clicks get one file, plus file paths can max many characters. file path includes full length folders file names\nexample file path 69 characters W:\\team\\projecta\\data\\wave1\\student\\survey\\projecta-w1-stu-svy-raw.csv\n\nExamples file path limits:\nSharePoint/OneDrive path limit 400 characters (Microsoft n.d.)\nWindows path limit 260 characters (Ashcraft 2022)\n\nshallow leads many files one folder difficult sort throughToo deep leads many clicks get one file, plus file paths can max many characters. file path includes full length folders file names\nexample file path 69 characters W:\\team\\projecta\\data\\wave1\\student\\survey\\projecta-w1-stu-svy-raw.csv\nexample file path 69 characters W:\\team\\projecta\\data\\wave1\\student\\survey\\projecta-w1-stu-svy-raw.csvExamples file path limits:\nSharePoint/OneDrive path limit 400 characters (Microsoft n.d.)\nWindows path limit 260 characters (Ashcraft 2022)\nSharePoint/OneDrive path limit 400 characters (Microsoft n.d.)Windows path limit 260 characters (Ashcraft 2022)Create folders specific enough can limit access\nexample want limit user access folders hold Personally Identifiable Information (PII)\nprotect files don’t want others accidentally edit (example clean datasets), also consider making files “read ”\nexample want limit user access folders hold Personally Identifiable Information (PII)protect files don’t want others accidentally edit (example clean datasets), also consider making files “read ”Decide want “archive” folder move old files want leave previous versions folderWhen naming folders:Consider setting character limit folder names (reduce problems hitting path character limits)Make folder names meaningful easy interpretNever use spaces folder names\nUse _ - separate words\nUse _ - separate wordsWith exception - _, don’t use special characters folder namesBe consistent delimiters capitalization. Follow existing naming convention (mentioned ).Example directory structure style guideExample directory structure created using style guide","code":"1. All project directories follow this hierarchical metadata structure  \n    - Level 1: Name of project  \n    - Level 2: Life cycle folders  \n    - Level 3: Data collection wave folders (if relevant)  \n    - Level 4: Participant folder (if relevent)\n    - Level 5: Specific content folder  \n    - Level 6: Archive folders  \n2. All folders should be named according to these rules  \n    - Meaningful name but no longer than 20 characters  \n    - No spaces or special characters in folder names  \n    - Only use lower case letters  \n    - Use `-` to separate words  \n3. All previous versions of files must be placed into their respective \"archive\" folder\n    - A changelog should be placed in each \"archive\" folder to document changes between document versions                               levelName\n1  project-new                          \n2   ¦--intervention                     \n3   ¦   °--cohort-1                     \n4   ¦       °--coaching_materials       \n5   ¦           °--archive              \n6   ¦               °--changelog.txt    \n7   ¦--project-mgmt                     \n8   ¦   °--cohort-1                     \n9   ¦       °--scheduling-materials     \n10  ¦           °--archive              \n11  ¦               °--changelog.txt    \n12  ¦--documentation                    \n13  ¦   ¦--sops                         \n14  ¦   ¦   °--archive                  \n15  ¦   ¦       °--changelog.txt        \n16  ¦   °--data-dictionaries            \n17  ¦       °--archive                  \n18  ¦           °--changelog.txt        \n19  ¦--data                             \n20  ¦   °--cohort-1                     \n21  ¦       °--student                  \n22  ¦           °--survey               \n23  ¦               °--archive          \n24  ¦                   °--changelog.txt\n25  °--tracking                         \n26      °--cohort-1                     \n27          ¦--participant-database     \n28          ¦   °--archive              \n29          ¦       °--changelog.txt    \n30          °--parent-consents          "},{"path":"style.html","id":"file-naming","chapter":"8 Style guide","heading":"8.3 File naming","text":"\nFigure 8.2: xkcd comic naming files\nxkcd (n.d.) aptly points Figure 8.2, many us pretty bad naming files consistent usable way. often rush save files maybe don’t consider unclear file names future users (including ).file names alone able answer questions :documents?documents created?document recent version?file naming style guide helps us name files way allows us answer questions. can one overarching file naming guide, may file naming guides different purposes need different organizational strategies (example one naming guide project meeting notes, another naming guide project data files). Let’s walk several conventions consider naming files.Make names descriptive (user able understand contents file without opening )Never use spaces words\nUse - _ separate words.\nUse - _ separate words.exception _ -, never use special charactersBe consistent delimiters capitalization. Follow existing naming convention.Consider limiting number allowable characters prevent hitting path limit (mentioned )use / dates format consistently. beneficial format dates using ISO 8601 standard one two ways:\nYYYY-MM-DD YYYYMMDD\nfirst way adds characters variable names, also may clearer users interpret. Either date formats sortable.\nYYYY-MM-DD YYYYMMDDWhile first way adds characters variable names, also may clearer users interpret. Either date formats sortable.versioning files, pick format add style guide\nplan version using number, consider left padding 0 single digit numbers keep file name length grows (v01, v02).\nmentioned Chapter 7, possible version programatically using tools like Git GitHub. However, tools always practical education research. practical means versioning may manually version files track changes changelog.\nplan version using number, consider left padding 0 single digit numbers keep file name length grows (v01, v02).mentioned Chapter 7, possible version programatically using tools like Git GitHub. However, tools always practical education research. practical means versioning may manually version files track changes changelog.files need run sequential order, add order number beginning file name, leading zeros ensure proper sorting (01_, 02_)Choose abbreviations /consistent terms use common names/phrases add style guide (student = stu).\nhelps reduce file name character lengths also creates standardized, searchable metadata, can allow easily, programmatically retrieve files (example, retrieve files containing phrase “stu_obs_raw”).\nhelps reduce file name character lengths also creates standardized, searchable metadata, can allow easily, programmatically retrieve files (example, retrieve files containing phrase “stu_obs_raw”).Keep redundant metadata file name\nreduces confusion ever move file different folder send file collaborator. also makes files searchable.\nexample, always put data collection wave file name, even file currently housed specific wave folder. always put project name file name, even file currently housed project folder.\nreduces confusion ever move file different folder send file collaborator. also makes files searchable.example, always put data collection wave file name, even file currently housed specific wave folder. always put project name file name, even file currently housed project folder.Choose order file name metadata (e.g., project -> time -> participant -> measure)Example file naming style guideExample file names created using style guide","code":"1. Never use spaces between words\n2. Never use special characters\n3. Use _ to separate words\n4. Only use lower case letters\n5. Keep names under 35 characters\n6. Use the following metadata file naming order:\n  - Order of use (if relevant–and always add a 0 before single digits)\n  - Project\n  - Cohort/Wave (if relevant)\n  - Participant\n  - Measure\n  - Further description\n  - Date (always add)\n  - Version (if necessary)\n7. Format dates as YYYY-MM-DD\n8. If there are multiple versions of a document on the same date, version using v# with a leading 0.\n9. Use the following abbreviations\n  - student = stu\n  - survey = svy\n  - wave = w\n  - project math efficacy = meme_stu_svy_sop_2022-08-01.docx\nme_w1_stu_svy_raw_2022-11-03.csv\nme_w1_stu_svy_cleaning_syntax_2023-01-22_v01.R\nme_w1_stu_svy_cleaning_syntax_2023-01-22_v02.R"},{"path":"style.html","id":"varname","chapter":"8 Style guide","heading":"8.4 Variable naming","text":"style guide necessary document start create data dictionaries. several considerations review developing variable naming style guide. broken two types rules, non-negotiable requirements really included style guide (follow rules run serious problems interpretation humans machines), best practices suggestions recommended required.Mandatory:Don’t name variable keywords functions used programming language (, , repeat) (R Core Team 2023; Stangroom 2019)Set character limit\nstatistical programs limit variable name characters\nSPSS 64\nStata 32\nSAS 32\nMplus 8\nR 10,000\n\nsaid, limit 8 characters based fact one future user may use program like Mplus. Consider balance character limit interpretation. difficult make good human-readable variable names 8 characters. much easier make 32. majority users using program limit 32 . one potential Mplus user, can always rename variables specific analysis.\nstatistical programs limit variable name characters\nSPSS 64\nStata 32\nSAS 32\nMplus 8\nR 10,000\nSPSS 64Stata 32SAS 32Mplus 8R 10,000With said, limit 8 characters based fact one future user may use program like Mplus. Consider balance character limit interpretation. difficult make good human-readable variable names 8 characters. much easier make 32. majority users using program limit 32 . one potential Mplus user, can always rename variables specific analysis.Use variable name across time project\nitem named anx1 fall, name item anx1 spring\nitem named anx1 fall, name item anx1 springDon’t use spaces special characters (except_), allowed programs\nEven - allowed programs R SPSS can mistaken minus sign\n. allowed R SPSS allowed Stata ’s best avoid using \nEven - allowed programs R SPSS can mistaken minus signWhile . allowed R SPSS allowed Stata ’s best avoid using itDo start variable name number. allowed many statistical programs.variable names unique\nabsolutely applies variables within dataset, also apply variables across datasets within project. reason , point may merge data across forms end identical variable names (programs allow).\n, example collect student gender survey also collect student gender school records, differentiate two (s_gender d_gender)\nabsolutely applies variables within dataset, also apply variables across datasets within project. reason , point may merge data across forms end identical variable names (programs allow)., example collect student gender survey also collect student gender school records, differentiate two (s_gender d_gender)substantively change item (substantive wording response options change) least one round data collected, version variable names order reduce errors interpretation.\nexample revised anx1 becomes anx1_v2\nexample revised anx1 becomes anx1_v2Suggested:Names meaningful\nInstead naming gender q1, name gender\nvariable part scale, consider using abbreviation scale plus scale item number (anx1, anx2, anx3)\nallow easily associate item scale, also allows programatically select manipulate scale items (example, sum items start “anx”)\n\nInstead naming gender q1, name genderIf variable part scale, consider using abbreviation scale plus scale item number (anx1, anx2, anx3)\nallow easily associate item scale, also allows programatically select manipulate scale items (example, sum items start “anx”)\nallow easily associate item scale, also allows programatically select manipulate scale items (example, sum items start “anx”)used question/scale , consider keeping variable name across projects. can useful ever want combine data across projects.consistent delimiters capitalization. Follow existing naming convention. programming languages case sensitive consider choosing convention feasible workflow.\nSnake case (scale_sum) – preferred method variable names\nKebab case (scale-sum) – don’t use variable names\nTrain case (Scale-Sum) – don’t use variable names\nSnake case (scale_sum) – preferred method variable namesKebab case (scale-sum) – don’t use variable namesTrain case (Scale-Sum) – don’t use variable namesConsider denoting reverse coding variable name reduce confusion (anx1_r)Choose abbreviations standard phrases use across variables. Using controlled vocabularies improves interpretation also makes data exploration manipulation easier (Riederer 2020).\nmean = mean\nscaled score = ss\npercentile rank = pr\nmean = meanscaled score = sspercentile rank = prInclude indication measure variable name (example prefix) always know instrument item came . can also help unique variable name requirement .\ns = student self-report\nt = teach report students\ns_anx1, t_conf2\ns = student self-reportt = teach report studentss_anx1, t_conf2Example variable naming style guideExample variable names created using style guide","code":"1. Use snake case\n2. Keep names under 32 characters\n3. Use meaningful variable names\n4. If part of a scale, use scale abbreviation plus item number from the scale (not order number)\n5. Include an indication of the measure as a prefix in the variable name\n  - student self-report survey = s_\n  - teacher self-report survey = t_\n  - district student level data = d_\n6. Denote reverse coded variables using suffix `_r`s_anx1\ns_anx1_r\ns_gender\nd_gender\nt_stress5"},{"path":"style.html","id":"time","chapter":"8 Style guide","heading":"8.4.1 Time","text":"moving one last consideration variable names. data longitudinal, may need add rules accounting time variable names well.Depending plan combine data time, two different ways account time.Concatenate time variable names. plan merge data across time wide format (see Chapter 3). reason need concatenate time variable names variable names repeat (anx1 wave 1, anx1 wave 2). remember guidelines , variable names dataset must unique. order create unique variable names correctly interpret items given, add time variable names. variables assign time linking variables (student unique identifier, teacher unique identifier, ). variables need stay identical linking purposes appear data merging.Create time variables add data. plan append data time long format (see Chapter 3). Appending data long format requires additional work terms variable naming. discussed Chapter 3, actually want variables identically formatted named across time appending. , order differentiate items asked, add new variable time wave add appropriate value row.active project, actually best combine data store datasets distinct files either ready internally use data ready publicly share data (preparing archiving phase). time can make decision best way combine data (need combine ), programatically add time variable names (necessary) (Reynolds, Schatschneider, Logan 2022). Waiting combine data benefits:variables named consistently time (time component added) allows easily reuse data collection data capture tools, well cleaning code, wave.Storing files separately prevents potentially wasting time combining data way ends actually useful wasting time merging datasets later need re-combined find error individual dataset point.","code":""},{"path":"style.html","id":"time-in-variable-names","chapter":"8 Style guide","heading":"8.4.1.1 Time in variable names","text":"combining datasets across time happen early project, helpful consider early might combine data future. plan potentially merge data wide format, can helpful go ahead plan rule adding time variable names, add rule style guide. Just abundantly clear guide time component added datasets combined.right wrong way assign time variable names necessarily. Just make sure continue follow rules (never starting variable name number). options adding time sample variable, s_gender.prefix suffix generic abbreviation, w1 wave 1, added delimiter _\nw1_s_gender s_gender_w1\nw1_s_gender s_gender_w1As prefix suffix meaningful abbreviation, f21 fall 2021, added delimiter (_)\nf21_s_gender s_gender_f21\nf21_s_gender s_gender_f21One options delimiter\nw1s_gender s_genderw1\nw1s_gender s_genderw1As number embedded variable certain location, instance, existing prefix s student survey\ns1_gender, s2_gender\ns1_gender, s2_genderWhile first second method add additional characters variable name, also benefits adding time ways. First, can easier visually spot interpret time component separated delimiter. Second, adding time standalone component also allows easily, programmatically, manipulate time component variable. gives flexibility working data, especially selecting variables restructuring datasets.","code":""},{"path":"style.html","id":"value-coding","chapter":"8 Style guide","heading":"8.5 Value Coding","text":"addition naming variables standardized way, variables values also need added consistently. Value codes apply categorical variable. may numeric categorical values associated labels (e.g., “” = 1) may character categorical values associated labels (e.g., “” = ‘n’).First, using pre-existing measure, assign values labels manner technical documentation tells assign codes. important derivations need make later based measures. Otherwise, assigning values labels. guidelines assigning codes labels (well examples apply guidelines) .Values must unique\n: Assign “yes” = 1 “” = 0\nDon’t: Assign “yes” = 1 “” = 1\n: Assign “yes” = 1 “” = 0Don’t: Assign “yes” = 1 “” = 1Values must consistent within variable\n: gender assign “male” = ‘m’\nDon’t: gender allow “male” = ‘m’ ‘M’ ‘Male’ ‘male’\n: gender assign “male” = ‘m’Don’t: gender allow “male” = ‘m’ ‘M’ ‘Male’ ‘male’Values must consistent across time\n: Assign anx1 values “yes” = 1 “” = 0 wave 1 wave 2\nDon’t: Assign anx1 values “yes” = 1 “” = 0 wave 1 values “yes” = 1 “” = 2 wave 2\n: Assign anx1 values “yes” = 1 “” = 0 wave 1 wave 2Don’t: Assign anx1 values “yes” = 1 “” = 0 wave 1 values “yes” = 1 “” = 2 wave 2Values consistent across project\n: Assign “yes” = 1 “” = 0 value yes/items\nDon’t: Assign “yes” = 1 “” = 0 variables, “yes” = 1 “” = 2 others\nUnless pre-existing measure determines variables coded\n\n: Assign “yes” = 1 “” = 0 value yes/itemsDon’t: Assign “yes” = 1 “” = 0 variables, “yes” = 1 “” = 2 others\nUnless pre-existing measure determines variables coded\nUnless pre-existing measure determines variables codedOrder Likert-type scale response options logical way\n: Assign “Strongly Disagree” = 1; “Disagree” = 2; “Agree” = 3; “Strongly Agree” = 4\nDon’t: Assign “Strongly Disagree” = 1; “Disagree” = 3; “Agree” = 4; “Strongly Agree” = 2\nUnless pre-existing measure tells code variables different way\n\n: Assign “Strongly Disagree” = 1; “Disagree” = 2; “Agree” = 3; “Strongly Agree” = 4Don’t: Assign “Strongly Disagree” = 1; “Disagree” = 3; “Agree” = 4; “Strongly Agree” = 2\nUnless pre-existing measure tells code variables different way\nUnless pre-existing measure tells code variables different way","code":""},{"path":"style.html","id":"missing-value-coding","chapter":"8 Style guide","heading":"8.6 Missing Value Coding","text":"little agreement missing data coded (White et al. 2013). essentially two options.can choose leave missing values blank.\nBenefits chance extreme values mistaken actual values\nconcern method way discern value truly missing, potentially erased accident skipped data entry (Broman Woo 2018)\nalso consideration statistical programs allow blank values (e.g., MPlus), therefore missing values need assigned point. Yet, mentioned earlier chapter, best make decisions based one potential use case. better make decisions based reasonable way assign missing values general audience.\nBenefits chance extreme values mistaken actual valuesThe concern method way discern value truly missing, potentially erased accident skipped data entry (Broman Woo 2018)also consideration statistical programs allow blank values (e.g., MPlus), therefore missing values need assigned point. Yet, mentioned earlier chapter, best make decisions based one potential use case. better make decisions based reasonable way assign missing values general audience.option define missing values add data. may one consistent value (.e., word NULL letters NA, may extreme numeric values -999 -98)\nbenefit method using defined values allows specify distinct reasons missing data (e.g., -97 = Applicable, -98 = Skipped) important study.\nAnother benefit removes uncertainty blank cells. value filled, now certain value deleted skipped data entry.\nbiggest problem can occur method either extreme values mistaken actual values (someone misses documentation missing values), use value match variable type, introduce new variable type issues (e.g., NULL used numeric variable, variable longer numeric)\nbenefit method using defined values allows specify distinct reasons missing data (e.g., -97 = Applicable, -98 = Skipped) important study.Another benefit removes uncertainty blank cells. value filled, now certain value deleted skipped data entry.biggest problem can occur method either extreme values mistaken actual values (someone misses documentation missing values), use value match variable type, introduce new variable type issues (e.g., NULL used numeric variable, variable longer numeric)Whichever method choose, ultimately just make sure adhere guidelines:decide fill defined missing values, use values match variable type (e.g., numeric missing values numeric variables) (Tourangeau 2015; ICPSR n.d.)\nsay , however, merit using text define missing values numeric variables prevent incorrect use missing values. try run mean variable, immediately notified possible variable stored character (string) column. care different types missingness, easily choose change missing values blank. However, care types missingness want keep included variable, need match variable type.\nsay , however, merit using text define missing values numeric variables prevent incorrect use missing values. try run mean variable, immediately notified possible variable stored character (string) column. care different types missingness, easily choose change missing values blank. However, care types missingness want keep included variable, need match variable type.use numeric values, use extreme values actually occur dataUse values consistently within across variables\nFigure 8.3: Missing values assigned ECLS-K:2011 data file\n","code":""},{"path":"style.html","id":"coding","chapter":"8 Style guide","heading":"8.7 Coding","text":"team plans clean data using code, can helpful create coding style guide. style guide can tailored specific language staff use (R Stata), can written generically apply coding language staff use clean data. small sampling good coding practices consider adding guide. looking guides specific language, can helpful google existing style guides language.Consider building implementing coding templates (Daskalova n.d.; Farewell 2018)\nTemplates can standardize format syntax files (using standard headers break code)\nalso standardize summary information provided beginning syntax (code author, project name, date created)\nTemplates can standardize format syntax files (using standard headers break code)also standardize summary information provided beginning syntax (code author, project name, date created)Use comments throughout code clearly explain purpose code chunk\nformat comments dependent coding language\nR uses #\nSPSS Stata use *\n\nformat comments dependent coding language\nR uses #\nSPSS Stata use *\nR uses #SPSS Stata use *Improve code readability using (Wickham n.d.; San Martin, Rodriguez-Ramirez, Suzuki 2023)\nspaces\nindentation\nsetting line limit code (e.g., 80 characters)\nspacesindentationsetting line limit code (e.g., 80 characters)Use relative file paths reproducibility\nSetting absolute file paths syntax reduces reproducibility future users may different file paths. important set file paths relative directory working (Wickham Grolemund 2017).\nSetting absolute file paths syntax reduces reproducibility future users may different file paths. important set file paths relative directory working (Wickham Grolemund 2017).create objects program (like R Python), consider adding object naming rules similar variable naming rules\nspaces object names\nspecial characters except _ separate words\nnames existing program keywords (, , etc.)\nspaces object namesNo special characters except _ separate wordsNo names existing program keywords (, , etc.)Reduce duplication, improve efficiency, increase ability troubleshoot errors using functions, loops, macros repetitive code chunksRecord session information future users\nRecord version information well operating system information relevant code increase reproducibility code\nRecord version information well operating system information relevant code increase reproducibility code","code":""},{"path":"track.html","id":"track","chapter":"9 Data Tracking","heading":"9 Data Tracking","text":"\nFigure 9.1: Tracking research project life cycle\nproject want able answer progress summary questions recruitment data collection activities.many participants consented study? many lost study ?much progress made cycle data collection? much data left collect?many forms collect cycle missing data forms?Questions like arise many times throughout study project coordination purposes, well external progress reporting publication purposes. Yet, answer questions? dig papers, search emails, download progress data, time need answer question status project activities? better solution track project activities participant tracking database.participant tracking database essential component project management data management. database contains study participants, relevant study information, well tracking information completion project milestones. database two underlying purposes.serve roster study participants “master key” (Pacific University Oregon 2014) houses identifying participant information well assigned unique study identifiers.aid project coordination reporting, tracking movement participants well completion milestones throughout study.database considered single source truth concerning everything happened throughout duration project. time participant consents participate, drops study, changes name, completes data collection measure, provided payment, moves locations, project coordinator, designated team member, updates information one location. Tracking administrative information one database, rather across disparate spreadsheets, emails, papers, ensures always one definitive source refer seeking answers sample project activities.Note \nwant reiterate single source truth concept. Information often coming multiple sources (e.g., data collectors field, emails project coordinators teachers, conversations administrators). important train team relevant contact information (e.g., name change, new email, moved district) gleaned must updated participant tracking database alone. people track information sources, personal spreadsheets, longer single source truth, multiple sources truth. makes difficult keep track going project. Whether single person designated update information database, multiple, make sure team members know either update information contact update information.","code":""},{"path":"track.html","id":"benefits","chapter":"9 Data Tracking","heading":"9.1 Benefits","text":"thorough complete participant database updated regularly beneficial following reasons:Data de-identification\nAssigning unique study identifiers linked participant’s true identity within one database necessary maintaining participant confidentiality. database stored restricted secure location (see Chapter 12), separate de-identified study datasets stored, typically destroyed period time project’s completion.\nAssigning unique study identifiers linked participant’s true identity within one database necessary maintaining participant confidentiality. database stored restricted secure location (see Chapter 12), separate de-identified study datasets stored, typically destroyed period time project’s completion.Project coordination record keeping\ndatabase can used customer relation management (CRM) tool, storing participant contact information, well tracking correspondence. can also used project coordination tool, storing scheduling information useful planning activities data collection.\nIntegrating database daily workflow allows team easily report status data collection activities (e.g., today completed 124 150 assessments). Furthermore, checking tracking incoming data daily, compared data collection complete, reduces likelihood missing data.\nLast, thorough tracking allows explain missing data reports publications (e.g., teacher 1234 went maternity leave).\ndatabase can used customer relation management (CRM) tool, storing participant contact information, well tracking correspondence. can also used project coordination tool, storing scheduling information useful planning activities data collection.Integrating database daily workflow allows team easily report status data collection activities (e.g., today completed 124 150 assessments). Furthermore, checking tracking incoming data daily, compared data collection complete, reduces likelihood missing data.Last, thorough tracking allows explain missing data reports publications (e.g., teacher 1234 went maternity leave).Sample rostering\ntime can pull study roster database accurately reflects participant’s current status. tracking information contained tool also aids creation documentation including flow participants CONSORT diagram.\ntime can pull study roster database accurately reflects participant’s current status. tracking information contained tool also aids creation documentation including flow participants CONSORT diagram.Data cleaning\npart data cleaning process, raw dataset sample sizes compared reported complete participant database ensure participants missing final datasets\nFurthermore, database can used de-identifying data. data collected identifiers name, roster tracking database can used merge unique study identifiers name can removed. similar process can used merge assigned variables contained database treatment cohort.\npart data cleaning process, raw dataset sample sizes compared reported complete participant database ensure participants missing final datasetsFurthermore, database can used de-identifying data. data collected identifiers name, roster tracking database can used merge unique study identifiers name can removed. similar process can used merge assigned variables contained database treatment cohort.","code":""},{"path":"track.html","id":"building-your-database","chapter":"9 Data Tracking","heading":"9.2 Building your database","text":"\nFigure 9.2: Timeline constructing using tracking database\nbeneficial build database begin recruiting participants, typically time building data collection tools. way, team recruits consents participants, can record name, assign study ID, enter necessary identifying contact information participant database (see Figure 9.2). Depending database system, may even able scan upload copies consent forms database.project coordinator can build database, can helpful consult data manager, someone relational database expertise, creating system. ensures system set efficiently comprehensively.database may standalone structure, used tracking anonymization purposes. may integrated part larger study system, study data collected /entered well.","code":""},{"path":"track.html","id":"relational-databases","chapter":"9 Data Tracking","heading":"9.2.1 Relational databases","text":"discuss build database, helpful basic understanding benefits relational databases, first introduced Chapter 3. Using relational database track participant information, compared disparate, non-connected spreadsheets, many benefits including reducing data entry errors improving efficiency. relational database organizes information tables, made records (rows) fields (columns), tables related keys (Bourgeois 2014; Chen 2022). general steps building relational database .Decide fields want collect want collect .Group fields entity (e.g.,students, teachers, schools) purpose. Create tables groups.Choose one fields uniquely identify rows tables primary keys. keys change point. Typically keys assigned unique study IDs.Create relationships tables primary foreign keysWe can also refine database normalization, structuring database according normal form rules (Bourgeois 2014; Nguyen 2017; Nobles 2020) reduce redundancy improve data integrity. Going detail normalization outside scope book building database follows normal form rules requires specific expertise, teams may . said, perfectly acceptable build database perfectly optimized works well team! important thing consider building relational database duplicate information across tables. one field need updated one location, never one.Let’s compare simple example building tracking database using relational model non-relational model.","code":""},{"path":"track.html","id":"relational-model","chapter":"9 Data Tracking","heading":"9.2.1.1 Relational model","text":"Figure 9.3 three entities need track database: schools, teachers, students. built simple database one table entity. Within table added fields need collect participants. also set tables include primary keys (uniquely identify rows table) foreign keys (includes values correspond primary key another table). keys unique study identifiers assigned study participants.\nFigure 9.3: Participant database built using relational model\ncan see table duplicated, repeating information. student table contains student level information, teacher table contains teacher level information, school table contains school level information. huge time saver. Imagine teacher’s last name changes. Rather updating name multiple places, now update , teacher table.\nwant see table student teacher information, can simply query database create new table. programs, type querying may simple point click option, programs may require someone write simple queries can used time user.Say example, needed pull roster students teacher. easily create run query, SQL query joins student teacher tables tch_id pulls relevant teacher student information tables, seen Table 9.1.SELECT t_l_name, t_f_name, s_l_name, s_f_name,  gradeFROM Student INNER JOIN Teacher Student.tch_id = Teacher.tch_idORDER t_l_name, t_f_name, s_l_name, s_f_nameTable 9.1:  Example roster created querying relational database tablesDepending design study structure database model, writing queries can become complicated. , want strike balance creating structure reduces inefficiencies data entry also isn’t complicated query based expertise team.","code":""},{"path":"track.html","id":"non-relational-model","chapter":"9 Data Tracking","heading":"9.2.1.2 Non-relational model","text":"Now imagine built non-relational database, three tabs Excel spreadsheet,track participant information (see Figure 9.4). Since unable set system links tables together, need enter redundant information table (teacher school name) order see information within table without flip back forth across tables find information need. Using method now enter repeating teacher school names student table, teacher names change, need update teacher table student table every student associated teacher. requires entry time creates opportunity data entry errors.\nFigure 9.4: Participant database built using non-relational model\nNote \nstudy includes variety related entities, tracked waves time, relational database helpful build. however, tracking one entity (e.g., just students) one wave data collection, database might overkill simple spreadsheet work just fine.","code":""},{"path":"track.html","id":"structuring-the-database","chapter":"9 Data Tracking","heading":"9.2.2 Structuring the database","text":"can begin construct database, need collect following pieces information.entities/units analysis?\nstudents, teachers, classrooms, districts, \nstudents, teachers, classrooms, districts, onAre collecting data longitudinally, across one wave?want use relational table structure?\nyes, want construct relate tables?\nyes, want construct relate tables?fields want include table?collect pieces information, can begin design database structure. can helpful visualize database model process. Figure 9.5 designing database structure scenario collecting information teachers schools, two waves data collection.\nFigure 9.5: Example participant database model\ndesigned database model way:four tables total\nTwo tables information constant based project assumptions (name, email, consent, one time payments sent , one time documents received)\ntime constant information changes (e.g., new last name, new principal), update information appropriate table make note change occurred “notes” field\n\nTwo tables longitudinal information\ntrack data collection activities wave, well information may change wave, based assumptions project. example, assume grade level may change, maybe data collection waves occur across school years teachers may move around. also assume participant can drop point study want track status wave.\n\nTwo tables information constant based project assumptions (name, email, consent, one time payments sent , one time documents received)\ntime constant information changes (e.g., new last name, new principal), update information appropriate table make note change occurred “notes” field\ntime constant information changes (e.g., new last name, new principal), update information appropriate table make note change occurred “notes” fieldTwo tables longitudinal information\ntrack data collection activities wave, well information may change wave, based assumptions project. example, assume grade level may change, maybe data collection waves occur across school years teachers may move around. also assume participant can drop point study want track status wave.\ntrack data collection activities wave, well information may change wave, based assumptions project. example, assume grade level may change, maybe data collection waves occur across school years teachers may move around. also assume participant can drop point study want track status wave.connected tables primary foreign keys (“tch_id” “sch_id”)model absolutely way can design tables. may efficient appropriate ways design database, long duplicating information, build works . example potentially efficient way structure database, combine waves data collection one table create concatenated primary key uses “tch_id” “wave” uniquely identify rows since “tch_id” now duplicated wave data collection (see Figure 9.6).\nFigure 9.6: Example participant database model\nexamples fairly simple scenario, can hopefully see might extrapolate model entities waves data collection, well might modify better meet needs specific project.Note \nstudy involves anonymous data collection, longer able track data associated specific individual. However, still helpful create form tracking system. Creating simplified database, tables based sites instance (school table, district table) allows still track project management data collection efforts (e.g., number student surveys received per school per wave, MOU received).","code":""},{"path":"track.html","id":"choosing-fields","chapter":"9 Data Tracking","heading":"9.2.3 Choosing fields","text":"design database model, also need choose fields include table. fields choose include dependent particular study design. participant tracking database may database enter study data, purposes chapter considering fields relevant project coordination participant de-identification. concerned fields collected part data collection measures (.e. survey items). can consider participant tracking database internal database used coordination, summary, linking purposes. database export data external data sharing.ideas field may consider adding database. Depeding design assumptions study, may collected , others may collected , longitudinally.Ideas fields collect:Study IDs (primary foreign keys relational database)Names (participants sites)Contact informationInformation relevant project coordination (grade level, class periods, block schedules)necessary linking identifiers (double IDs, district/school IDs)Information helpful data collection scheduling (blocks, class times)Consent/assent statusRandomization (treatment/control)Grouping information (cohort)Summary information rates (# consents sent , # students class)Administrative data status (W-9 received, MOUs received)Movement/drop statusData collection status (unique fields instrument)Incentive status (gift cards sent )Notes\nReasons changes (example changes name, email)\nReasons movement/drop \nCommunication participants\nReasons missing data\nErrors data\nReasons changes (example changes name, email)Reasons movement/drop outCommunication participantsReasons missing dataErrors data","code":""},{"path":"track.html","id":"structuring-fields","chapter":"9 Data Tracking","heading":"9.2.3.1 Structuring fields","text":"choose fields also need make decisions structure fields.Set data types fields (e.g., character, integer, date)\nRestrict entry values allowable data types reduce errors\nRestrict entry values allowable data types reduce errorsSet allowable values ranges\nexample, categorical status field may allow “complete”, “partially complete” “incomplete”\nexample, categorical status field may allow “complete”, “partially complete” “incomplete”lump separate pieces information together field\nexample separate first name last name two fields\nexample separate first name last name two fieldsName fields according variable naming rules discussed Chapter 8","code":""},{"path":"track.html","id":"choosing-a-tool","chapter":"9 Data Tracking","heading":"9.2.4 Choosing a tool","text":"many criteria consider choosing tool build database .Choose tool customizable needs\nCan build relational table structure?\nCan export files? Can connect database via application programming interfaces (APIs)?\nCan query data?\nCan build relational table structure?Can export files? Can connect database via application programming interfaces (APIs)?Can query data?Choose tool user-friendly\ndon’t want tool steep learning curve users.\ndon’t want tool steep learning curve users.running project across multiple sites, consider accessibility tool\nexample, may want tool cloud-based site coordinators can access \nmay also want make sure multiple users can access time\nexample, may want tool cloud-based site coordinators can access itYou may also want make sure multiple users can access timeChoose tool interoperable\ninstance, tools may difficulties running certain operating systems\ninstance, tools may difficulties running certain operating systemsConsider cost licensing\nmany free tools, may provide functionality want\nproducts already access ? institution license ?\nmany free tools, may provide functionality wantWhat products already access ? institution license ?Consider security\nSecurity terms participant confidentiality\ntool meet HIPAA/FERPA requirements?\nCan limit access entire database? specific tables?\nSince database contains PII want place restrictions can access enter data\n\n\nProtect data loss\nCan backup system?\nCan protect overwriting data?\nCan keep versions database case mistake ever made need go back older version?\n\nSecurity terms participant confidentiality\ntool meet HIPAA/FERPA requirements?\nCan limit access entire database? specific tables?\nSince database contains PII want place restrictions can access enter data\n\ntool meet HIPAA/FERPA requirements?Can limit access entire database? specific tables?\nSince database contains PII want place restrictions can access enter data\nSince database contains PII want place restrictions can access enter dataProtect data loss\nCan backup system?\nCan protect overwriting data?\nCan keep versions database case mistake ever made need go back older version?\nCan backup system?Can protect overwriting data?Can keep versions database case mistake ever made need go back older version?Data quality protection\nCan set data quality constraints? example, restrict input types/values\nCan set data quality constraints? example, restrict input types/valuesThere many tool options can choose . sampling options . tools represent wide range criteria . Take time review options see one best meets needs.Microsoft AccessMicrosoft ExcelQuickbaseAirtableREDCapClaris FileMakerGoogle Sheets Google FormsForms feed relational database, maintained using SQL (structured query language) database engine SQLite, MySQL, PostgreSQL","code":""},{"path":"track.html","id":"track-enter","chapter":"9 Data Tracking","heading":"9.3 Entering data","text":"last consideration building database , want team enter data database? many ways enter data including using SQL statements, importing data, integrating data collection platform tracking database, even scanning forms using QR codes. options may work great project, going talk two simplest common options: manually entering data spreadsheet view, manually entering data form.","code":""},{"path":"track.html","id":"entering-data-in-a-spreadsheet-view","chapter":"9 Data Tracking","heading":"9.3.1 Entering data in a spreadsheet view","text":"first option manually enter data spreadsheet format participant row. common () option using tools Microsoft Excel Google Sheets. However, can also use option entering database tools Microsoft Access. pros cons method.Pros: quickest easiest method. also allows view data holistically.Cons: method can lead errors someone enters data wrong row/record.\nFigure 9.7: Example spreadsheet view data entry\n","code":""},{"path":"track.html","id":"entering-data-in-a-form","chapter":"9 Data Tracking","heading":"9.3.2 Entering data in a form","text":"second option create form linked tables. enter data forms, automatically populates tables information. option possible many systems including Microsoft Access, RedCap, even Google Forms populates Google Sheets.Pros: method reduces data entry errors working one participant form timeCons: Takes time, possibly expertise, set data entry forms\nFigure 9.8: Example form view data entry\nNote \nparticipant tracking database separate data collection tools, information need entered team using one ways mentioned Section 9.3. However, participant tracking tool also data collection/data capture tool (collect data using REDCap), fields data collection status (e.g., survey completed) may need manually entered. Rather may automated populate “complete” participant submits responses data collection tool.","code":""},{"path":"track.html","id":"ids","chapter":"9 Data Tracking","heading":"9.4 Creating unique identifiers","text":"Participant unique identifiers numeric alphanumeric values typically range 2-10 digits. Assigning identifiers important part protecting privacy human participants. publicly sharing study data, personally identifying information removed identifiers allow uniquely identify link participants data.several ways participant identifiers can assigned (e.g., created participants , assigned data collection software), commonly, research team assigns identifiers participants. participants recruited added participant database, assign unique participant ID. confidentiality promised schools districts, also want assign identifiers sites well.can helpful develop ID schema planning phase, document schema SOP (see Chapter 7). developing schema, several best practices consider.Participants must keep identifier entire project.\neven applies circumstances participant opportunity re-recruited study (seen Figure 9.9). participant still keeps ID throughout study. can use variables identify unique instances participant (e.g., cohort associated participant)\nstatic participant ID allows track flow participant study provides added benefit helping measure dosage.\neven applies circumstances participant opportunity re-recruited study (seen Figure 9.9). participant still keeps ID throughout study. can use variables identify unique instances participant (e.g., cohort associated participant)static participant ID allows track flow participant study provides added benefit helping measure dosage.\nFigure 9.9: Example keeping participant IDs entire study\nParticipant identifiers must unique within across entities\nexample, duplicating IDs within students across teachers schools\nduplicating within entities imperative maintain uniqueness records, duplicating across reduces confusion form belongs reduces potential errors\nrunning multiple studies time, using identical forms across studies, can even helpful assign unique schemas across projects forms accidentally mixed across projects.\nexample, duplicating IDs within students across teachers schoolsNot duplicating within entities imperative maintain uniqueness records, duplicating across reduces confusion form belongs reduces potential errorsIf running multiple studies time, using identical forms across studies, can even helpful assign unique schemas across projects forms accidentally mixed across projects.identifier randomly assigned completely distinct personal information. ensures confidentiality.\nID associated name, dob, income, grade level, forth. example:\nsort names date birth assign IDs sequential order\ngroup grade level assign IDs\ninclude initials part identifier\n\nID associated name, dob, income, grade level, forth. example:\nsort names date birth assign IDs sequential order\ngroup grade level assign IDs\ninclude initials part identifier\nsort names date birth assign IDs sequential orderDo group grade level assign IDsDo include initials part identifierDo embed project information ID potential change\nresearchers prefer embed project level information ID help tracking information. absolutely okay long included project information expected change.\nexample, including project code okay. project code “02”, researchers may choose make first 2 digits student identifiers. long actual student IDs still unique randomly assigned, adding project ID identifier works just fine value change.\n\nHowever, embedding information wave session identifier variable guarantees identifiers remain constant. information added dataset ways (.e., either variable concatenated variable names)\nEmbedding information teacher IDs, school IDs, treatment, cohort also potential cause problems. longitudinal studies, depending study design, possible students move study teachers, teachers move study schools, participants get re-recruited cohorts. issues cause problems information embedded ID ID longer reflect accurate information require IDs changed, breaking best practice #1. , additional identifiers can tracked separate variables (e.g., stu_id, tch_id, sch_id, cohort, treatment, wave) added forms datasets needed\nresearchers prefer embed project level information ID help tracking information. absolutely okay long included project information expected change.\nexample, including project code okay. project code “02”, researchers may choose make first 2 digits student identifiers. long actual student IDs still unique randomly assigned, adding project ID identifier works just fine value change.\nexample, including project code okay. project code “02”, researchers may choose make first 2 digits student identifiers. long actual student IDs still unique randomly assigned, adding project ID identifier works just fine value change.However, embedding information wave session identifier variable guarantees identifiers remain constant. information added dataset ways (.e., either variable concatenated variable names)Embedding information teacher IDs, school IDs, treatment, cohort also potential cause problems. longitudinal studies, depending study design, possible students move study teachers, teachers move study schools, participants get re-recruited cohorts. issues cause problems information embedded ID ID longer reflect accurate information require IDs changed, breaking best practice #1. , additional identifiers can tracked separate variables (e.g., stu_id, tch_id, sch_id, cohort, treatment, wave) added forms datasets neededLast, less important data tracking phase, study datasets identifiers stored character variables. Even ID variable numbers, stored character type. helps prevent people inappropriately working values (.e., taking mean ID variable).\nFigure 9.10: Example study id schema created using best practices\nNote \ntime assign unique identifiers collect anonymous data. situation able assign identifiers since know participants . However, still possible assign identifiers known entities school sites anonymity required.","code":""},{"path":"collect.html","id":"collect","chapter":"10 Data Collection","heading":"10 Data Collection","text":"\nFigure 10.1: Data collection research project life cycle\ncollecting original data part study (.e., administering survey assessment opposed using existing data), data management best practices interwoven throughout data collection process. number one way ensure integrity data spend time planning data collection efforts. planning minimize errors, also keeps data secure, valid, relieves future data cleaning headaches.ever created data collection instrument expected export data looks like image left (Figure 10.2), instead export data looks like image right, know mean. Collecting quality data doesn’t just happen create instrument, takes careful consideration, structure, care part entire team.\nFigure 10.2: comparison data collected without planning data collected planning\n","code":""},{"path":"collect.html","id":"quality-assurance-and-control","chapter":"10 Data Collection","heading":"10.1 Quality assurance and control","text":"addition planning data collection logistics (.e. data collected, collect , ), teams spend time prior data collection anticipating potential data integrity problems may arise data collection putting procedures place reduce errors (DIME Analytics 2021a; Northern Illinois University n.d.). shown Figure 10.1, creating data collection instruments typically collaborative effort project management data management team members. Even project management team builds tools, data management team overseeing data collected tool aligns expectations set data dictionary. chapter review two types practices project management data management team members can implement improve integrity data.Quality assurance practices happen data collected\nBest practices associated designing building data collection instruments\nBest practices associated designing building data collection instrumentsQuality control practices implemented data collection\nBest practices associated managing reviewing data collection\nBest practices associated managing reviewing data collectionBefore dive collecting data, ’s important first review ethical legal considerations data collection effort. working human subjects likely Institutional Review Board (IRB) need review approve data collection instruments well agreement forms collected part study. next section provide overview IRB requirements well best practices creating agreement forms participants partners.","code":""},{"path":"collect.html","id":"institutional-review-board","chapter":"10 Data Collection","heading":"10.2 Institutional Review Board","text":"IRB formal organization designated review monitor human participant research ensure welfare, rights, privacy research participants maintained throughout project (Oregon State University 2012). conducting education research human participants likely interaction oversight one IRBs. IRBs exist institutions research conducted, including universities, school districts, state agencies, research institutions, industries clinical research, even independent IRBs available hire (Emanuel, Lemmens, Elliot 2006). Even within institutions can one IRB (e.g., medical review board behavioral science review board) (Duru Sautmann 2023). reviewing potential requirements, let’s review history administrative body.","code":""},{"path":"collect.html","id":"background","chapter":"10 Data Collection","heading":"10.2.1 Background","text":"1974 IRB established part National Research Act response long history unethical research conducted human participants (Qiao 2018). 1979, Belmont Report (National Commission Protection Human Subjects Biomedical Behavioral Research 1979) outlined set ethical principles research human participants. ethical principles included following (Duru Sautmann 2023; huisman_3_2023?):Respect persons\nincluded protecting autonomy participants acquiring consent well providing plan protect participant privacy\npractice means acquiring consent way ensures participants can comprehend asked , ensuring understand participation voluntary, ensuring understand plan protect privacy\n\nincluded protecting autonomy participants acquiring consent well providing plan protect participant privacy\npractice means acquiring consent way ensures participants can comprehend asked , ensuring understand participation voluntary, ensuring understand plan protect privacy\npractice means acquiring consent way ensures participants can comprehend asked , ensuring understand participation voluntary, ensuring understand plan protect privacyBeneficence\ninvolved maximizing good minimizing harm study, participants society large\npractice means taking time assess risk benefits study intervention well data collection efforts (e.g., burdensome survey)\n\ninvolved maximizing good minimizing harm study, participants society large\npractice means taking time assess risk benefits study intervention well data collection efforts (e.g., burdensome survey)\npractice means taking time assess risk benefits study intervention well data collection efforts (e.g., burdensome survey)Justice\nincluded providing additional care consideration working subjects vulnerable coercion undue influence (e.g., children, prisoners), well making sure practices non-exploitative fair distribution costs benefits across participants\npractice involves fairness selection participants\n\nincluded providing additional care consideration working subjects vulnerable coercion undue influence (e.g., children, prisoners), well making sure practices non-exploitative fair distribution costs benefits across participants\npractice involves fairness selection participants\npractice involves fairness selection participantsHeavily influenced Belmont Report, 1991 Federal Policy Protection Human Subjects published, establishing core procedures human subject protections. policy, 45 CFR part 46 (Office Human Research Protections 2016), included four subparts. Subpart , known “Common Rule” 15 federal departments agencies codified policy separate regulations, provided set protections human subjects research including informed consent, review IRB, compliance monitoring (National Institute Justice 2007; Office Human Research 2009).2018 Common Rule revised order better protect research participants reduce administrative burden (Office Human Research Office Human Research 2018; U.S. Department Health Human Services n.d.). many revisions made, changes applicable education researchers include following (Fordham University n.d.):Revisions additions exempt categories, many applicable research conducted educational settingsReduced burden continuing review, particularly exempt expedited studiesClarifications informed consent organized, written, provided","code":""},{"path":"collect.html","id":"requirements","chapter":"10 Data Collection","heading":"10.2.2 Requirements","text":"institution’s IRB submission process different, typically study involves working human subjects required submit application IRB. submitting application need determine review category study falls application differs depending category (Lafayette College n.d.; Northwestern University n.d.; University California Berkeley 2022).Exempt\nstudies usually involve minimal risk fit within categories predefined IRB (e.g., evaluating use accepted revised standardized tests). studies typically involve shorter review process quicker review non-exempt studies.\nstudies usually involve minimal risk fit within categories predefined IRB (e.g., evaluating use accepted revised standardized tests). studies typically involve shorter review process quicker review non-exempt studies.Expedited\nstudies also involve minimal risk meet criteria exempt status (e.g., collection voice, video, image data non-vulnerable populations).\nstudies also involve minimal risk meet criteria exempt status (e.g., collection voice, video, image data non-vulnerable populations).Full Review\nstudy fall one two categories (e.g., collection information illegal behavior), requires full review, discussed full board convened meeting.\nstudy fall one two categories (e.g., collection information illegal behavior), requires full review, discussed full board convened meeting.education research often submitting application institution’s IRB. However, situations process might vary. instance, case multi-site cooperative, non-exempt projects, may necessary submit single IRB (sIRB). situation, authorization agreement signed collaborating institutions one institution designated IRB--Record. institution becomes authority review continuing oversight study activities (Cornell Research Services 2019; University Michigan 2023).part application, common documents may required submit include following (Cabrini University n.d.; Duru Sautmann 2023):Certificates human subjects training (e.g., CITI training52)Research protocol (see Chapter 7)\nwriting protocol, make sure review IRB’s rules around data handling include information plan. IRBs typically specific rules things paper electronic data must stored backed , long data retained, data can transferred shared, data anonymized (Filip 2023).\nwriting protocol, make sure review IRB’s rules around data handling include information plan. IRBs typically specific rules things paper electronic data must stored backed , long data retained, data can transferred shared, data anonymized (Filip 2023).Study materials (e.g., recruitment materials)Copies instruments (e.g., surveys, interview guides, observation forms)\nNote need created can submit IRB make sure consider timing start building instruments early enough give time submit IRB data collection\nNote need created can submit IRB make sure consider timing start building instruments early enough give time submit IRB data collectionCopy informed consent/assent forms\n, give plenty time submit start participant recruitment\n, give plenty time submit start participant recruitmentIf collecting data sites (e.g., school districts) sharing data sites, supporting documentation partners may required (MOUs, data use/sharing agreements, letters support, confidentiality agreements)partnering institutions, IRB approval letters partner institutions may also requiredThe review process can take several weeks common IRB request revisions materials. Make sure review timeline give plenty time work process need begin recruitment data collection.","code":""},{"path":"collect.html","id":"collect-agreements","chapter":"10 Data Collection","heading":"10.2.3 Agreements","text":"several types agreements may required research study ethical legal reasons. discuss common type agreements, informed consent assent, well agreements used working external partners including memorandum understanding documents, data use agreements, non-disclosure agreements.","code":""},{"path":"collect.html","id":"collect-consent","chapter":"10 Data Collection","heading":"10.2.3.1 Consents","text":"Informed consent involves obtaining participant’s voluntary agreement participate research study. described Belmont Report (National Commission Protection Human Subjects Biomedical Behavioral Research 1979), informed consent meet following criteria (Huisman n.d.):Describe study expected participantUse accessible language ensure comprehension. Avoid technical jargon explain terms may easily understood.Explain participation voluntaryReview participant privacy maintainedWith revised Common Rule, additional requirements informed consent added (Fordham University n.d.).top consent must begin concise review key information allows participants make informed decisionsAll information must presented sufficient detail make decisions, just bulleted lists factsThe form must disclose plans use participant data future researchFigure 10.3 shows common elements included participant consent form (Bellevue College, n.d.; Turing Way Community 2022).\nFigure 10.3: Common topics include informed consent information sheet\nDepending type research study, participant signature check box denoting consent may required. , can helpful put information cover/information sheet, separate page signed consent. signing, participants required acknowledge theyHave read understood information providedHave given opportunity ask questionsUnderstand participation voluntaryUnderstand may withdraw study timeNot studies require active consent (University Virginia n.d.). studies may allow passive consent may obtained providing information sheet participants following type information:consent study, additional action required; simply move forward study.\nchoose withdraw, can notify specified contact.institution’s IRB let know type consent required study language required.","code":""},{"path":"collect.html","id":"data-sharing","chapter":"10 Data Collection","heading":"10.2.3.1.1 Data sharing","text":"increase federal data sharing requirements, important consider want gain consent public data sharing. Meyer (2018) provides general best practices consider adding language public data sharing consent form.Don’t promise destroy data (unless funder/IRB explicitly requires )\nincorporate data-retention sharing plans including letting participants know access data\nincorporate data-retention sharing plans including letting participants know access dataDon’t promise share data\nget consent retain share data (consider adding specific repository plan share data ).\nConsider offering tiered levels consent participants may want data publicly shared allow .\nget consent retain share data (consider adding specific repository plan share data ).Consider offering tiered levels consent participants may want data publicly shared allow .Don’t promise research analyses collected data limited certain topics\nsay data may used future research share general purposes (e.g., replication, new analyses)\nsay data may used future research share general purposes (e.g., replication, new analyses)review ways plan de-identify data thoughtful considering risks re-identification (ex: small sample size sub-groups)essentially three different ways can go obtaining consent data sharing (Gilmore, Kennedy, Adolph 2018).Include line public data sharing consent participate research.\nmethod, participant consents agreeing participate research study data shared publicly.\nmethod, participant consents agreeing participate research study data shared publicly.participants consent data sharing time provide research study consent, provide separate consent form purposes public data sharing.participants consent data sharing separate consent form, later time, research activities completed.\nObtaining consent way ensures participants fully aware data collected can make informed decision future data.\nObtaining consent way ensures participants fully aware data collected can make informed decision future data.limitation using method 1, discussed Gilmore, et al. (2018), participant uncomfortable data publicly shared, also lose study participant. method 2 3 may best option. choose go method 2 3, important track participant study consent status tracking database (discussed Chapter 9), also add field track consent status data sharing publicly share data given permission .","code":""},{"path":"collect.html","id":"assents","chapter":"10 Data Collection","heading":"10.2.3.2 Assents","text":"study involves participants age 18, may also required obtain participant assent form, addition parent/guardian consent form. guidelines assent needed varies across IRBs, typically child age 7 older (Duru Sautmann 2023), assent parent consent needed. including similar information provided consent, usually shorter forms require much simplistic language depending age child.","code":""},{"path":"collect.html","id":"collecting-consent-and-assent","chapter":"10 Data Collection","heading":"10.2.3.3 Collecting consent and assent","text":"Last, many institutions started collecting electronic consent rather paper consents, especially rise remote data collection efforts. benefits method including reducing manual labor collecting paper forms removing need store paper forms scan electronic form. However, still things consider collecting electronic consent (Lee, Hughes, Marsh 2020; Malow et al. 2021).Make sure IRB approves methodUse institution IRB approved tools collect consent (e.g., Qualtrics, DocuSign)Find information required IRB (e.g., signature, typed name, check box, date)Consider consents stored (e.g., download PDFs, download spreadsheet, store collection tool)collecting paper consent assent, still additional things consider.consents sent packets, say schools, make sure system place track form belongs . consents start coming back, ’s possible names illegible, duplicate names across sites. Tracking origin form look something like :\nCollecting class rosters ahead time pre-printing names identifiers (e.g., teacher, school) consents sending packets (allowed IRB school)\nAsking teachers print student teacher name form consents/assents handed \nCollecting class rosters ahead time pre-printing names identifiers (e.g., teacher, school) consents sending packets (allowed IRB school)Asking teachers print student teacher name form consents/assents handed outIf consents collected -person data collectors, want similar process\nEither pre-print names forms data collectors print names identifiers (e.g., teacher, school) forms collected\nEither pre-print names forms data collectors print names identifiers (e.g., teacher, school) forms collectedTemplates Resources","code":""},{"path":"collect.html","id":"other-agreements","chapter":"10 Data Collection","heading":"10.2.3.4 Other agreements","text":"education research common collaborate external partners (e.g., school districts, state agencies, nonprofit organizations) implement study collect data. partnerships may involve several different types agreements. Across different types agencies even across similar agency types (e.g., across school districts), agreements required can vary widely. Let’s review types agreements may encounter.Memorandum understanding (MOU)\nagreement provides framework collaboration. legally binding, establishes commitment partnership outlines responsibilities expectations partner (Duru Kopper 2021; National Center Education Statistics n.d.b; REL West, n.d.). document may also synonymous letter intent letter support.\nagreement provides framework collaboration. legally binding, establishes commitment partnership outlines responsibilities expectations partner (Duru Kopper 2021; National Center Education Statistics n.d.b; REL West, n.d.). document may also synonymous letter intent letter support.Data use agreement (DUA)\nDUA, also sometimes referred data sharing agreement, contractual agreement provides terms conditions working data restricted-use (.e., contain sensitive identifiable information), often protected laws HIPAA FERPA. addition describing data shared time frame sharing, DUAs may include information purposes data can used, data security safeguarding expectations, data destruction rules (Feeney et al. 2021; FSU Office Research n.d.; Geraghty Feeney 2021). DUAs commonly written data sharing partnering school districts. example, DUA may include terms sharing, working , storing education records data. However, DUAs can used provide guidance outgoing data well (.e., researcher sharing original data agency). DUAs can standalone documents may incorporated MOU.\nDUA, also sometimes referred data sharing agreement, contractual agreement provides terms conditions working data restricted-use (.e., contain sensitive identifiable information), often protected laws HIPAA FERPA. addition describing data shared time frame sharing, DUAs may include information purposes data can used, data security safeguarding expectations, data destruction rules (Feeney et al. 2021; FSU Office Research n.d.; Geraghty Feeney 2021). DUAs commonly written data sharing partnering school districts. example, DUA may include terms sharing, working , storing education records data. However, DUAs can used provide guidance outgoing data well (.e., researcher sharing original data agency). DUAs can standalone documents may incorporated MOU.Non-disclosure agreement (NDA)\nNDAs, also may synonymous confidentiality agreements, restrict use proprietary confidential information (University Washington n.d.) legally enforceable agreements.\nNDAs, also may synonymous confidentiality agreements, restrict use proprietary confidential information (University Washington n.d.) legally enforceable agreements.types agreements required differ across partners, agreements submitted also vary. one example, working school districts, districts may informal research request systems place simply submit required documentation approval. Others may formal systems place, including institutional review process involves committee reviewal timeline. process often kicked submitting research request application. addition providing information study, requests often ask researchers submit documents :researcher’s institutional approved IRB documentationAny required agreement forms (e.g., DUA, NDA, confidentiality agreement)Copies IRB approved data collection instrumentsConsent/assent forms\nmay copy consent/assent forms submitted institutional IRB. However, also requesting school records data, districts may also require additional consent form completed parents/guardians, specifying specific records shared, disclosing education records (University Michigan 2019). study meets FERPA exception, possible consent may required (U.S. Department Education, Privacy Technical Assistance Center 2014).\nmay copy consent/assent forms submitted institutional IRB. However, also requesting school records data, districts may also require additional consent form completed parents/guardians, specifying specific records shared, disclosing education records (University Michigan 2019). study meets FERPA exception, possible consent may required (U.S. Department Education, Privacy Technical Assistance Center 2014).Last, order IRB applications submitted agreements made depend IRBs working , well research partners external collaborators make sure start discussions early order documents ready appropriate timeline.Templates Resources","code":""},{"path":"collect.html","id":"collect-assurance","chapter":"10 Data Collection","heading":"10.3 Quality Assurance","text":"\nFigure 10.4: Common education research data collection methods\nNow baseline understanding ethical legal considerations, can dive protecting data quality data collection. Education researchers collect original data many ways (see Figure 10.4). focus chapter data collected via forms (.e., document spaces respond questions). Forms widely used collect data education research (think surveys, assessments, observation forms, progress monitoring form website), yet developed poorly, can produce problematic data issues. flip side, practices discussed chapter implemented, forms can also easiest tool remedy issues .focus forms discount importance data collected means video audio recording, issues participant privacy data security integrity absolutely also considered. However, even types data collection efforts, often teams ultimately still coding data using sort form (e.g., observation form), supporting need build forms collect quality data.collecting information using forms can certainly best fix data errors data collection cleaning process. However, one effective ways ensure quality data correct source. means designing items building data collection tools way produces valid, reliable, secure data. creating original data collection instruments, four ways collect higher quality data.Using good questionnaire design principlesImplementing series pilot testChoosing data collection tools meet needsBuilding instrument end mindWe discuss phases .Note \ncollecting data using standardized assessment, along provided instrument (e.g., computer-adaptive testing program), information section applicable. situations, best adhere guidelines provided assessment company.","code":""},{"path":"collect.html","id":"collect-design","chapter":"10 Data Collection","heading":"10.3.1 Questionnaire design","text":"Chapter 7 discussed importance documenting instrument items data dictionary creating data collection instruments. develop items add data dictionary, vital consider questionnaire design.instruments (e.g., cognitive assessments) typically standardized items, instruments, surveys, often predefined, allowing researchers freedom design instrument can lead negative effects errors, bias, potential harm (DIME Analytics 2021a; Northern Illinois University n.d.). Question ordering, response option ordering, question wording, can impact participant responses. questionnaire design actually outside scope book, tips help collect valid, reliable, ethical survey data. addition following tips, make sure consult methodologist designing questionnaire.Use existing standards possible\nOrganizations National Institutes Health (n.d.b) National Center Education Statistics (n.d.) developed repositories (Common Data Elements64 Common Education Data Standards65) standardized question wording paired set allowable response options commonly used data elements. Using standards collecting commonly used variables, demographics, provides following benefits (ICPSR 2022; Kush et al. 2020):\nReduces bias\nAllows harmonization data across research studies also across field\nallows researchers draw conclusions using larger samples comparing data time\nalso reduces costs integrating datasets\n\nImproves interpretation information\n\nOrganizations National Institutes Health (n.d.b) National Center Education Statistics (n.d.) developed repositories (Common Data Elements64 Common Education Data Standards65) standardized question wording paired set allowable response options commonly used data elements. Using standards collecting commonly used variables, demographics, provides following benefits (ICPSR 2022; Kush et al. 2020):\nReduces bias\nAllows harmonization data across research studies also across field\nallows researchers draw conclusions using larger samples comparing data time\nalso reduces costs integrating datasets\n\nImproves interpretation information\nReduces biasAllows harmonization data across research studies also across field\nallows researchers draw conclusions using larger samples comparing data time\nalso reduces costs integrating datasets\nallows researchers draw conclusions using larger samples comparing data timeIt also reduces costs integrating datasetsImproves interpretation informationMake sure questions clearly worded answer choices clear comprehensive\nConsider language might interpreted. question wording confusing? Can response options misinterpreted?\nRather asking “county ?” looking participant’s current location, specific ask “county currently reside ?”\nRather asking “parent ?” providing response options “m” “f”, “m” “f” interpreted “male” “female”, clearly write response options make sure comprehensive (mother, father, legal guardian, forth)\nRather asking “children siblings?” can confusing, remove negative ask “children siblings?” (Reynolds, Schatschneider, Logan 2022)\nquestion leading/biased?\nresponse options ordered leading way?\n\none way answer question?\nresponse categories mutually exclusive exhaustive (ICPSR n.d.)?\n\n\nConsider language might interpreted. question wording confusing? Can response options misinterpreted?\nRather asking “county ?” looking participant’s current location, specific ask “county currently reside ?”\nRather asking “parent ?” providing response options “m” “f”, “m” “f” interpreted “male” “female”, clearly write response options make sure comprehensive (mother, father, legal guardian, forth)\nRather asking “children siblings?” can confusing, remove negative ask “children siblings?” (Reynolds, Schatschneider, Logan 2022)\nquestion leading/biased?\nresponse options ordered leading way?\n\none way answer question?\nresponse categories mutually exclusive exhaustive (ICPSR n.d.)?\n\nRather asking “county ?” looking participant’s current location, specific ask “county currently reside ?”Rather asking “parent ?” providing response options “m” “f”, “m” “f” interpreted “male” “female”, clearly write response options make sure comprehensive (mother, father, legal guardian, forth)Rather asking “children siblings?” can confusing, remove negative ask “children siblings?” (Reynolds, Schatschneider, Logan 2022)question leading/biased?\nresponse options ordered leading way?\nresponse options ordered leading way?one way answer question?\nresponse categories mutually exclusive exhaustive (ICPSR n.d.)?\nresponse categories mutually exclusive exhaustive (ICPSR n.d.)?Consider data ethics questionnaire design (Gaddy Scott 2020; Kaplowitz Johnson 2020; Kopper Parry 2021; mathematica_tips_nodate?; Narvaiz n.d.)\nConsider item tie questions outcomes\nDon’t cause undue burden participants collecting data just data\ncollecting demographic information, provide explanation information necessary used research\n\nReview question wording\npotential harm participants? benefits outweigh risks?\nsensitive questions included, make sure discuss protect respondent’s information\n\nMake questions inclusive population also capturing categories relevant research\nquestion multiple choice, still include “” option open-text field\ndemographic information, allow participants select one option\n\nConsider including one general free-text field survey allow participants provide additional information feel captured elsewhere\nConsider item tie questions outcomes\nDon’t cause undue burden participants collecting data just data\ncollecting demographic information, provide explanation information necessary used research\nDon’t cause undue burden participants collecting data just dataIf collecting demographic information, provide explanation information necessary used researchReview question wording\npotential harm participants? benefits outweigh risks?\nsensitive questions included, make sure discuss protect respondent’s information\npotential harm participants? benefits outweigh risks?sensitive questions included, make sure discuss protect respondent’s informationMake questions inclusive population also capturing categories relevant research\nquestion multiple choice, still include “” option open-text field\ndemographic information, allow participants select one option\nquestion multiple choice, still include “” option open-text fieldFor demographic information, allow participants select one optionConsider including one general free-text field survey allow participants provide additional information feel captured elsewhereLimit collection personally identifiable information (PII)\nCollecting identifiable information balancing act protecting participant confidentiality collecting information necessary implement study. often need collect identifying information either purposes record linking purposes related study outcomes (e.g., scoring assessment based participant’s age).\ngeneral rule, want collect PII absolutely necessary project, (Gaddy Scott 2020). discussed Chapter 2, PII can include direct identifiers (e.g., name email) well indirect identifiers (e.g., birthdate). sharing data, PII need removed altered protect confidentiality.\nCollecting identifiable information balancing act protecting participant confidentiality collecting information necessary implement study. often need collect identifying information either purposes record linking purposes related study outcomes (e.g., scoring assessment based participant’s age).general rule, want collect PII absolutely necessary project, (Gaddy Scott 2020). discussed Chapter 2, PII can include direct identifiers (e.g., name email) well indirect identifiers (e.g., birthdate). sharing data, PII need removed altered protect confidentiality.Survey Design Resources","code":""},{"path":"collect.html","id":"pilot-the-instrument","chapter":"10 Data Collection","heading":"10.3.2 Pilot the instrument","text":"Gathering feedback instruments integral part quality assurance process. three phases piloting instrument (DIME Analytics 2021b) (see Figure 10.5):Gathering internal feedback items\ndiscussed Chapter 7, items instrument added data dictionary, team review data dictionary provide feedback\ndiscussed Chapter 7, items instrument added data dictionary, team review data dictionary provide feedbackPiloting instrument content\nteam approved items collected, second phase piloting can begin. Create printable draft instrument can shared people study population gather feedback\npiloting instrument small population (N < 10), either gathering feedback checklist collecting data using instrument intent disseminate outcomes research data, IRB approval likely required (Cornell University 2019; Stanford University n.d.). said, always consult institution’s IRB rules can vary.\nteam approved items collected, second phase piloting can begin. Create printable draft instrument can shared people study population gather feedbackIf piloting instrument small population (N < 10), either gathering feedback checklist collecting data using instrument intent disseminate outcomes research data, IRB approval likely required (Cornell University 2019; Stanford University n.d.). said, always consult institution’s IRB rules can vary.Piloting instrument data related issues\ninstrument created chosen data collection tool, share instrument team review\ninterested whether data collecting accurate, comprehensive, usable\ndiscuss phase greater detail Section 10.3.4\ninstrument created chosen data collection tool, share instrument team reviewHere interested whether data collecting accurate, comprehensive, usableWe discuss phase greater detail Section 10.3.4Last, move piloting phases, remember update changes tool also data dictionary relevant documentation.\nFigure 10.5: Data collection instrument pilot phases\n","code":""},{"path":"collect.html","id":"collect-tools","chapter":"10 Data Collection","heading":"10.3.3 Choose quality data collection tools","text":"content piloting completed, teams ready begin building instruments data collection tools (see Figure 10.4). Research teams may restricted tools use collect data variety reasons including limited resources, research design, population studied, chosen instrument (e.g., existing assessment can collected using provided tool). However, flexibility choose collect data, pick tool meets various needs project also providing data quality security controls. Things consider choosing data collection tool :Pick tool meets needs project\ncrowdsourcing required?\nmulti-site access required?\nentering data (.e., data collectors, participants)?\nparticipants entering data, tool accessible population?\n\ntechnical requirements tool (.e., internet available plan use web-based tool)?\ntool customizable features necessary instrument (e.g., branching logic, automated email reminders, alert system ecological momentary assessments, options embed data, options calculate scores tool)?\ncrowdsourcing required?multi-site access required?entering data (.e., data collectors, participants)?\nparticipants entering data, tool accessible population?\nparticipants entering data, tool accessible population?technical requirements tool (.e., internet available plan use web-based tool)?tool customizable features necessary instrument (e.g., branching logic, automated email reminders, alert system ecological momentary assessments, options embed data, options calculate scores tool)?Compliance security\ncollect identifiable data, tool HIPAA compliant? FERPA compliant? (see Chapter ?? learn data classification levels)\ntool approved institution?\ncollecting anonymous data, option anonymize responses tool (e.g., remove IP Address identifying metadata collected tool)?\ncollect identifiable data, tool HIPAA compliant? FERPA compliant? (see Chapter ?? learn data classification levels)tool approved institution?collecting anonymous data, option anonymize responses tool (e.g., remove IP Address identifying metadata collected tool)?Training needed\nadditional team training needed allow team use /build instruments tool?\nadditional team training needed allow team use /build instruments tool?Associated costs\ncost associated tool? budget tool?\nadditional costs line (e.g., collecting data paper means someone need hand enter data later)?\ncost associated tool? budget tool?additional costs line (e.g., collecting data paper means someone need hand enter data later)?Data quality features\ntool allow set data validation?\ntool version control?\ntool features deal fraud/bots?\ntool allow set data validation?tool version control?tool features deal fraud/bots?variety tool options, nutshell comes data collected via forms, collecting data one two ways—electronic paper. addition choosing tools based criteria, general benefits associated method also considered (Cohen, Manion, Morrison 2007; Douglas, Ewell, Brauer 2023; Gibson 2021; ICPSR n.d.; Malow et al. 2021; Society Critical Care Medicine 2018; Bochove, Alper, Gu n.d.).\nFigure 10.6: Comparison data collection tool benefits\nNote \nchoose collect data electronic format, highly recommend using web-based tool directly feeds shared database rather offline tools store data individual devices. Using web-based tool, data stored remotely database can easily downloaded connected time. additional work required. \nHowever, collecting data various tablets field, forms offline later connected web-based form, data stored individually tablet. may less secure (e.g., tablet becomes corrupted), may also require additional data wrangling work including downloading data tablet secure storage location day combining files single dataset. use electronic tool site internet, consider using one many tools (e.g., Qualtrics, SurveyCTO) allow collect data using offline app upload data back platform internet connection .Tool Comparison Resources","code":""},{"path":"collect.html","id":"collect-build","chapter":"10 Data Collection","heading":"10.3.4 Build with the end in mind","text":"Last, want build tool end mind. means taking time consider data collect translated dataset (Beals Schectman 2014; Lewis 2022b; UK Data Service 2023b). Recall Chapter 3, ultimately need data rectangular format, organized according basic data organization rules, order analyzable.process building tools end mind fairly different electronic tools compared paper forms going talk two processes separately.","code":""},{"path":"collect.html","id":"collect-electronic","chapter":"10 Data Collection","heading":"10.3.4.1 Electronic data collection","text":"first thing want building tool bring data dictionary. data dictionary guide build instrument. tools, REDCap, provide option upload data dictionary can used automate creation data collection forms opposed building scratch (Patridge Bardyn 2018).However, building instrument manually, adhering following guidelines ensure collect data easier interpret usable, also reduce amount time need spend future data cleaning (Lewis 2022b).Include items data dictionary\nincludes substantive questions, well items necessary linking purposes (e.g., participant identifiers, rater ids inter-rater reliability)\ninclude variables data dictionary derived (e.g., sum scores) grouping variables added data cleaning phase (e.g., treatment, cohort)\nincludes substantive questions, well items necessary linking purposes (e.g., participant identifiers, rater ids inter-rater reliability)include variables data dictionary derived (e.g., sum scores) grouping variables added data cleaning phase (e.g., treatment, cohort)Name items correct variable name data dictionary (UK Data Service 2023b)\nexample, instead using platform default name “Q2”, rename item “tch_years”\nmentioned Chapter 8, ’s also best concatenate time component variable names project longitudinal. makes difficult reuse instrument time periods, creating additional work team.\nexample, instead using platform default name “Q2”, rename item “tch_years”mentioned Chapter 8, ’s also best concatenate time component variable names project longitudinal. makes difficult reuse instrument time periods, creating additional work team.Code values data dictionary\nexample, 1 = “strongly agree”, 2 = “agree”, 3 = “disagree”, 4 = “strongly disagree”\nMany times tools assign default value response options values may align ’ve designated data dictionary\nedit survey, continue check coded values change due reordering, removal, addition new response options\nexample, 1 = “strongly agree”, 2 = “agree”, 3 = “disagree”, 4 = “strongly disagree”Many times tools assign default value response options values may align ’ve designated data dictionaryAs edit survey, continue check coded values change due reordering, removal, addition new response optionsUse data validation reduce errors missing data (UK Data Service 2023b)\nContent validation open-text boxes\nRestrict entry type assigned data dictionary (e.g., numeric)\nRestrict entry format assigned data dictionary (e.g., YYYY-MM-DD)\nRestrict ranges based allowable ranges data dictionary (e.g., 1-50)\neven include validating previous responses (e.g., SchoolA selected previous question, grade level 6-8, SchoolB selected, grade level 7-8)\n\n\nResponse validation\nConsider use forced-response request-response options reduce missing data\nForced-response options allow participants move forward without completing item. Request-response options notify respondent skip question ask still like move forward without responding\naware adding forced-response option sensitive questions potential harmful produce bad data. adding forced-response option sensitive question, consider allowing participants opt-another way (e.g., “Prefer answer”).\n\n\nContent validation open-text boxes\nRestrict entry type assigned data dictionary (e.g., numeric)\nRestrict entry format assigned data dictionary (e.g., YYYY-MM-DD)\nRestrict ranges based allowable ranges data dictionary (e.g., 1-50)\neven include validating previous responses (e.g., SchoolA selected previous question, grade level 6-8, SchoolB selected, grade level 7-8)\n\nRestrict entry type assigned data dictionary (e.g., numeric)Restrict entry format assigned data dictionary (e.g., YYYY-MM-DD)Restrict ranges based allowable ranges data dictionary (e.g., 1-50)\neven include validating previous responses (e.g., SchoolA selected previous question, grade level 6-8, SchoolB selected, grade level 7-8)\neven include validating previous responses (e.g., SchoolA selected previous question, grade level 6-8, SchoolB selected, grade level 7-8)Response validation\nConsider use forced-response request-response options reduce missing data\nForced-response options allow participants move forward without completing item. Request-response options notify respondent skip question ask still like move forward without responding\naware adding forced-response option sensitive questions potential harmful produce bad data. adding forced-response option sensitive question, consider allowing participants opt-another way (e.g., “Prefer answer”).\n\nConsider use forced-response request-response options reduce missing data\nForced-response options allow participants move forward without completing item. Request-response options notify respondent skip question ask still like move forward without responding\naware adding forced-response option sensitive questions potential harmful produce bad data. adding forced-response option sensitive question, consider allowing participants opt-another way (e.g., “Prefer answer”).\nForced-response options allow participants move forward without completing item. Request-response options notify respondent skip question ask still like move forward without respondingBe aware adding forced-response option sensitive questions potential harmful produce bad data. adding forced-response option sensitive question, consider allowing participants opt-another way (e.g., “Prefer answer”).Choose appropriate type format display item\nBecome familiar various questions types available tool (e.g., rank order, multiple choice, text box, slider scale)\nBecome familiar various formats (e.g., radio button, drop-, checkbox)\nexample, item rank order question (ranking 3 items), creating question multi-line, free-text entry form may lead duplicate entries (entering rank 1 ). However, using something like rank order question type drag drop format ensures participants allowed duplicate rankings.\nBecome familiar various questions types available tool (e.g., rank order, multiple choice, text box, slider scale)Become familiar various formats (e.g., radio button, drop-, checkbox)example, item rank order question (ranking 3 items), creating question multi-line, free-text entry form may lead duplicate entries (entering rank 1 ). However, using something like rank order question type drag drop format ensures participants allowed duplicate rankings.finite number response options item, number isn’t large (less ~ 20) use controlled vocabularies (.e., pre-defined list values) rather open-text field (OpenAIRE_eu 2018; UK Data Service 2023b)\nexample, list school name drop-item rather participants enter school name\nprevents variation text entry (e.g., “Sunvalley Middle”, “sunvalley”, “Snvally Middle”), ultimately creates unnecessary data cleaning work may even lead unusable values\n\nexample, list school name drop-item rather participants enter school name\nprevents variation text entry (e.g., “Sunvalley Middle”, “sunvalley”, “Snvally Middle”), ultimately creates unnecessary data cleaning work may even lead unusable values\nprevents variation text entry (e.g., “Sunvalley Middle”, “sunvalley”, “Snvally Middle”), ultimately creates unnecessary data cleaning work may even lead unusable valuesIf infinite number response options item number options large, use open-text box\ncan create searchable field tool, allowing participants easily sift options, absolutely . Otherwise, use text-box opposed participants scroll large list options\nConsider adding examples possible response options clarify looking \nUsing open-ended text boxes mean regroup information categories later cleaning process. just time-consuming requires interpretation decision-making part data cleaner\ncan create searchable field tool, allowing participants easily sift options, absolutely . Otherwise, use text-box opposed participants scroll large list optionsConsider adding examples possible response options clarify looking forUsing open-ended text boxes mean regroup information categories later cleaning process. just time-consuming requires interpretation decision-making part data cleanerOnly ask one piece information per question\nexample, rather asking “Please list number students algebra class geometry class”, split two separate questions questions download two separate items dataset\nalso includes simple examples splitting first name last name two separate fields\nprevents confusion case participant data collector swaps order information\nexample, rather asking “Please list number students algebra class geometry class”, split two separate questions questions download two separate items datasetThis also includes simple examples splitting first name last name two separate fieldsThis prevents confusion case participant data collector swaps order informationTo protect participant privacy ensure integrity data, consider adding line introduction web-based instrument, instructing participants close browser upon completion others may access responsesLast, possible, export instrument human-readable document perform final checks\nquestions accounted ?\nresponse options accounted coded ?\nskip logic shown expected?\nquestions accounted ?response options accounted coded ?skip logic shown expected?tool created, last step pilot data issues (see Figure 10.5). Collect sample responses team members. Create feedback checklist complete review instrument (Gibson Louw 2020). Assign different reviewers enter survey using varying criteria (e.g., different schools, different grade levels). Let team members know actively try break things (Kopper Parry 2020). Try enter nonsensical values, try skip items, try enter duplicate entries. problems tool, now time find .sample responses collected team members, export sample data using chosen data capture process (see Chapter 11) review data following:unexpected missing variables?unexpected variable names?unexpected values variables?missing values expect data?unexpected variable formats?data exporting analyzable, rectangular format?issues found either team feedback reviewing exported sample data, take time update tool well documentation needed starting data collection.Last, also time update data dictionary. review exported file, update data dictionary reflect unexpected variables included (e.g., metadata), unexpected formatting, well newly discovered recoding calculations required data cleaning process. example, upon downloading sample data learn “select ” question differently expected, now time add information, along necessary future transformations, data dictionary.","code":""},{"path":"collect.html","id":"collect-paper","chapter":"10 Data Collection","heading":"10.3.4.2 Paper data collection","text":"many situations collecting data electronically may feasible best option project. definitely trickier design paper tool way prevents bad data, still steps can take improve data quality.Use data dictionary guide create paper form\nMake sure questions included response options accurately added form\nMake sure questions included response options accurately added formHave clear instructions complete paper form (Kopper Parry 2021)\nMake sure overall instructions top form also explicit instructions question completed\nwrite answers (e.g., margin)\nanswers recorded (e.g., YYYY-MM-DD, 3 digit number)\nmany answers recorded (e.g., circle one answer, check applicable boxes)\nnavigate branching logic (e.g., include visual arrows)\n\nMake sure overall instructions top form also explicit instructions question completed\nwrite answers (e.g., margin)\nanswers recorded (e.g., YYYY-MM-DD, 3 digit number)\nmany answers recorded (e.g., circle one answer, check applicable boxes)\nnavigate branching logic (e.g., include visual arrows)\nwrite answers (e.g., margin)answers recorded (e.g., YYYY-MM-DD, 3 digit number)many answers recorded (e.g., circle one answer, check applicable boxes)navigate branching logic (e.g., include visual arrows)ask one piece information per question reduce confusion interpretationOnce tool created, want pilot instrument team data issues (see Figure 10.5). Using feedback collected, edit tool needed sending field.Last, unless paper data collected using machine-readable form, need manually entered electronic format data capture phase. talk data entry specifically Chapter 11, point instrument creation great time create annotated instrument (Neild, Robinson, Agufa 2022). includes taking copy instrument writing associated codes alongside item (.e., variable name, value codes). annotated instrument can useful data entry process serve linking key instrument data dictionary (see Figure 10.7) (Hart, Schatschneider, Taylor 2018).\nFigure 10.7: Annotated instrument Florida State Twin Registry project\n","code":""},{"path":"collect.html","id":"identifiers","chapter":"10 Data Collection","heading":"10.3.4.3 Identifiers","text":"building data collection tools, matter paper electronic, vitally important make sure collecting unique identifiers (Kopper Parry 2021). Whether participants enter unique identifier form link study ID form way, ’s important accidentally collect anonymous data. Without unique identifiers data, unable link data across time forms. possible, want avoid collecting names unique identifiers following reasons (McKenzie 2010):protect confidentiality want use names little possible forms\nused forms, want remove soon possible\nused forms, want remove soon possibleNames unique\ncollect names, ’ll want ask additional identifying information combined, make participant unique (e.g., student name email)\ncollect names, ’ll want ask additional identifying information combined, make participant unique (e.g., student name email)Names change (e.g., someone gets married/divorced)much room error\nnames hand entered, endless issues case sensitivity, spelling errors, special characters, spacing, forth\nnames hand entered, endless issues case sensitivity, spelling errors, special characters, spacing, forthAll issues make difficult link data. decide collect names, remember need remove names data processing replace unique study identifiers.Figure 10.8 shows data de-identification process looks like (O’Toole et al. 2018). Dataset 1 incoming survey data identifiers, Dataset 2 roster exported participant database (see Chapter 9), Dataset 3 clean, de-identified dataset, created merging Dataset 1 Dataset 2 unique identifier dropping identifying variables. want emphasize importance using “merge” discuss Chapter 13, opposed replacing names IDs hand entering identifiers. possible, want completely avoid hand entry study IDs. Hand entry error-prone can lead many mistakes.\nFigure 10.8: Process creating de-identified dataset\nRather de-identify data cleaning process, another option collect different type unique identifier, pre-link unique study identifiers names instrument, removing many issues (DIME Analytics 2021a; Gibson Louw 2020). discuss methods separately electronic data paper data.Note \nstudy designed collect anonymous data, assign study identifiers participant identifying information collected instruments (e.g., name, email, date birth). also want make sure tool collects identifying metadata IP Address worker IDs case crowdsourcing tools (e.g., MTurk), information included downloaded data. \nRemember collect anonymous data, able link data across measures across time. However, study randomizes participants entity (e.g., school district), need collect identifying information entity order cluster information (e.g., school name).","code":""},{"path":"collect.html","id":"electronic-data","chapter":"10 Data Collection","heading":"10.3.4.3.1 Electronic Data","text":"many ways might consider collecting unique identifiers names. possible options provided . method choose depend data collection design, participant population, tool capabilities, team expertise.Create unique links participants\nMany tools allow preload contact list participants (participant database) includes names study IDs. Using list, tool can create unique links participant. error-proof way ensure study IDs entered correctly.\nexport data, correct ID already linked participant can choose export names data.\nusing method, make sure build data check system. participant opens unique link, verify identity asking, “{first name}?” “initials {initials}?”. order protect participant identities, share full names.\nsay yes, move forward. say , system redirects someone contact. ensures participants completing someone else’s survey IDs connected correct participant.\n\nMany tools allow preload contact list participants (participant database) includes names study IDs. Using list, tool can create unique links participant. error-proof way ensure study IDs entered correctly.export data, correct ID already linked participant can choose export names data.using method, make sure build data check system. participant opens unique link, verify identity asking, “{first name}?” “initials {initials}?”. order protect participant identities, share full names.\nsay yes, move forward. say , system redirects someone contact. ensures participants completing someone else’s survey IDs connected correct participant.\nsay yes, move forward. say , system redirects someone contact. ensures participants completing someone else’s survey IDs connected correct participant.Provide one link participants separately, email, person, mail, provide participants study ID enter system.\nmight preferred method collecting data computer lab tablets school site, tool option create unique links\ncan possibly introduce error participant enters study ID incorrectly.\nSimilar first option, participant enters ID, verify identity\n\nNote participants becoming aware study identifier, identifiers associated participants. However, team, IRB, uncomfortable participants knowing study IDs can also consider using “double ID” yet another set unchanging unique identifiers use sole purpose data collection. identifiers need tracked participant tracking database need replaced study IDs clean data\nmight preferred method collecting data computer lab tablets school site, tool option create unique linksThis can possibly introduce error participant enters study ID incorrectly.\nSimilar first option, participant enters ID, verify identity\nSimilar first option, participant enters ID, verify identityNote participants becoming aware study identifier, identifiers associated participants. However, team, IRB, uncomfortable participants knowing study IDs can also consider using “double ID” yet another set unchanging unique identifiers use sole purpose data collection. identifiers need tracked participant tracking database need replaced study IDs clean dataIf previously assigned study identifiers (.e., consent assent process part instrument), can participants enter identifying information (e.g., name) tool assign unique identifier participants\nUsing method, can potentially download two separate files\nOne just instrument data assigned study ID, name removed\nOne just identifying information assigned study ID (information added participant tracking database)\n\nUsing method, can potentially download two separate files\nOne just instrument data assigned study ID, name removed\nOne just identifying information assigned study ID (information added participant tracking database)\nOne just instrument data assigned study ID, name removedOne just identifying information assigned study ID (information added participant tracking database)","code":""},{"path":"collect.html","id":"paper-data","chapter":"10 Data Collection","heading":"10.3.4.3.2 Paper Data","text":"take paper forms field consider following connect data participant (O’Toole et al. 2018; Reynolds, Schatschneider, Logan 2022).Write study ID, relevant identifiers (e.g., school ID teacher ID), page data collection form use either removable label participant name relevant information place ID attach cover sheet information. return office, can remove name label/cover sheet left ID form.\nID enter data entry form data capture process, name.\nRemoving label/cover sheet also ensures data entry team sees study ID enter data, increasing privacy minimizing number people see see participant names.\nimportant double triple check study identifiers participant database make sure information correct removing label cover sheet\nMake plan labels/cover sheets (either shred longer needed, store securely locked file cabinet shred later point)\nID enter data entry form data capture process, name.Removing label/cover sheet also ensures data entry team sees study ID enter data, increasing privacy minimizing number people see see participant names.important double triple check study identifiers participant database make sure information correct removing label cover sheetMake plan labels/cover sheets (either shred longer needed, store securely locked file cabinet shred later point)\nFigure 10.9: Example cover sheet paper data collection instrument\n","code":""},{"path":"collect.html","id":"quality-control","chapter":"10 Data Collection","heading":"10.4 Quality Control","text":"addition implementing quality assurance measures planning phases, equally important implement several quality control measures data collection underway. measures include:Field data managementOngoing data checksTracking data collection dailyCollecting data consistentlyWe discuss measures now.","code":""},{"path":"collect.html","id":"field-data-management","chapter":"10 Data Collection","heading":"10.4.1 Field data management","text":"data collection efforts include field data collection (e.g., data collectors administering assessments school), several steps team can implement keep data secure field, help project coordinator keep better track happens field, lead accurate usable data. best practices field data collection include following (DIME Analytics 2021a):Keep data secure field\nMake sure paper forms kept folder (even lock box) times promptly returned office (e.g., left car, left someone’s home)\nMake sure data collection devices (e.g., phones, tablets) password protected never left open unattended. Keep identifiable information encrypted field devices (.e., data encoded password can decipher ). may also consider remote wiping capabilities portable devices case loss theft (O’Toole et al. 2018)\nMake sure paper forms kept folder (even lock box) times promptly returned office (e.g., left car, left someone’s home)Make sure data collection devices (e.g., phones, tablets) password protected never left open unattended. Keep identifiable information encrypted field devices (.e., data encoded password can decipher ). may also consider remote wiping capabilities portable devices case loss theft (O’Toole et al. 2018)Create tracking sheets use field\nsheets include names /identifiers every participant data collectors collecting data \nNext participant, include relevant information track \ndata collected (.e., check box)\ncollected data (.e., data collector initials ID)\nDate data collected\nwell notes section describe potential issues data (e.g., “Student leave classroom halfway assessment - partially completed”)\n\ntracking sheet allows project coordinator keep track occurring field information can accurately recorded participant tracking database forms can sent back completion needed\nsheets include names /identifiers every participant data collectors collecting data fromNext participant, include relevant information track \ndata collected (.e., check box)\ncollected data (.e., data collector initials ID)\nDate data collected\nwell notes section describe potential issues data (e.g., “Student leave classroom halfway assessment - partially completed”)\ndata collected (.e., check box)collected data (.e., data collector initials ID)Date data collectedAs well notes section describe potential issues data (e.g., “Student leave classroom halfway assessment - partially completed”)tracking sheet allows project coordinator keep track occurring field information can accurately recorded participant tracking database forms can sent back completion neededCheck paper data field\nImmediately upon completing form, data collectors spot checks. problems found, follow participant correction possible.\nCheck missing data\nCheck duplicate answers given\nCheck answers provided outside assigned area (e.g., answers written margins)\nCheck calculations scoring (e.g., basals, ceilings, raw scores)\n\nImmediately upon completing form, data collectors spot checks. problems found, follow participant correction possible.\nCheck missing data\nCheck duplicate answers given\nCheck answers provided outside assigned area (e.g., answers written margins)\nCheck calculations scoring (e.g., basals, ceilings, raw scores)\nCheck missing dataCheck duplicate answers givenCheck answers provided outside assigned area (e.g., answers written margins)Check calculations scoring (e.g., basals, ceilings, raw scores)Assign field supervisor. person assigned :\nanother round data checks field data collector returns paper forms -site central location (e.g., data collectors set teacher’s lounge)\nEnsure data equipment accounted returned office\navailable trouble shooting needed\nanother round data checks field data collector returns paper forms -site central location (e.g., data collectors set teacher’s lounge)Ensure data equipment accounted returned officeBe available trouble shooting neededDo another round paper data spot checking soon data returned office (see Figure 10.10)\nproject coordinator may round checking tracking information participant database\nissues found, note tracking database send form back field correction\npaper forms mailed back participants, rather returned field data collectors, still important -office spot checks. possible, reach participants corrections.\nproject coordinator may round checking tracking information participant databaseIf issues found, note tracking database send form back field correctionIf paper forms mailed back participants, rather returned field data collectors, still important -office spot checks. possible, reach participants corrections.wave data collection wraps , collect feedback data collectors improve future data collection efforts\nwent well? didn’t?\nwent well? didn’t?\nFigure 10.10: series spot checks occur paper data\nTracking sheet templates","code":""},{"path":"collect.html","id":"ongoing-data-checks","chapter":"10 Data Collection","heading":"10.4.2 Ongoing data checks","text":"collect data via web-based form, want perform frequent data quality checks, similar checks performed content data piloting phase. want check programming errors (.e., skip logic programmed incorrectly) well response quality errors (e.g. bots, survey comprehension) (DIME Analytics 2021a; Gibson 2021).Checks comprehension\nquestions misinterpreted?\nquestions misinterpreted?Checks missing data\nitems skipped skipped?\nparticipants/data collectors finishing forms?\nitems skipped skipped?participants/data collectors finishing forms?Checks ranges formats\nvalues unexpected formats falling outside unexpected ranges?\nvalues unexpected formats falling outside unexpected ranges?Checks duplicate forms\nduplicate entries participants?\nduplicate entries participants?skip logic working expected?\npeople directed correct location based responses items?\npeople directed correct location based responses items?checks can performed programmatically (.e., can write validation script program R, run script recurring schedule data collection check things values range). checks may manual check data (e.g., downloading data recurring schedule reviewing open-ended questions nonsensical responses). errors found, consider revising instrument prevent future errors possible without jeopardizing consistency data.Note \nweb-based data collection efforts chapter assume making private link sharing targeted list (e.g., students classroom, teachers school). However, may times need publicly recruit collect data study opens instrument plethora data quality issues. Bots, fraudulent data, incoherent synthetic responses issues can plague online data collection efforts, particularly crowdsourcing platforms (Douglas, Ewell, Brauer 2023; Veselovsky, Ribeiro, West 2023; Webb Tangney 2022). possible, avoid using public survey links. One possible workaround first create public link screener. participants verified screener, send private, unique link instrument. \nworkaround possible need use public link, suggestions can help secure instrument detect fraud include following (Arndt et al. 2022; Simone 2019; Teitcher et al. 2015):\n- posting link social media\n- Using CAPTCHA verification\n- Using tools allow block suspicious geolocations\n- automating payment upon survey completion\n- Including open-ended questions\n- Building attention/logic checks survey\n- Asking questions twice (early end) \nLast, check data thoroughly bots fraudulent responses analyzing providing payments participants. following types things worth looking :\n- Forms completed short period time\n- Forms collected suspicious geolocations\n- Duplicated nonsensical responses open-ended questions\n- Nonsensical responses attention logic checking questions\n- Inconsistent responses across repeated questions","code":""},{"path":"collect.html","id":"tracking-data-collection","chapter":"10 Data Collection","heading":"10.4.3 Tracking data collection","text":"Throughout data collection team tracking completion forms (e.g., consents, paperwork, data collection forms). team may designate one person track data (e.g., project coordinator), may designate multiple. working across multiple sites, multiple teams, likely one people site tracking data comes .tracking best practices include:track data physically (paper electronic)\nNever track data “complete” someone just tells collected.\ncan always mark information “notes” field track “complete” physical data.\n\nNever track data “complete” someone just tells collected.\ncan always mark information “notes” field track “complete” physical data.\ncan always mark information “notes” field track “complete” physical data.Track daily data collection\nwait end data collection track data collected\nhelps ensure don’t miss opportunity collect data thought never actually collected\nwait end data collection track data collectedThis helps ensure don’t miss opportunity collect data thought never actually collectedOnly track complete data “complete”\nReview data marking complete, including consents, assents, administrative forms. form partially completed plan send back field completion, mark “notes” mark “completed”. “partially completed” option, can mark option.\nReview data marking complete, including consents, assents, administrative forms. form partially completed plan send back field completion, mark “notes” mark “completed”. “partially completed” option, can mark option.","code":""},{"path":"collect.html","id":"collecting-data-consistently","chapter":"10 Data Collection","heading":"10.4.4 Collecting data consistently","text":"mentioned Chapter 8, ’s important collect data consistently entire project ensure interoperability. Keep following consistent across time forms (e.g., Spanish English version form, link SchoolA link SchoolB):Variable names\nUse names items (remember ’s best add time component variable names time)\nUse names items (remember ’s best add time component variable names time)Variable types\nexample, gender collected numeric variable, keep numeric variable\nexample, gender collected numeric variable, keep numeric variableValue codes\nMake sure response options consistently coded using values (e.g., 0 = “”, 1 = “Yes”)\nMake sure response options consistently coded using values (e.g., 0 = “”, 1 = “Yes”)Question type format\nslider question used “Percent time homework”, continue ask question using slider question\nslider question used “Percent time homework”, continue ask question using slider questionFailing collect data consistently many consequences:can make difficult impossible compare outcomesIt makes work less reproducibleIt reduces ability physically combine data (.e., append dissimilar variables)can lead errors interpretationLast, collecting data consistently also means measuring things way time across forms don’t bias results. slightest change item wording response options can result dramatic changes outcomes (ICPSR 2022; Pew Research Center 2023).","code":""},{"path":"collect.html","id":"review","chapter":"10 Data Collection","heading":"10.5 Review","text":"Recall Chapter 5, discussed designing visualizing data collection workflow planning phase. ’ve learned chapter, errors can happen point workflow important consider entire data collection process holistically integrate quality assurance quality control procedures throughout. Figure 10.11 helps us see practices fit different phases workflow.workflow developed quality assurance control practices integrated, consider ensure team implements practices fidelity. Document specifics plan SOP (see Chapter 7), including assigning roles responsibilities task process. Last, train team implement data collection SOP, implement refresher trainings needed.\nFigure 10.11: Integrating quality assurance control data collection workflow\nInstrument Workflow Resources","code":""},{"path":"capture.html","id":"capture","chapter":"11 Data Capture","heading":"11 Data Capture","text":"\nFigure 11.1: Data capture research project life cycle\ndata collection period complete, next phase cycle capture data, meaning extracting, creating, acquiring file can save designated storage location. quantitative research typically want capture data electronic, rectangular format (see Chapter 3). chapter review common ways capture data based three data collection methods (see Figure 11.2). Similar data collection, possible data errors occur phase. reviewing data capture methods, also cover data quality can managed phase.\nFigure 11.2: Common data capture methods\n","code":""},{"path":"capture.html","id":"capture-electronic","chapter":"11 Data Capture","heading":"11.1 Electronic data capture","text":"discussed Chapter 10, electronic data can collected using variety software (either web-based offline). Since electronic forms typically funnel data spreadsheet database, makes process data capture much easier compared paper data. However, still much consider.data captured?\ncommon way capture data electronic format download platform.\nAnother option may web-based forms capture data via API (application programming interface). regularly need review data final capture, using API can great way remove burden manually logging program going point click process downloading file. Instead can write script, program R, extract data. script created, can run often want. However, option tool API available (e.g., Qualtrics).\ncommon way capture data electronic format download platform.Another option may web-based forms capture data via API (application programming interface). regularly need review data final capture, using API can great way remove burden manually logging program going point click process downloading file. Instead can write script, program R, extract data. script created, can run often want. However, option tool API available (e.g., Qualtrics).file type data captured ?\nelectronic data collection tools provide option export one file formats (e.g., SAV, CSV, TXT). important choose file type analyzable (.e., rectangular formatted), opposed something like PDF file. rectangular file type choose mostly depend project plans. Things consider might include:\nwant text values responses numeric values? choice may limit options.\nwant embedded metadata, variable value labels, raw file? , choice narrow options.\nwant non-proprietary, interoperable format? yes, want capture data file types XLSX SAV require proprietary software view.\nfile types create issues variables?\ninstance, Microsoft Excel well-known applying unwanted formatting values. example, assessment tool collects age format years-months, oftentimes Microsoft Excel change variable date, converting value 10-2 (10 years 2 months old) 2-Oct. suitable file types situation may CST TXT files, apply formatting.\n\nfile structure don’t want work ?\nexample, structure SAV file may look different compared XLSX file depending tool. tool like Qualtrics, XLSX CSV file may export multiple header rows whereas SAV file .\n\n\nelectronic data collection tools provide option export one file formats (e.g., SAV, CSV, TXT). important choose file type analyzable (.e., rectangular formatted), opposed something like PDF file. rectangular file type choose mostly depend project plans. Things consider might include:\nwant text values responses numeric values? choice may limit options.\nwant embedded metadata, variable value labels, raw file? , choice narrow options.\nwant non-proprietary, interoperable format? yes, want capture data file types XLSX SAV require proprietary software view.\nfile types create issues variables?\ninstance, Microsoft Excel well-known applying unwanted formatting values. example, assessment tool collects age format years-months, oftentimes Microsoft Excel change variable date, converting value 10-2 (10 years 2 months old) 2-Oct. suitable file types situation may CST TXT files, apply formatting.\n\nfile structure don’t want work ?\nexample, structure SAV file may look different compared XLSX file depending tool. tool like Qualtrics, XLSX CSV file may export multiple header rows whereas SAV file .\n\nwant text values responses numeric values? choice may limit options.want embedded metadata, variable value labels, raw file? , choice narrow options.want non-proprietary, interoperable format? yes, want capture data file types XLSX SAV require proprietary software view.file types create issues variables?\ninstance, Microsoft Excel well-known applying unwanted formatting values. example, assessment tool collects age format years-months, oftentimes Microsoft Excel change variable date, converting value 10-2 (10 years 2 months old) 2-Oct. suitable file types situation may CST TXT files, apply formatting.\ninstance, Microsoft Excel well-known applying unwanted formatting values. example, assessment tool collects age format years-months, oftentimes Microsoft Excel change variable date, converting value 10-2 (10 years 2 months old) 2-Oct. suitable file types situation may CST TXT files, apply formatting.file structure don’t want work ?\nexample, structure SAV file may look different compared XLSX file depending tool. tool like Qualtrics, XLSX CSV file may export multiple header rows whereas SAV file .\nexample, structure SAV file may look different compared XLSX file depending tool. tool like Qualtrics, XLSX CSV file may export multiple header rows whereas SAV file .additional formatting options need considered?\naddition choosing file type, may options tool allows consider. Examples might look like :\nwant export text values numeric values categorical items?\nwant export “select ” questions?\nDepending chosen file type, may allowed choose want format “select ” questions. Typically options export one variable, option separated comma, can split option column.\n\nwant recode seen missing values?\noption commonly provided “select ” questions often split multiple variables, 1 indicates option selected, blank represents either option selected item skipped entirely.\nTypically tools provide option recode 0 -99. can also choose recode leave responses blank. types missing data matter study, leaving missing values blank typically straightforward option. However, adding extreme value like -99 can make easier know blank “select ” values “” response (recoded -99) values never seen actually represent missing value (left blank).\n\n\naddition choosing file type, may options tool allows consider. Examples might look like :\nwant export text values numeric values categorical items?\nwant export “select ” questions?\nDepending chosen file type, may allowed choose want format “select ” questions. Typically options export one variable, option separated comma, can split option column.\n\nwant recode seen missing values?\noption commonly provided “select ” questions often split multiple variables, 1 indicates option selected, blank represents either option selected item skipped entirely.\nTypically tools provide option recode 0 -99. can also choose recode leave responses blank. types missing data matter study, leaving missing values blank typically straightforward option. However, adding extreme value like -99 can make easier know blank “select ” values “” response (recoded -99) values never seen actually represent missing value (left blank).\n\nwant export text values numeric values categorical items?want export “select ” questions?\nDepending chosen file type, may allowed choose want format “select ” questions. Typically options export one variable, option separated comma, can split option column.\nDepending chosen file type, may allowed choose want format “select ” questions. Typically options export one variable, option separated comma, can split option column.want recode seen missing values?\noption commonly provided “select ” questions often split multiple variables, 1 indicates option selected, blank represents either option selected item skipped entirely.\nTypically tools provide option recode 0 -99. can also choose recode leave responses blank. types missing data matter study, leaving missing values blank typically straightforward option. However, adding extreme value like -99 can make easier know blank “select ” values “” response (recoded -99) values never seen actually represent missing value (left blank).\noption commonly provided “select ” questions often split multiple variables, 1 indicates option selected, blank represents either option selected item skipped entirely.Typically tools provide option recode 0 -99. can also choose recode leave responses blank. types missing data matter study, leaving missing values blank typically straightforward option. However, adding extreme value like -99 can make easier know blank “select ” values “” response (recoded -99) values never seen actually represent missing value (left blank).file stored named?\ndecision based guidelines laid style guide (see Chapter 8). Important things consider :\ndata contains identifiable information (e.g., name, IP Address, email), needs stored securely limited access (see Chapter 12).\ntool may provide name file, give meaningful name based style guide rules. name indicate raw data file, along relevant metadata (e.g., project acronym, type data, data collection wave, date file downloaded)\n\ndecision based guidelines laid style guide (see Chapter 8). Important things consider :\ndata contains identifiable information (e.g., name, IP Address, email), needs stored securely limited access (see Chapter 12).\ntool may provide name file, give meaningful name based style guide rules. name indicate raw data file, along relevant metadata (e.g., project acronym, type data, data collection wave, date file downloaded)\ndata contains identifiable information (e.g., name, IP Address, email), needs stored securely limited access (see Chapter 12).tool may provide name file, give meaningful name based style guide rules. name indicate raw data file, along relevant metadata (e.g., project acronym, type data, data collection wave, date file downloaded)documentation needs accompany data capture?\ndiscussed Section 7.3, additional documents can helpful include alongside file.\nReadme can beneficial include anything file future person managing data aware .\nchangelog can also beneficial. common redownload raw data file due errors found new participants added. changelog can help team identify recent version raw data file, well understand differences files.\n\ndiscussed Section 7.3, additional documents can helpful include alongside file.\nReadme can beneficial include anything file future person managing data aware .\nchangelog can also beneficial. common redownload raw data file due errors found new participants added. changelog can help team identify recent version raw data file, well understand differences files.\nReadme can beneficial include anything file future person managing data aware .changelog can also beneficial. common redownload raw data file due errors found new participants added. changelog can help team identify recent version raw data file, well understand differences files.capture data?\ndoesn’t necessarily matter takes responsibility. matters person expertise capture data responsibility documented. person capturing data person oversees data collection, important still assign person responsibility documenting relevant information Readme.\ndoesn’t necessarily matter takes responsibility. matters person expertise capture data responsibility documented. person capturing data person oversees data collection, important still assign person responsibility documenting relevant information Readme.checks need happen data handed ?\nimportant person responsible data capture basic review file handing data next step.\nformat file look expected? data ? variables expected?\nparticipants data? excellent time compare number unique participants file number participants completed data participant tracking database. numbers match, person charge data capture begin reconciling errors handing data.\nparticipant accidentally dropped file? someone incorrectly marked complete tracking database? duplicate entries file?\nerrors can corrected (e.g., someone incorrectly tracked data point, participant left capture process), corrections made now. corrections involve manipulating raw data (e.g., reconciling duplicate IDs data), corrections made time. Instead, added Readme file corrected data cleaning phase.\n\n\n\nimportant person responsible data capture basic review file handing data next step.\nformat file look expected? data ? variables expected?\nparticipants data? excellent time compare number unique participants file number participants completed data participant tracking database. numbers match, person charge data capture begin reconciling errors handing data.\nparticipant accidentally dropped file? someone incorrectly marked complete tracking database? duplicate entries file?\nerrors can corrected (e.g., someone incorrectly tracked data point, participant left capture process), corrections made now. corrections involve manipulating raw data (e.g., reconciling duplicate IDs data), corrections made time. Instead, added Readme file corrected data cleaning phase.\n\n\nformat file look expected? data ? variables expected?participants data? excellent time compare number unique participants file number participants completed data participant tracking database. numbers match, person charge data capture begin reconciling errors handing data.\nparticipant accidentally dropped file? someone incorrectly marked complete tracking database? duplicate entries file?\nerrors can corrected (e.g., someone incorrectly tracked data point, participant left capture process), corrections made now. corrections involve manipulating raw data (e.g., reconciling duplicate IDs data), corrections made time. Instead, added Readme file corrected data cleaning phase.\n\nparticipant accidentally dropped file? someone incorrectly marked complete tracking database? duplicate entries file?\nerrors can corrected (e.g., someone incorrectly tracked data point, participant left capture process), corrections made now. corrections involve manipulating raw data (e.g., reconciling duplicate IDs data), corrections made time. Instead, added Readme file corrected data cleaning phase.\nerrors can corrected (e.g., someone incorrectly tracked data point, participant left capture process), corrections made now. corrections involve manipulating raw data (e.g., reconciling duplicate IDs data), corrections made time. Instead, added Readme file corrected data cleaning phase.Note \nimportant never make changes directly raw data files. also includes making changes directly data data collection tool. see errors raw data file can’t fixed simply re-downloading data, make notes Readme future correction noted . corrections can made data cleaning process. one exception rule accidentally collect data non-consented participant. case, may best delete data participant directly data collection tool record kept.decisions made documented time developing data collection tools. Making decisions early allows also implement pilot testing data checking processes. instance, plan capture data exporting CSV file data collection platform variety options selected, want use method data piloting data checking process. allows know exactly data look like data collection complete make adjustments needed.discussed Chapter 5, data capture process added workflow diagram detailed SOP. decisions exist relevant SOP. ensures workflows standardized reproducible. ’ve learned section, one deviation SOP potential produce different data product (e.g., format CSV file compared SAV file can vary). can produce errors can also undermine reproducibility data cleaning pipeline. Imagine scenario data cleaning syntax written import CSV file expected format, format changes. pipeline longer reproducible. Last, documenting timeline data capture process occur can also beneficial person responsible data capture, well people responsible subsequent phases data cleaning.","code":""},{"path":"capture.html","id":"paper-data-capture","chapter":"11 Data Capture","heading":"11.2 Paper data capture","text":"common method capturing paper forms manual entry. capturing electronic data fairly quick straightforward, planning implementing paper data entry much involved. Similar electronic data collection, want start planning data entry long data collected, need build data entry tool data capture phase (e.g., creating data collection tools).can imagine, manually entering data comes potential many data quality issues. developing data entry process, important implement quality assurance practices similar discussed Section 10.3.Choose quality data entry toolBuild data entry form end mindDevelop data entry procedure","code":""},{"path":"capture.html","id":"choose-a-quality-data-entry-tool","chapter":"11 Data Capture","heading":"11.2.1 Choose a quality data entry tool","text":"choosing data entry tool, already using relational database participant tracking, may make sense use database data entry data can stored one location tables can linked. However, need choose new tool data entry, criteria choosing one similar reviewed Section 10.3.3. Considerations project needs, security, costs, data quality still reviewed.addition reviewing criteria, can also beneficial use tool allows create entry forms, similar form saw Figure 9.8, rather entering directly spreadsheet. Building data entry form laid similar paper form can help reduce errors data entry. Data entered form fed table can exported.however, choose use spreadsheet program SPSS Microsoft Excel data entry, important aware limitations possible issues tools including:Possible formatting issues\nexample, Microsoft Excel formatting may cause errors data (e.g., dates get formatted numeric, strings get formatted dates, leading zeros get dropped values)\nexample, Microsoft Excel formatting may cause errors data (e.g., dates get formatted numeric, strings get formatted dates, leading zeros get dropped values)Potential skip around\nspreadsheet, ability click anywhere makes easy enter data wrong cell skip cells completely (Eaker 2016). may even write existing data accident. ’s also possible incorrectly sort data resulting errors (Reynolds, Schatschneider, Logan 2022).\nspreadsheet, ability click anywhere makes easy enter data wrong cell skip cells completely (Eaker 2016). may even write existing data accident. ’s also possible incorrectly sort data resulting errors (Reynolds, Schatschneider, Logan 2022).","code":""},{"path":"capture.html","id":"build-with-the-end-in-mind","chapter":"11 Data Capture","heading":"11.2.2 Build with the end in mind","text":"export save dataset data entry tool, meet data structure rules (see Chapter 3), variables formatted described data dictionary, including correct name, variable type, allowable values. order accomplish goal, need build data entry screens, whether spreadsheet form layout, following rules similar discussed Section 10.3.4.Make sure items laid order appear paper form people entering data can easily follow flow (Reynolds, Schatschneider, Logan 2022).Using annotated instrument discussed Section 10.3.4.2, name items data entry screen match final item names (e.g., instead “Q2” use final name “tch_years”).quicker data entry, less errors, allow people enter numeric values associated response options annotated instrument rather text values (e.g., enter “1” rather “strongly disagree”). prefer use text values, build drop-values, removing variation entry.matter data entry tool choose, make sure include content response validation\nRestrict data type, format, ranges, values\nallow people skip items\nRestrict data type, format, ranges, valuesDo allow people skip itemsBefore releasing data entry tool world, want pilot issues, just like electronic data collection tools (see Section 10.3.4.1). Collect sample responses team members collect feedback work well entering data. download, using chosen download format, simply review data already final format (e.g., Microsoft Excel). Check data looks expect make edits entry tool needed.","code":""},{"path":"capture.html","id":"develop-a-data-entry-procedure","chapter":"11 Data Capture","heading":"11.2.3 Develop a data entry procedure","text":"building reliable data entry tool absolutely important ensuring data quality, developing clear standard data entry process even important. Make sure create data entry process includes following things.paper forms stored, pulled, returned.\nConsider organizing forms way people entering data know entered entered\nConsider organizing forms way people entering data know entered enteredWhere electronic entry databases files stored named.\nSimilar Section 11.1, want name files according style guide (e.g., proja_w1_stu_svy_raw_entry1.xlsx).\nSimilar Section 11.1, want name files according style guide (e.g., proja_w1_stu_svy_raw_entry1.xlsx).Specific data entry rules follow\nvalues enter categorical variables (numeric values text values)\nitems allow free-text entry, provide specific data entry rules prevent inconsistencies. adding data validation help remove inconsistencies, rules may needed depending items. example:\nEnter decimals leading zero (e.g. “0.4” “.4”)\nEnter “yes” values “Y” (e.g., change “y” “yes” “Y”)\nenter numeric values measurements (e.g., “5” “5cm”)\n\ncode missing data\nsomeone comes across variety common data errors. example:\nSomeone circled one response item\nSomeone written responses margin\nSomeone written value range unallowable response\n\nvalues enter categorical variables (numeric values text values)items allow free-text entry, provide specific data entry rules prevent inconsistencies. adding data validation help remove inconsistencies, rules may needed depending items. example:\nEnter decimals leading zero (e.g. “0.4” “.4”)\nEnter “yes” values “Y” (e.g., change “y” “yes” “Y”)\nenter numeric values measurements (e.g., “5” “5cm”)\nEnter decimals leading zero (e.g. “0.4” “.4”)Enter “yes” values “Y” (e.g., change “y” “yes” “Y”)enter numeric values measurements (e.g., “5” “5cm”)code missing dataWhat someone comes across variety common data errors. example:\nSomeone circled one response item\nSomeone written responses margin\nSomeone written value range unallowable response\nSomeone circled one response itemSomeone written responses marginSomeone written value range unallowable responseWho reach questions.denote form entered.\nexample, staff can write initials form entry\nexample, staff can write initials form entryWho notify forms entered.Steps performed handing entered data.\nSimilar process Section 11.1, imperative whoever overseeing data entry process check data handing next step data cleaning. importantly, check see correct number participants exist file compared participant tracking database (e.g., duplicate entries, missing entries). data entry data tracking errors exist, fix mistakes needed. inherent data issues exist, make notes Readme corrected data cleaning phase.\nSimilar process Section 11.1, imperative whoever overseeing data entry process check data handing next step data cleaning. importantly, check see correct number participants exist file compared participant tracking database (e.g., duplicate entries, missing entries). data entry data tracking errors exist, fix mistakes needed. inherent data issues exist, make notes Readme corrected data cleaning phase.\nFigure 11.3: flow decisions make regarding data entry process\nNote \nreminder, data capture phase time , capture data already collected. time score, calculate, add additional fields. time enter exact items found form. Calculations, adding creating variables, data quality checks occur data cleaning phase. \nexception rule collected assessment requires entry proprietary scoring program. data entered, tools often export file includes derived scores assessment still considered raw captured data sources. aside though, proprietary scoring program export raw item values along derived scores, consider first manually entering raw items using data capture process, entering values scoring program can capture raw values derived scores. types values can beneficial include final project datasets.","code":""},{"path":"capture.html","id":"double-entry","chapter":"11 Data Capture","heading":"11.2.3.1 Double entry","text":"Last, important integrate quality control data entry process. studies, Schmitt Burchinal (2011) found error-rate 5-10% data entered second person double-check data entry improves data quality. several ways double checking data including visual checking read aloud methods, double entry method shown reliable error-reducing technique (Barchard et al. 2020), ensuring displayed paper form entered database. typical double entry process looks something like :designated team member creates two identical entry forms. One person enters forms first entry screen, different person enters forms second entry screen. Depending tool might two separate files, two separate tabs spreadsheet, two separate tables forms database.\nimportant second entry completed different person systematic errors created one person’s interpretation information repeated across files.\nimportant second entry completed different person systematic errors created one person’s interpretation information repeated across files.entries complete, system used check inconsistencies across datasets.\nsystem varies across tools. tools built systematic ways check errors across entry screens. tools may require build system (e.g., write formulas compare cells draft syntax compare spreadsheets). Ultimately, comparisons done, report tells errors exist across two forms.\nsystem varies across tools. tools built systematic ways check errors across entry screens. tools may require build system (e.g., write formulas compare cells draft syntax compare spreadsheets). Ultimately, comparisons done, report tells errors exist across two forms.Using report, designated team member/members makes corrections (Yenni et al. 2019). involves pulling original paper forms seeing correct value error.\nvarying ways can make corrections point. can make corrections just one form, can make corrections forms, can make corrections third, new form contains correct data. Different tools handle different ways.\nHowever, creating system, consider making corrections forms. way, make correction ever entry file error. corrections made, can run comparison system , now let know errors corrected. errors fixed, can simply choose either file “master” raw data file.\nFigure 11.4 example using process. Data entered two spreadsheets, files imported R comparison program run check errors report returned 76. can see identifies error “stress1” variable. Entry file 1 (“BASE”) different value entry file 2 (“COMPARE”). now need go back original files see actual reported answer fix value corresponding file. value incorrect files, correct run comparison system ensure errors exist handing file .\nvarying ways can make corrections point. can make corrections just one form, can make corrections forms, can make corrections third, new form contains correct data. Different tools handle different ways.However, creating system, consider making corrections forms. way, make correction ever entry file error. corrections made, can run comparison system , now let know errors corrected. errors fixed, can simply choose either file “master” raw data file.Figure 11.4 example using process. Data entered two spreadsheets, files imported R comparison program run check errors report returned 76. can see identifies error “stress1” variable. Entry file 1 (“BASE”) different value entry file 2 (“COMPARE”). now need go back original files see actual reported answer fix value corresponding file. value incorrect files, correct run comparison system ensure errors exist handing file .\nFigure 11.4: report displaying differences two entry files\nDepending amount data collected can time consuming process. Double data entry matter weighing costs benefits. double entering data best way reduce data errors, cost double entering data might high, may decide double enter portion data gain smaller benefit.Whatever decisions throughout process, document every decision SOP assign team members step. includes assigning someone create double entry files, oversee data entry, create double entry comparison system, conduct comparison, make corrections, final checks handing data . Make sure train team system implemented consistently.Note \nentering data proprietary scoring system provide double entry option, make sure consider ways can reduce data entry errors (e.g., batch upload double entered raw values).","code":""},{"path":"capture.html","id":"scanning-forms","chapter":"11 Data Capture","heading":"11.2.4 Scanning forms","text":"Although less common now, possible may collect paper data using forms can scanned converted automatically machine-readable dataset. Depending whether team personally scanning whether external company captures data, potential save time energy compared manual data entry process. may also potential less error-prone manual entry, yet process still error-free caution taken capturing data (Jørgensen Karlsmose 1998). still important data checks ensure correct values recorded electronic file.","code":""},{"path":"capture.html","id":"capture-extant","chapter":"11 Data Capture","heading":"11.3 Extant data","text":"common education research also capture external supplemental data sources either link original data sources describe information sample. process collecting data vary widely depending source. Furthermore, quality usability data can also vary widely. section going review practices help acquire better, interpretable data. divide discussion two types data sources, non-public public.","code":""},{"path":"capture.html","id":"non-public-data-sources","chapter":"11 Data Capture","heading":"11.3.1 Non-public data sources","text":"Non-public data sources files directly accessed public website. sources often individual level may contain protected sensitive information (e.g., student school records). Acquiring sources typically involves data request process (see Figure 11.5) may also include one agreements discussed Section 10.2.3 (e.g., participant consent, DUA).\nFigure 11.5: Example non-public confidential data request process\nalready included provider’s data request process, important share following information:list variables requesting\nplan link data, make sure request unique identifier ’ve collected exists external data (e.g., state student unique identifier), combination identifiers (e.g., name DOB), allows link external data existing original data.\nplanning combine data multiple sources (e.g., multiple school districts), can require hours harmonization make data comparable due variations data collected across agencies. flexibility request process, can helpful provide details data provider like variables formatted, helping standardize inputs removing room interpretation (Feeney et al. 2021).\nVariable type (e.g., numeric, text)\nVariable formats (e.g., DOB YYYY-MM-DD)\nValue groups (e.g., specify code free/reduced lunch groups)\nhandle missing data (e.g., leave cell blank)\naggregate summary data (e.g., Number days absent full year term)\ncalculated variables (e.g., age assessment) consider requesting raw inputs calculate values (e.g., request date assessment DOB)\n\nplan link data, make sure request unique identifier ’ve collected exists external data (e.g., state student unique identifier), combination identifiers (e.g., name DOB), allows link external data existing original data.planning combine data multiple sources (e.g., multiple school districts), can require hours harmonization make data comparable due variations data collected across agencies. flexibility request process, can helpful provide details data provider like variables formatted, helping standardize inputs removing room interpretation (Feeney et al. 2021).\nVariable type (e.g., numeric, text)\nVariable formats (e.g., DOB YYYY-MM-DD)\nValue groups (e.g., specify code free/reduced lunch groups)\nhandle missing data (e.g., leave cell blank)\naggregate summary data (e.g., Number days absent full year term)\ncalculated variables (e.g., age assessment) consider requesting raw inputs calculate values (e.g., request date assessment DOB)\nVariable type (e.g., numeric, text)Variable formats (e.g., DOB YYYY-MM-DD)Value groups (e.g., specify code free/reduced lunch groups)handle missing data (e.g., leave cell blank)aggregate summary data (e.g., Number days absent full year term)calculated variables (e.g., age assessment) consider requesting raw inputs calculate values (e.g., request date assessment DOB)Figure 11.6 example might provide information data provider.\nFigure 11.6: Example variable request external data provider\nClarify periods requesting data \nmay current year alone, may also need previous year well comparison\nmay current year alone, may also need previous year well comparisonAsk data shared\nAsk many data files provided file contain (e.g., enrollment file, assessment file, attendance file)\nRequest timeline data shared\nProvide preferred file format data (e.g., CSV file)\nAsk data shared (e.g., email, drop secure folder). data contain identifiable information, make sure use secure file transfer method (see Chapter 12). received, make sure follow data sharing agreements around data stored.\nAsk many data files provided file contain (e.g., enrollment file, assessment file, attendance file)Request timeline data sharedProvide preferred file format data (e.g., CSV file)Ask data shared (e.g., email, drop secure folder). data contain identifiable information, make sure use secure file transfer method (see Chapter 12). received, make sure follow data sharing agreements around data stored.Identify points contact\nneed contact information acquiring data, also need know contact questions concerns come data received.\nneed contact information acquiring data, also need know contact questions concerns come data received.Request documentation accompany file\nReceiving data dictionaries codebooks along data vital allowing correctly interpret variables. especially important observing variations variables measured across sites even within sites across time (e.g., test score measured differently subsequent year)\ndocumentation exist, provide data provider form complete allows enter relevant, variable information.\nvariable represents\nvalue represents variable categorical\nvariable captured calculated (e.g., hand entered)\nuniverse variable (e.g., grades 3-5)\ndata quality concerns variables\n\nreceive new exports year, make sure request documentation year. possible way variables collected recorded change time.\nReceiving data dictionaries codebooks along data vital allowing correctly interpret variables. especially important observing variations variables measured across sites even within sites across time (e.g., test score measured differently subsequent year)documentation exist, provide data provider form complete allows enter relevant, variable information.\nvariable represents\nvalue represents variable categorical\nvariable captured calculated (e.g., hand entered)\nuniverse variable (e.g., grades 3-5)\ndata quality concerns variables\nvariable representsWhat value represents variable categoricalHow variable captured calculated (e.g., hand entered)universe variable (e.g., grades 3-5)data quality concerns variablesIf receive new exports year, make sure request documentation year. possible way variables collected recorded change time.Figure 11.7 example document can ask data provider complete.\nFigure 11.7: Sample documentation form external data provider complete\nNote \nworking external datasets, possible encounter inconsistencies across data sources (e.g., student shown different school across two files), well duplicate records within data source (e.g., student two state reading assessment scores) (Levesque, Fitzgerald, Pfeiffer 2015). anomalies can happen due human error due circumstances student mobility. may able work data provider solve data issues, others may important develop document data management rules consistently apply external data sources data cleaning phase (e.g., duplicate assessment records exist, earliest assessment date used).","code":""},{"path":"capture.html","id":"public-data-sources","chapter":"11 Data Capture","heading":"11.3.2 Public data sources","text":"Publicly available data sources typically aggregated (.e., state, district, school level) de-identified individual level datasets available various agencies state departments education federal agencies. datasets often extracted downloading file, although organizations may sophisticated API capabilities. quality datasets may vary. tips working publicly available datasets :Find associated documentation read thoroughly. Types documentation look :\nData dictionaries codebooks\ndocuments help interpret use variables correctly\n\nChangelogs\nPublic data sources constantly updating (e.g., new data acquired, errors found). ’s important understand version data working .\n\nData quality documentation\ndocumentation helps make aware known issues data\n\nData dictionaries codebooks\ndocuments help interpret use variables correctly\ndocuments help interpret use variables correctlyChangelogs\nPublic data sources constantly updating (e.g., new data acquired, errors found). ’s important understand version data working .\nPublic data sources constantly updating (e.g., new data acquired, errors found). ’s important understand version data working .Data quality documentation\ndocumentation helps make aware known issues data\ndocumentation helps make aware known issues dataDo hesitate reach help\nTypically site include contact information questions. Never hesitate reach contact something understand data.\nTypically site include contact information questions. Never hesitate reach contact something understand data.extract data across states (e.g., Missouri Department Elementary Secondary Education Oklahoma State Department Education), aware information may easily comparable. may find states use similar standards, common states collect store data different ways (e.g., different state assessments, different ways reporting enrollment). Depending data needs, may better use data source aggregates information across states. Examples data sources include Department Education’s Common Core Data 77 EDFacts 78. needing use multiple data sources, tools, Urban Institute’s Education Data Portal 79, even harmonized variables documentation across several federal government datasets, allowing researchers access multiple data sources single site.","code":""},{"path":"storage.html","id":"storage","chapter":"12 Data Storage and Security","heading":"12 Data Storage and Security","text":"","code":""},{"path":"storage.html","id":"types-of-data-youll-be-storing","chapter":"12 Data Storage and Security","heading":"12.1 Types of data you’ll be storing","text":"","code":""},{"path":"storage.html","id":"general-security-rules","chapter":"12 Data Storage and Security","heading":"12.2 General security rules","text":"","code":""},{"path":"storage.html","id":"trackingstorage","chapter":"12 Data Storage and Security","heading":"12.3 Participant tracking database","text":"","code":""},{"path":"storage.html","id":"electronic-data-1","chapter":"12 Data Storage and Security","heading":"12.4 Electronic data","text":"","code":""},{"path":"storage.html","id":"detachable-media","chapter":"12 Data Storage and Security","heading":"12.5 Detachable media","text":"","code":""},{"path":"storage.html","id":"audiovisual-data","chapter":"12 Data Storage and Security","heading":"12.6 Audio/visual data","text":"","code":""},{"path":"storage.html","id":"paper-data-1","chapter":"12 Data Storage and Security","heading":"12.7 Paper data","text":"","code":""},{"path":"storage.html","id":"sharing-data","chapter":"12 Data Storage and Security","heading":"12.8 Sharing data","text":"","code":""},{"path":"clean.html","id":"clean","chapter":"13 Data Cleaning","heading":"13 Data Cleaning","text":"","code":""},{"path":"clean.html","id":"foundational-knowledge","chapter":"13 Data Cleaning","heading":"13.1 Foundational knowledge","text":"","code":""},{"path":"clean.html","id":"data-structure","chapter":"13 Data Cleaning","heading":"13.2 Data structure","text":"","code":""},{"path":"clean.html","id":"data-cleaning-plan-1","chapter":"13 Data Cleaning","heading":"13.3 Data cleaning plan","text":"","code":""},{"path":"clean.html","id":"data-validation","chapter":"13 Data Cleaning","heading":"13.4 Data validation","text":"","code":""},{"path":"clean.html","id":"why-use-code","chapter":"13 Data Cleaning","heading":"13.5 Why use code?","text":"","code":""},{"path":"share.html","id":"share","chapter":"14 Data Sharing","heading":"14 Data Sharing","text":"","code":""},{"path":"share.html","id":"why-share-your-data","chapter":"14 Data Sharing","heading":"14.1 Why share your data?","text":"","code":""},{"path":"share.html","id":"considering-fair-principles","chapter":"14 Data Sharing","heading":"14.2 Considering FAIR principles","text":"","code":""},{"path":"share.html","id":"best-practices","chapter":"14 Data Sharing","heading":"14.3 Best practices","text":"","code":""},{"path":"share.html","id":"retractions-and-revisions","chapter":"14 Data Sharing","heading":"14.4 Retractions and revisions","text":"","code":""},{"path":"wrapping-it-up-1.html","id":"wrapping-it-up-1","chapter":"15 Wrapping It Up","heading":"15 Wrapping It Up","text":"Diagram accomplished phase","code":""},{"path":"wrapping-it-up-1.html","id":"connecting-practices-to-outcomes","chapter":"15 Wrapping It Up","heading":"15.1 Connecting practices to outcomes","text":"","code":""},{"path":"wrapping-it-up-1.html","id":"putting-in-the-work","chapter":"15 Wrapping It Up","heading":"15.2 Putting in the work","text":"","code":""},{"path":"call-to-action.html","id":"call-to-action","chapter":"16 Call to Action","heading":"16 Call to Action","text":"","code":""},{"path":"call-to-action.html","id":"last-thoughts","chapter":"16 Call to Action","heading":"16.1 Last thoughts","text":"","code":""},{"path":"call-to-action.html","id":"training-for-future-researchers","chapter":"16 Call to Action","heading":"16.2 Training for future researchers","text":"","code":""},{"path":"call-to-action.html","id":"investing-in-data-management-and-data-managers","chapter":"16 Call to Action","heading":"16.3 Investing in data management and data managers","text":"","code":""},{"path":"glossary.html","id":"glossary","chapter":"17 Glossary","heading":"17 Glossary","text":"","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
