
@article{tenopir_data_2016,
	title = {Data {Management} {Education} from the {Perspective} of {Science} {Educators}},
	volume = {11},
	copyright = {Copyright (c)},
	issn = {1746-8256},
	url = {http://www.ijdc.net/article/view/11.1.232},
	doi = {10.2218/ijdc.v11i1.389},
	abstract = {In order to better understand the current state of data management education in multiple fields of science, this study surveyed scientists, including information scientists, about their data management education practices, including at what levels they are teaching data management, which topics they covering, and what barriers they experience in teaching these topics. We found that a handful of scientists are teaching data management in undergraduate, graduate, and other types of courses, as well as outside of classroom settings. Commonly taught data management topics included quality control, protecting data, and management planning. However, few instructors felt they were covering data management topics thoroughly, and respondents cited barriers such as lack of time, lack of necessary expertise, and lack of information for teaching data management. We offer some potential explanations for the existing state of data management education and suggest areas for further research.},
	language = {en},
	number = {1},
	urldate = {2022-09-15},
	journal = {International Journal of Digital Curation},
	author = {Tenopir, Carol and Allard, Suzie and Sinha, Priyanki and Pollock, Danielle and Newman, Jess and Dalton, Elizabeth and Frame, Mike and Baird, Lynn},
	month = oct,
	year = {2016},
	keywords = {curation, DCC, digital curation, digital preservation, IJDC, International Journal of Digital Curation, preservation},
	pages = {232--251},
}

@article{baker_1500_2016,
	title = {1,500 scientists lift the lid on reproducibility},
	volume = {533},
	copyright = {2016 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/533452a},
	doi = {10.1038/533452a},
	abstract = {Survey sheds light on the ‘crisis’ rocking research.},
	language = {en},
	number = {7604},
	urldate = {2022-09-15},
	journal = {Nature},
	author = {Baker, Monya},
	month = may,
	year = {2016},
	keywords = {Peer review, Publishing, Research management},
	pages = {452--454},
}

@misc{osf_cos_2022,
	title = {{COS} {Engagement} with the {Education} {Community}},
	url = {https://docs.google.com/presentation/d/1LpyVOj8oJPr3SVkRM2GfCFnl2Qeo10YbbqcqwtwrVUM},
	urldate = {2022-09-15},
	author = {OSF},
	year = {2022},
}

@misc{doucette_drowning_2013,
	title = {Drowning in {Research} {Data}: {Addressing} {Data} {Management} {Literacy} of {Graduate} {Students} - {PDF} {Free} {Download}},
	shorttitle = {Drowning in {Research} {Data}},
	url = {https://docplayer.net/8853333-Drowning-in-research-data-addressing-data-management-literacy-of-graduate-students.html},
	abstract = {Drowning in Research Data: Addressing Data Management Literacy of Graduate Students Introduction Graduate students work in increasingly complex research environments where advances in technology and research},
	urldate = {2022-09-15},
	author = {Doucette, Lise and Fyfe, Bruce},
	year = {2013},
}

@article{campos-varela_misconduct_2019,
	title = {Misconduct as the main cause for retraction. {A} descriptive study of retracted publications and their authors},
	volume = {33},
	issn = {0213-9111},
	url = {https://www.sciencedirect.com/science/article/pii/S0213911118300724},
	doi = {10.1016/j.gaceta.2018.01.009},
	abstract = {Objective
To analyze the causes of retracted publications and the main characteristics of their authors.
Method
A descriptive cross-sectional study was designed including all retracted publications from January 1st, 2013-December 31st, 2016 indexed in PubMed. The causes of retraction were classified as: data management, authorship issues, plagiarism, unethical research, journal issues, review process, conflict of interest, other causes, and unknown reasons. Then, misbehaviour was classified as misconduct, suspicion of misconduct or no misconduct suspicion.
Results
1,082 retracted publications were identified. The retraction rate for the period was 2.5 per 10,000 publications. The main cause of retraction was misconduct (65.3\%), and the leading reasons were plagiarism, data management and compromise of the review process. The highest proportion of retracted publications corresponded to Iran (15.52 per 10,000), followed by Egypt and China (11.75 and 8.26 per 10,000).
Conclusions
Currently, misconduct is the main cause of retraction. Specific strategies to limit this phenomenon must be implemented. It would be useful to standardize reasons and procedures for retraction. The development of a standard retraction form to be permanently indexed in a database might be relevant.
Resumen
Objetivo
Analizar las causas de las retractaciones y las características fundamentales de sus autores.
Método
Se diseñó un estudio descriptivo, transversal, que incluyó todas las publicaciones con retractación entre el 1 de enero de 2013 y el 31 de diciembre de 2016 indexadas en PubMed. Las causas de la retractación fueron clasificadas como manejo de datos, asuntos de autoría, plagio, investigación no ética, asuntos de las revistas, proceso de revisión, conflictos de intereses, otras causas y razones desconocidas. Tras esto, la conducta indebida fue clasificada como mala conducta, sospecha de mala conducta y sin sospecha de mala conducta.
Resultados
Se identificaron 1.082 publicaciones retractadas. La proporción de publicaciones retractadas fue de 2,5 por cada 10.000 publicaciones para el periodo evaluado. La principal causa de retractación fue la mala conducta (65,3\%), y las causas principales fueron plagio, manejo de los datos y compromiso del proceso de revisión. La mayor proporción de publicaciones retractadas correspondió a Irán (15,52 por 10.000), seguido de Egipto y China (11,75 y 8,26 por 10.000, respectivamente).
Conclusiones
Actualmente, la mala conducta es la principal causa de retractación. Deberían implementarse estrategias específicas para limitar este fenómeno. Sería de utilidad uniformar los motivos y procedimientos para la retractación. Podría ser apropiado el desarrollo de un formulario estándar de retractación que sea indexado permanentemente en una base de datos.},
	language = {en},
	number = {4},
	urldate = {2022-09-15},
	journal = {Gaceta Sanitaria},
	author = {Campos-Varela, Isabel and Ruano-Raviña, Alberto},
	month = jul,
	year = {2019},
	keywords = {Peer review, Authorship, Autoría, Fraud, Fraude, Mala conducta científica, Plagiarism, Plagio, Revisión por pares, Scientific misconduct},
	pages = {356--360},
}

@misc{ceviren_ceviren_logan_ehe_forum_2022pdf_2022,
	type = {presentation},
	title = {Ceviren\_Logan\_EHE\_Forum\_2022.pdf},
	url = {https://figshare.com/articles/presentation/Ceviren_Logan_EHE_Forum_2022_pdf/19514368/1},
	abstract = {Presentation at the 2022 EHE Research Forum, College of Education and Human Ecology, The Ohio State University. Data management is crucial to any research involving quantitative data. However, little is known about how data management activities are accomplished and the extent to which researchers have received training in these areas within the education field. To fill this gap, this study describes the data management activities of education researchers.},
	language = {en},
	urldate = {2022-09-15},
	author = {Ceviren, A. Busra and Logan, Jessica},
	month = apr,
	year = {2022},
	doi = {10.6084/m9.figshare.19514368.v1},
}

@article{borghi_data_2021,
	title = {Data management and sharing: {Practices} and perceptions of psychology researchers},
	volume = {16},
	issn = {1932-6203},
	shorttitle = {Data management and sharing},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0252047},
	doi = {10.1371/journal.pone.0252047},
	abstract = {Research data is increasingly viewed as an important scholarly output. While a growing body of studies have investigated researcher practices and perceptions related to data sharing, information about data-related practices throughout the research process (including data collection and analysis) remains largely anecdotal. Building on our previous study of data practices in neuroimaging research, we conducted a survey of data management practices in the field of psychology. Our survey included questions about the type(s) of data collected, the tools used for data analysis, practices related to data organization, maintaining documentation, backup procedures, and long-term archiving of research materials. Our results demonstrate the complexity of managing and sharing data in psychology. Data is collected in multifarious forms from human participants, analyzed using a range of software tools, and archived in formats that may become obsolete. As individuals, our participants demonstrated relatively good data management practices, however they also indicated that there was little standardization within their research group. Participants generally indicated that they were willing to change their current practices in light of new technologies, opportunities, or requirements.},
	language = {en},
	number = {5},
	urldate = {2022-09-15},
	journal = {PLOS ONE},
	author = {Borghi, John A. and Gulick, Ana E. Van},
	month = may,
	year = {2021},
	keywords = {Clinical psychology, Cognitive psychology, Data management, Developmental psychology, Neuroimaging, Open science, Psychology, Software tools},
	pages = {e0252047},
}

@book{briney_data_2015,
	address = {Exeter, UK},
	series = {Research skills series},
	title = {Data management for researchers: organize, maintain and share your data for research success},
	isbn = {978-1-78427-011-7 978-1-78427-012-4},
	shorttitle = {Data management for researchers},
	abstract = {A comprehensive guide for scientific researchers providing everything they need to know about data management and how to organize, document, use and reuse their data.--},
	publisher = {Pelagic Publishing},
	author = {Briney, Kristin},
	year = {2015},
	note = {OCLC: ocn921133380},
	keywords = {Data sets, Electronic data processing, Information services, Information storage and retrieval systems, Management, Research},
}

@book{hubbard_data_2017,
	title = {Data {Cleaning} in {Mathematics} {Education} {Research}: {The} {Overlooked} {Methodological} {Step}},
	shorttitle = {Data {Cleaning} in {Mathematics} {Education} {Research}},
	url = {https://eric.ed.gov/?id=ED583982},
	abstract = {The results of educational research studies are only as accurate as the data used to produce them. Drawing on experiences conducting large-scale efficacy studies of classroom-based algebra interventions for community college and middle school students, I am developing practice-based data cleaning procedures to support scholars in conducting rigorous research. The paper identifies common sources of data errors in mathematics education research and offers a framework and related data cleaning process designed to address these errors. [This paper was published in: Weinberg, A., Rasmussen, C., Rabin, J., Wawro, M., and Brown, S. (Eds.), "Proceedings of the 20th Annual Conference on Research in Undergraduate Mathematics Education," p129-140. San Diego, CA (2017).]},
	language = {en},
	urldate = {2022-09-15},
	author = {Hubbard, Aleata},
	year = {2017},
	keywords = {Algebra, Barriers, Data Analysis, Data Collection, Educational Research, Efficiency, Error Correction, Error Patterns, Intervention, Journal Writing, Mathematics Education, Mathematics Tests, Middle School Students, Research Methodology, Student Attitudes, Student Surveys, Two Year College Students},
}

@misc{noauthor_presentation_nodate,
	title = {Presentation: {Pre}-{Registration}: {What} and {Why}},
	shorttitle = {Presentation},
	url = {https://www.acf.hhs.gov/opre/training-technical-assistance/presentation-methods-video-pre-registration-what-and-why},
	abstract = {In this video, Dr. Katherine Corker of Grand Valley State University defines pre-registration and outlines how and why to pre-register a study. Dr. Corker gave this presentation at OPRE’s 2019 Methods Meeting, Methods for Promoting Open Science in Social Policy Research.},
	language = {en},
	urldate = {2022-09-07},
	keywords = {preregistration, webinar},
}

@misc{noauthor_preregistration_nodate,
	title = {Preregistration {APA} {Guide}},
	url = {https://www.apa.org/pubs/journals/resources/preregistration},
	abstract = {Preregistration allows researchers to specify and share the details of their research in a public registry before conducting the study.},
	language = {en},
	urldate = {2022-09-07},
	journal = {https://www.apa.org},
	keywords = {preregistration, guide},
}

@article{cook_preregistration_2022,
	title = {Preregistration of {Randomized} {Controlled} {Trials}},
	issn = {1049-7315, 1552-7581},
	url = {http://journals.sagepub.com/doi/10.1177/10497315221121117},
	doi = {10.1177/10497315221121117},
	abstract = {Randomized controlled trials (RCTs) are designed to answer causal questions with internal validity. However, threats to internal validity exist for even well-designed RCTs. In this article, we focus on how preregistration can help address some specific threats to internal validity related to the reporting of results. Preregistration involves researchers publicly posting critical decision points in a study prior to conducting it for the purpose of making researcher plans transparent, making deviations from those plans discoverable, and improving the validity of tests of significance. We provide a brief overview of null-hypothesis significance testing; consider how questionable research practices (e.g., p-hacking) and conducting data-dependent analysis threaten the validity of significance tests; discuss how preregistration can help address these threats and how preregistration works for RCTs; note limitations and challenges to preregistration; and provide recommendations for increasing the use of preregistration by researchers conducting RCTs in social work, education, and related fields.},
	language = {en},
	urldate = {2022-09-07},
	journal = {Research on Social Work Practice},
	author = {Cook, Bryan G. and Wong, Vivian C. and Fleming, Jesse I. and Solari, Emily J.},
	month = aug,
	year = {2022},
	keywords = {preregistration, license, paper},
	pages = {104973152211211},
}

@misc{noauthor_making_nodate,
	title = {Making {Science} {More} {Open}},
	url = {https://slu-dss.github.io/open-science-framework/#1},
	urldate = {2022-09-07},
	keywords = {pre-prints, doi, open science, replication, slides},
}

@misc{noauthor_open_nodate,
	title = {Open {Science}},
	url = {https://osf.io/9f7vb/},
	urldate = {2022-09-07},
	keywords = {preregistration, open access, slides, FAIR, osf},
}

@misc{noauthor_preregistering_nodate,
	title = {Preregistering {Your} {Study}-3},
	url = {https://osf.io/5vsp3/},
	urldate = {2022-09-07},
	keywords = {preregistration, slides},
}

@misc{noauthor_preregistering_nodate-1,
	title = {Preregistering {Your} {Study}-2},
	url = {https://osf.io/fqhdy/},
	urldate = {2022-09-07},
	keywords = {preregistration, slides},
}

@misc{noauthor_pre-registering_nodate,
	title = {Pre-registering {Your} {Study}},
	url = {https://osf.io/f8v93/},
	urldate = {2022-09-07},
	keywords = {preregistration, slides},
}

@misc{noauthor_osf_nodate,
	title = {{OSF} 101: {Supporting} {Solutions} {Across} the {Open} {Research} {Lifecycle}},
	shorttitle = {{OSF} 101},
	url = {https://www.youtube.com/watch?v=-UNZ6-a1oJ0},
	abstract = {The Open Science Framework (OSF) is a free, open-source platform that helps researchers openly and transparently collaborate and share their work throughout ...},
	language = {en},
	urldate = {2022-09-07},
	keywords = {webinar, open science, osf},
}

@article{kathawalla_easing_2021,
	title = {Easing {Into} {Open} {Science}: {A} {Guide} for {Graduate} {Students} and {Their} {Advisors}},
	volume = {7},
	issn = {2474-7394},
	shorttitle = {Easing {Into} {Open} {Science}},
	url = {https://online.ucpress.edu/collabra/article/doi/10.1525/collabra.18684/115927/Easing-Into-Open-Science-A-Guide-for-Graduate},
	doi = {10.1525/collabra.18684},
	abstract = {This article provides a roadmap to assist graduate students and their advisors to engage in open science practices. We suggest eight open science practices that novice graduate students could begin adopting today. The topics we cover include journal clubs, project workflow, preprints, reproducible code, data sharing, transparent writing, preregistration, and registered reports. To address concerns about not knowing how to engage in open science practices, we provide a difficulty rating of each behavior (easy, medium, difficult), present them in order of suggested adoption, and follow the format of what, why, how, and worries. We give graduate students ideas on how to approach conversations with their advisors/collaborators, ideas on how to integrate open science practices within the graduate school framework, and specific resources on how to engage with each behavior. We emphasize that engaging in open science behaviors need not be an all or nothing approach, but rather graduate students can engage with any number of the behaviors outlined.},
	language = {en},
	number = {1},
	urldate = {2022-09-07},
	journal = {Collabra: Psychology},
	author = {Kathawalla, Ummul-Kiram and Silverstein, Priya and Syed, Moin},
	month = jan,
	year = {2021},
	keywords = {preregistration, paper, pre-prints, open science, data sharing, registered report, reproducibility, workflow},
	pages = {18684},
}

@article{wolen_osfr_2020,
	title = {osfr: {An} {R} {Interface} to the {Open} {Science} {Framework}},
	volume = {5},
	issn = {2475-9066},
	shorttitle = {osfr},
	url = {https://joss.theoj.org/papers/10.21105/joss.02071},
	doi = {10.21105/joss.02071},
	abstract = {Wolen et al., (2020). osfr: An R Interface to the Open Science Framework. Journal of Open Source Software, 5(46), 2071, https://doi.org/10.21105/joss.02071},
	language = {en},
	number = {46},
	urldate = {2022-09-07},
	journal = {Journal of Open Source Software},
	author = {Wolen, Aaron R. and Hartgerink, Chris H. j and Hafen, Ryan and Richards, Brian G. and Soderberg, Courtney K. and York, Timothy P.},
	month = feb,
	year = {2020},
	keywords = {osf, api, documentation, package, r},
	pages = {2071},
}

@article{noauthor_osf_nodate-1,
	title = {{OSF} {\textbar} {Julia} {Strand}},
	url = {https://osf.io/bq92g/},
	abstract = {Hosted on the Open Science Framework},
	language = {en},
	urldate = {2022-09-07},
	keywords = {osf, repository},
}

@misc{noauthor_research_nodate,
	title = {Research {Rigor} and {Reproducbility}},
	url = {https://osf.io/x9pz2/},
	urldate = {2022-09-07},
	keywords = {open science, replication, slides, data sharing, reproducibility, documentation, dmp, life cycle, orcid, rdm definition},
}

@article{van_dijk_open_2021,
	title = {Open {Science} in {Education} {Sciences}},
	volume = {54},
	issn = {0022-2194, 1538-4780},
	url = {http://journals.sagepub.com/doi/10.1177/0022219420945267},
	doi = {10.1177/0022219420945267},
	abstract = {The Open Science movement has gained considerable traction in the last decade. The Open Science movement tries to increase trust in research results and open the access to all elements of a research project to the public. Central to these goals, Open Science has promoted five critical tenets: Open Data, Open Analysis, Open Materials, Preregistration, and Open Access. All Open Science elements can be thought of as extensions to the traditional way of achieving openness in science, which has been scientific publication of research outcomes in journals or books. Open Science in education sciences, however, has the potential to be much more than a safeguard against questionable research. Open Science in education science provides opportunities to (a) increase the transparency and therefore replicability of research and (b) develop and answer research questions about individuals with learning disabilities and learning difficulties that were previously impossible to answer due to complexities in data analysis methods. We will provide overviews of the main tenets of Open Science (i.e., Open Data, Open Analysis, Open Materials, Preregistration, and Open Access), show how they are in line with grant funding agencies’ expectations for rigorous research processes, and present resources on best practices for each of the tenets.},
	language = {en},
	number = {2},
	urldate = {2022-09-02},
	journal = {Journal of Learning Disabilities},
	author = {van Dijk, Wilhelmina and Schatschneider, Christopher and Hart, Sara A.},
	month = mar,
	year = {2021},
	keywords = {open science, replication, education},
	pages = {139--152},
}

@article{briney_foundational_2020,
	title = {Foundational {Practices} of {Research} {Data} {Management}},
	volume = {6},
	issn = {2367-7163},
	url = {https://riojournal.com/article/56508/},
	doi = {10.3897/rio.6.e56508},
	abstract = {The importance of research data has grown as researchers across disciplines seek to ensure reproducibility, facilitate data reuse, and acknowledge data as a valuable scholarly commodity. Researchers are under increasing pressure to share their data for validation and reuse. Adopting good data management practices allows researchers to efficiently locate their data, understand it, and use it throughout all of the stages of a project and in the future. Additionally, good data management can streamline data analysis, visualization, and reporting, thus making publication less stressful and time-consuming. By implementing foundational practices of data management, researchers set themselves up for success by formalizing processes and reducing common errors in data handling, which can free up more time for research. This paper provides an introduction to best practices for managing all types of data.},
	urldate = {2022-09-02},
	journal = {Research Ideas and Outcomes},
	author = {Briney, Kristin and Coates, Heather and Goben, Abigail},
	month = jul,
	year = {2020},
	keywords = {data sharing, workflow, documentation, roles, storage, style guide, versioning},
	pages = {e56508},
}

@article{borghi_promoting_2022,
	title = {Promoting {Open} {Science} {Through} {Research} {Data} {Management}},
	url = {https://hdsr.mitpress.mit.edu/pub/72kcw990},
	doi = {10.1162/99608f92.9497f68e},
	language = {en},
	urldate = {2022-09-02},
	journal = {Harvard Data Science Review},
	author = {Borghi, John and Van Gulick, Ana},
	month = jul,
	year = {2022},
	keywords = {open science, workflow, rdm definition, why rdm},
}

@article{logan_data_2021,
	title = {Data {Sharing} in {Education} {Science}},
	volume = {7},
	issn = {2332-8584, 2332-8584},
	url = {http://journals.sagepub.com/doi/10.1177/23328584211006475},
	doi = {10.1177/23328584211006475},
	abstract = {Many research agencies are now requiring that data collected as part of funded projects be shared. However, the practice of data sharing in education sciences has lagged these funder requirements. We assert that this is likely because researchers generally have not been made aware of these requirements and of the benefits of data sharing. Furthermore, data sharing is usually not a part of formal training, so many researchers may be unaware of how to properly share their data. Finally, the research culture in education science is often filled with concerns regarding the sharing of data. In this article, we address each of these areas, discussing the wide range of benefits of data sharing, the many ways by which data can be shared; provide a step by step guide to start sharing data; and respond to common concerns.},
	language = {en},
	urldate = {2022-08-24},
	journal = {AERA Open},
	author = {Logan, Jessica A. R. and Hart, Sara A. and Schatschneider, Christopher},
	month = jan,
	year = {2021},
	keywords = {FAIR, data sharing, education, consent, de-identification},
	pages = {233285842110064},
}

@article{eisenstein_pursuit_2022,
	title = {In pursuit of data immortality},
	volume = {604},
	copyright = {2022 Nature},
	url = {https://www.nature.com/articles/d41586-022-00929-3},
	doi = {10.1038/d41586-022-00929-3},
	abstract = {Data sharing can save important scientific work from extinction, but only if researchers take care to ensure that resources are easy to find and reuse.},
	language = {en},
	number = {7904},
	urldate = {2022-08-24},
	journal = {Nature},
	author = {Eisenstein, Michael},
	month = apr,
	year = {2022},
	keywords = {FAIR},
	pages = {207--208},
}

@article{makel_both_2021,
	title = {Both {Questionable} and {Open} {Research} {Practices} {Are} {Prevalent} in {Education} {Research}},
	volume = {50},
	issn = {0013-189X, 1935-102X},
	url = {http://journals.sagepub.com/doi/10.3102/0013189X211001356},
	doi = {10.3102/0013189X211001356},
	abstract = {Concerns about the conduct of research are pervasive in many fields, including education. In this preregistered study, we replicated and extended previous studies from other fields by asking education researchers about 10 questionable research practices and five open research practices. We asked them to estimate the prevalence of the practices in the field, to self-report their own use of such practices, and to estimate the appropriateness of these behaviors in education research. We made predictions under four umbrella categories: comparison to psychology, geographic location, career stage, and quantitative orientation. Broadly, our results suggest that both questionable and open research practices are used by many education researchers. This baseline information will be useful as education researchers seek to understand existing social norms and grapple with whether and how to improve research practices.},
	language = {en},
	number = {8},
	urldate = {2022-08-24},
	journal = {Educational Researcher},
	author = {Makel, Matthew C. and Hodges, Jaret and Cook, Bryan G. and Plucker, Jonathan A.},
	month = nov,
	year = {2021},
	keywords = {why rdm},
	pages = {493--504},
}

@article{pasek_education_2019,
	title = {Education {Needs} in {Research} {Data} {Management} for {Science}-{Based} {Disciplines}: {Self}-{Assessment} {Surveys} of {Graduate} {Students} and {Faculty} at {Two} {Public} {Universities}},
	issn = {1092-1206},
	shorttitle = {Education {Needs} in {Research} {Data} {Management} for {Science}-{Based} {Disciplines}},
	url = {https://journals.library.ualberta.ca/istl/index.php/istl/article/view/12},
	doi = {10.29173/istl12},
	abstract = {Research data management is a prominent and evolving consideration for the academic community, especially in scientific disciplines. This research study surveyed 131 graduate students and 79 faculty members in the sciences at two public doctoral universities to determine the importance, knowledge, and interest levels around research data management training and education. The authors adapted 12 competencies for measurement in the study. Graduate students and faculty ranked the following areas most important among the 12 competencies: ethics and attribution, data visualization, and quality assurance. Graduate students indicated they were least knowledgeable and skilled in data curation and re-use, metadata and data description, data conversion and interoperability, and data preservation. Their responses generally matched the perceptions of faculty. The study also examined how graduate students learn research data management, and how faculty perceive that their students learn research data management. Results showed that graduate students utilize self-learning most often and that faculty may be less influential in research data management education than they perceive. Responses for graduate students between the two institutions were not statistically different, except in the area of perceived deficiencies in data visualization competency.},
	number = {92},
	urldate = {2022-08-24},
	journal = {Issues in Science and Technology Librarianship},
	author = {Pasek, Judith E and Mayer, Jennifer},
	month = jul,
	year = {2019},
	keywords = {why rdm},
}

@article{kovacs_role_2021,
	title = {The {Role} of {Human} {Fallibility} in {Psychological} {Research}: {A} {Survey} of {Mistakes} in {Data} {Management}},
	volume = {4},
	issn = {2515-2459, 2515-2467},
	shorttitle = {The {Role} of {Human} {Fallibility} in {Psychological} {Research}},
	url = {http://journals.sagepub.com/doi/10.1177/25152459211045930},
	doi = {10.1177/25152459211045930},
	abstract = {Errors are an inevitable consequence of human fallibility, and researchers are no exception. Most researchers can recall major frustrations or serious time delays due to human errors while collecting, analyzing, or reporting data. The present study is an exploration of mistakes made during the data-management process in psychological research. We surveyed 488 researchers regarding the type, frequency, seriousness, and outcome of mistakes that have occurred in their research team during the last 5 years. The majority of respondents suggested that mistakes occurred with very low or low frequency. Most respondents reported that the most frequent mistakes led to insignificant or minor consequences, such as time loss or frustration. The most serious mistakes caused insignificant or minor consequences for about a third of respondents, moderate consequences for almost half of respondents, and major or extreme consequences for about one fifth of respondents. The most frequently reported types of mistakes were ambiguous naming/defining of data, version control error, and wrong data processing/analysis. Most mistakes were reportedly due to poor project preparation or management and/or personal difficulties (physical or cognitive constraints). With these initial exploratory findings, we do not aim to provide a description representative for psychological scientists but, rather, to lay the groundwork for a systematic investigation of human fallibility in research data management and the development of solutions to reduce errors and mitigate their impact.},
	language = {en},
	number = {4},
	urldate = {2022-09-15},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Kovacs, Marton and Hoekstra, Rink and Aczel, Balazs},
	month = oct,
	year = {2021},
	pages = {251524592110459},
}

@article{butters_recognizing_2020,
	title = {Recognizing, reporting and reducing the data curation debt of cohort studies},
	volume = {49},
	issn = {0300-5771, 1464-3685},
	url = {https://academic.oup.com/ije/article/49/4/1067/5866677},
	doi = {10.1093/ije/dyaa087},
	abstract = {Abstract 
            Good data curation is integral to cohort studies, but it is not always done to a level necessary to ensure the longevity of the data a study holds. In this opinion paper, we introduce the concept of data curation debt—the data curation equivalent to the software engineering principle of technical debt. Using the context of UK cohort studies, we define data curation debt—describing examples and their potential impact. We highlight that accruing this debt can make it more difficult to use the data in the future. Additionally, the long-running nature of cohort studies means that interest is accrued on this debt and compounded over time—increasing the impact a debt could have on a study and its stakeholders. Primary causes of data curation debt are discussed across three categories: longevity of hardware, software and data formats; funding; and skills shortages. Based on cross-domain best practice, strategies to reduce the debt and preventive measures are proposed—with importance given to the recognition and transparent reporting of data curation debt. Describing the debt in this way, we encapsulate a multi-faceted issue in simple terms understandable by all cohort study stakeholders. Data curation debt is not only confined to the UK, but is an issue the international community must be aware of and address. This paper aims to stimulate a discussion between cohort studies and their stakeholders on how to address the issue of data curation debt. If data curation debt is left unchecked it could become impossible to use highly valued cohort study data, and ultimately represents an existential risk to studies themselves.},
	language = {en},
	number = {4},
	urldate = {2022-09-15},
	journal = {International Journal of Epidemiology},
	author = {Butters, Oliver W and Wilson, Rebecca C and Burton, Paul R},
	month = aug,
	year = {2020},
	pages = {1067--1074},
}

@misc{cowles_research_nodate,
	title = {Research {Guides}: {Research} {Data} {Management} at {Princeton}: {Home}},
	copyright = {Copyright Princeton University 2022},
	shorttitle = {Research {Guides}},
	url = {https://libguides.princeton.edu/c.php?g=102546&p=665862},
	abstract = {An overview of best practices for managing research data},
	language = {en},
	urldate = {2022-09-15},
	author = {Cowles, Wind},
}

@article{alston_beginners_2021,
	title = {A {Beginner}'s {Guide} to {Conducting} {Reproducible} {Research}},
	volume = {102},
	issn = {0012-9623, 2327-6096},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/bes2.1801},
	doi = {10.1002/bes2.1801},
	language = {en},
	number = {2},
	urldate = {2022-09-15},
	journal = {The Bulletin of the Ecological Society of America},
	author = {Alston, Jesse M. and Rick, Jessica A.},
	month = apr,
	year = {2021},
}

@misc{the_white_house_executive_2013,
	title = {Executive {Order} -- {Making} {Open} and {Machine} {Readable} the {New} {Default} for {Government} {Information}},
	url = {https://obamawhitehouse.archives.gov/the-press-office/2013/05/09/executive-order-making-open-and-machine-readable-new-default-government-},
	abstract = {EXECUTIVE ORDER - - - - - - - MAKING OPEN AND MACHINE READABLE THE NEW DEFAULT FOR GOVERNMENT INFORMATION},
	language = {en},
	urldate = {2022-09-15},
	journal = {whitehouse.gov},
	author = {The White House},
	month = may,
	year = {2013},
}

@misc{office_of_science_and_technology_policy_ostp_2022,
	title = {{OSTP} {Issues} {Guidance} to {Make} {Federally} {Funded} {Research} {Freely} {Available} {Without} {Delay}},
	url = {https://www.whitehouse.gov/ostp/news-updates/2022/08/25/ostp-issues-guidance-to-make-federally-funded-research-freely-available-without-delay/},
	abstract = {Today, the White House Office of Science and Technology Policy (OSTP) updated U.S. policy guidance to make the results of taxpayer-supported research immediately available to the American public at no cost. In a memorandum to federal departments and agencies, Dr. Alondra Nelson, the head of OSTP, delivered guidance for agencies to update their public access…},
	language = {en-US},
	urldate = {2022-09-15},
	journal = {The White House},
	author = {Office of Science {and} Technology Policy},
	year = {2022},
}

@article{tenopir_data_2016-1,
	title = {Data {Management} {Education} from the {Perspective} of {Science} {Educators}},
	volume = {11},
	copyright = {Copyright (c)},
	issn = {1746-8256},
	url = {http://www.ijdc.net/article/view/11.1.232},
	doi = {10.2218/ijdc.v11i1.389},
	abstract = {In order to better understand the current state of data management education in multiple fields of science, this study surveyed scientists, including information scientists, about their data management education practices, including at what levels they are teaching data management, which topics they covering, and what barriers they experience in teaching these topics. We found that a handful of scientists are teaching data management in undergraduate, graduate, and other types of courses, as well as outside of classroom settings. Commonly taught data management topics included quality control, protecting data, and management planning. However, few instructors felt they were covering data management topics thoroughly, and respondents cited barriers such as lack of time, lack of necessary expertise, and lack of information for teaching data management. We offer some potential explanations for the existing state of data management education and suggest areas for further research.},
	language = {en},
	number = {1},
	urldate = {2022-09-15},
	journal = {International Journal of Digital Curation},
	author = {Tenopir, Carol and Allard, Suzie and Sinha, Priyanki and Pollock, Danielle and Newman, Jess and Dalton, Elizabeth and Frame, Mike and Baird, Lynn},
	month = oct,
	year = {2016},
	keywords = {curation, DCC, digital curation, digital preservation, IJDC, International Journal of Digital Curation, preservation},
	pages = {232--251},
}

@misc{noauthor_psych-ds_nodate,
	title = {Psych-{DS} {Specification}},
	url = {https://docs.google.com/document/d/1u8o5jnWk0Iqp_J06PTu5NjBfVsdoPbBhstht6W0fFp0/edit?usp=embed_facebook},
	abstract = {Psych-DS  A technical specification for psychological datasets  Version 0.4.0 (“MAY 2022” DRAFT) - IN PROGRESS  Available under the CC-BY 4.0 International license Based on the BIDS specification for fMRI data, with inspiration from the BIDS Eyetracking extension draft.  Welcome! If this is y...},
	language = {en},
	urldate = {2022-09-16},
	journal = {Google Docs},
}

@misc{reynolds_basics_2022,
	title = {The {Basics} of {Data} {Management}},
	url = {https://figshare.com/articles/preprint/The_Basics_of_Data_Management/13215350/2},
	doi = {10.6084/m9.figshare.13215350.v2},
	abstract = {Data sharing starts with good data management. See this document for some general best tips for good data management practices.},
	language = {en},
	urldate = {2022-09-16},
	publisher = {figshare},
	author = {Reynolds, Tara and Schatschneider, Christopher and Logan, Jessica},
	month = apr,
	year = {2022},
}

@misc{ldbase_general_nodate,
	title = {General {Data} {Management} {Guidelines} {\textbar} {LDbase}},
	url = {https://www.ldbase.org/resources/best-practices/general-data-management-guidelines},
	abstract = {Data sharing starts with good data management. See this document for some general best tips for good data management practices. This work is built from a repository of experience. This work speaks only to quantitative data, and is specifically targeted to research data collection in schools. The Basics of Data Management Paper by Reynolds, Schatschneider \& Logan, 2022, available under a CC By 4.0 license. Planning ahead and implementing proper data management will save you time, resources and many headaches. You will get high-quality data that are organized, labeled and accurate. It will also allow your data to be more easily shared, understood and reused by others. Good data management will result in wider dissemination of your work and more citations. Your data must be managed, though, throughout the life cycle of your research: from planning your project to archiving your data. We will go through four crucial steps that will help you get there: Planning, Data Documentation, Data Entry, and Cleaning Data. PLANNING A DATABASE When planning and creating a database for your research, you must consider the data-collection surveys and assessments you are using and also be able to visualize how this data should be stored and structured into tables for analyses. This process is multifaceted and can and should start as soon as possible in the timeline of your research project. Databases. When planning a study, determine first how many different tables/spreadsheets/datasets you will need in order to capture all of the information you will be collecting. If collecting information about children, teachers, and schools, for example, you may need three datasets: one for each type of participant. Also think about how they will relate to one another and how that will be stored from table to table. In a complex project, you may also consider keeping one separate file with demographic information only (student IDs, birthdates, socio-economic status, gender identity). These are pieces of information that you do not expect to change during the course of the project, and that you do not want to store more than one time. Identifiers. In any study, each participant will need an identification or ID. This ID will be linked to that individual’s data throughout the project. The ID is crucial to ensure that statistical analyses such as correlations or regressions can be used to examine how different responses on measures covary across participants. The ID variable is the most critical variable in your dataset. To ensure that IDs are consistently and appropriately applied, develop a set of rules to govern your ID-creation. There are three critical rules to follow when developing an ID system. First, every individual must have their own unique ID; second, the ID never changes; and third, the ID is always associated with the same person. The simplest way to create an ID scheme is to give the first individual recruited the ID number 1 (or 100, or 1000) and then count up. It is possible for the ID to include some information (e.g., the year of recruitment, the site of recruitment), however these can make ID rules more difficult to apply. Further, IDs should never include any identifying information (e.g., assigned treatment condition, name of a school). They also must never include any information that may change over time (e.g., the age of the child, the month of data collection). Instead, create additional variables that represent the information you want to store about the data collection. In addition to these rules, we also suggest that the ID variable should always be formatted as a character variable, even if you are only using numbers in the ID, because many statistical programs try to change numbers to dates and often drop leading zeros. In sum, the key for IDs is to never change your numbering system, never change an ID once it has been assigned, never recycle an ID even if a participant drops from the study, and never have a way for a participant to have two different IDs. Variable Names. All datasets contain variables. Eventually, you (or someone) will write code, using these variables to run your data analyses. You will want to create a variable naming standard, and you should be consistent with this standard throughout your project. There are many ways to do this. Some use the “One-up numbers” system, naming variables from V1 through Vn until all variables are named. Although this is consistent and clean, it provides no indication of what data is held in the variable and leaves more room for errors during data cleanup and analyses. Some use the “Question number” for naming variables, for example, Q1, Q2, Q2a, Q2b, etc., connecting the variable to the original questionnaire. This presents the same problems as the One-up method and gets confusing when one Question contains many parts/responses. We have also seen some questionnaires that contain two Question \#3’s! So, it can get sketchy. We tend to use a naming system of 7 characters: the first two characters contain letters that represent the assessment name; the next three represent the subtest (if applicable) then the next two characters represent what type of score it is (RS for raw score, SS for standard score etc.). If you need to number item level data, always put the number at the end of variable name. This system was developed in ancient times when analytic programs required variable names to be 8 characters or less, but we have found that keeping the variable names short and predictable makes data analysis much easier. If you are creating new variable names like this from scratch, run it by a few people first. Make sure that the abbreviation system makes sense to them as well. Another more systematic approach adds prefixes and suffixes onto roots that represent a general category. For example, your root might be MED representing medical history. MOMED might represent the mother’s medical history and FAMED might represent the father’s medical history. Choosing a prefix, root, suffix system requires a lot of planning before setting up your database to establish a list of standard two- or three-letter abbreviations. Always, make sure you keep the naming consistent in your longitudinal projects. Some further tips on creating variables: All variable names must be unique The length of the variable must follow the requirements of the data entry and data analysis software packages you have chosen. Many programs do not allow the length to exceed 64 characters. As a rule, shorter is generally better; around 8 and not exceeding 12 characters is a good length to aim for. Most programs do not allow you to begin variable names with a digit. Begin with a letter. Should not end with a period Should not contain spaces Should not contain special characters (!, ?, \_) Avoid naming variables using keywords that are used during statistical analyses like ALL, AND, BY, EQ, GE, GT, LE, LT, NE, NOT, OR, TO, WITH. For item level data, always put the number of the item LAST. This is helpful in data analyses if you are running a sum on 300 variables, you can just say item1-item300 rather than typing out all 300 variable names. In addition to naming them, variables have two other main properties: length and type (char, var, date, etc.) Be consistent with the length and type of variables across datasets. Otherwise, merging datasets may become a nightmare! Once you have your naming system in place, you will want to name and identify all of the variables you will collect. A helpful way to do this is to take a blank assessment packet, highlight every field you want to capture in data entry, and then write the desired variable names on it, along with their length and type. Variable Labels. Many data entry and statistical programs allow you to add a text description to the variable name that describes the content of that variable. This is called a Variable Label. For example, it might contain the item or question number of your data collection instrument, the word being assessed in a vocabulary test, or if it is a calculation field, the way that this number was calculated. The length of a label might be limited, so plan ahead and stay consistent with your labeling method across the project. Variable Values. When translating text to data, it is important to really consider how responses will be recorded (their value) and analyzed from a data standpoint. Also consider the language used in a question and how that ties to your variable name. For example, “Check here if your child does not have siblings.” Typically, people looking at data consider a “checked” box is stored as a “Yes” response or a “1” in data. So, if your variable name is “siblings” and the value is “1” many people will assume ‘yes, this child has siblings’ (which is not the case). You might want to change the question to say, “Check if your child has siblings.” Then a checked value creates siblings =1 which means it is ‘true’ and they do have siblings. If it makes sense on the form to “check” for not having a specific value or the papers have already been designed, then maybe consider naming your variable “nosiblings” (then the check mark can still=1 and be true). When responses to multiple items contain the same answer set (e.g., Strongly Agree, Slightly Agree…) keep these variable values consistent across the project. This provides cleaner coding when you get to the data analysis stage. Tips on creating variable values: Usually 1 = Yes, TRUE, correct or present Usually 0 = No, FALSE, incorrect or absent If Sex or Gender is collected as either “male” or “female,” then instead of using “sex” or “gender” as the name of the variable, use “Male” (or “Female”). Then make the 1’s represent that variable as being present. So, if you named the variable “Male” then make 1 represent males. With multiple response items and scales, the variable value in the dataset should contain the same value as the assessment responses to minimize confusion. For example, a multiple-choice question has response options that are numbered 1-5; you will want to store that as 1-5 in your dataset as well. This will allow someone looking at the paper form and also looking at a dataset variable value to understand what is stored there. So, if you want an answer of “none” to be a “0” in your dataset, it should be 0 on the paper also (as people using your data will likely refer to the paper assessment also). Otherwise, you have a paper score of a “2” circled, yet you are storing it as a “1” in your database. Stay consistent with scoring your scaled and multiple-choice options. Do you like starting scales at 0 or 1? When coding grouped responses (similar from question to question) keep the values that represent the response the same. Ambiguity will cause coding difficulties and problems with the interpretation of the data later on. Avoid open-ended text responses when possible. Try to make participants select a predetermined response. When you let participants write/type a response you will get a wide variety of answers for something that should be scored exactly the same way. For example, grade: they could type in 1, 1st, First, Elementary, Early Education, or even have a typo. Make them select or circle “1st Grade”, and then store that value as the numeric value of “1”, not the text. Missing Data. You will need to decide how to handle missing data for every variable in your potential dataset. Missing data could mean a variety of situations, such as the participant did not know the answer, they refused to answer, the question was accidentally skipped, they were unable to answer, you cannot read the assessor’s handwriting, there was an error made by the interviewer and not the participant, they answered outside of the range of allowable responses, the question was not applicable, they were supposed to skip the question and it actually should be blank, etc. For some projects, you may want to record in your database why the data was missing. One way this can be done is by adding additional variables to the database. However, this can quickly become cumbersome when multiple measures are given. Instead, you may choose to represent missingness with a specific value, with different values representing different missing value codes for the variable in question (see “variable values” section above). Some programs allow for different system-missing values, and you might choose to use that. If you do choose to represent missingness with a selected value, never choose zero as an indicator of missing data. Whatever way you decide to represent missingness, make sure that the missingness decision is documented within your study protocols. Planning For Paper Data Collection: Tracking Participation. It is important to “roster” your study before going out for data collection. Rostering means knowing which participants will be participating in your study. This implies having signed consent forms (if required by your study’s Institutional Review Board (IRB)) and rosters usually obtained from the school. Assessment Administration as Packets. Some research designs require multiple standardized assessments to be administered by a trained professional assessor (project staff). When collecting data with paper forms, assessments are typically given in a specific order (either one set order, or one of a few randomly assigned set orders). When administering measures such as these, there are several opportunities for errors to be introduced. To keep these administration errors minimal, we suggest you develop packets to give to your study participants. Packets are a collection of assessments/questionnaires that will be administered to your participants. Having the answer sheets grouped together into packets eliminates the likelihood of missing an assessment, mixing up which assessment belongs to which student, or delivering assessments in the incorrect order. All packets should have a Student ID or Packet ID pre-printed on each page of the packet. Packet IDs. We encourage people to create something we call “Packet IDs” when creating an assessment packet. Packet IDs are unique IDs associated with each assembled packet, and typically function like a count (the first packet gets ID 1 and so on). Packet IDs can either be preprinted on each page of the packet or printed on labels that can be placed on each page of the packet at any point in time. Packet IDs can then be assigned to a particular student ID (or teacher ID) when administered. Much like keeping track of the participants in your study, it is also beneficial to track the assessment packets. This has several benefits. First, the Packet ID allows the researcher to minimize the number of times a participant’s name or ID appears on the assessments, which is a requirement for some funding agencies. Second, a Packet ID allows you to easily give an assessment to a student that was not on the roster at the time you printed the assessments (i.e., have an extra stack of assessments with Packet IDs on them, and connect these new students to a specific Packet ID while in the field). Third, if a packet is ruined (have you ever spilled a coffee?), you can grab another packet and associate that Packet ID to the student’s ID. Fourth, if the first page of the assessment packet is a cover page, the link between the packet ID, students name, and students ID can be linked on this page, then torn off and kept in a secure location, which would prevent/minimize data entry personnel from knowing the student’s name. Identifying Information and Cover Sheets. We recommend creating a cover sheet to add on the front of your packet. On this, you will record the Packet ID and any sensitive information such as participant name, date of birth, ethnicity and gender, and the participant’s unique Participant ID. The information recorded here is information that will usually not change throughout the course of the study. Using the cover sheet method allows your team, upon receiving the packet from the field, to tear off the first page of the packet (this cover sheet) and record that the participant’s information has been received, while at the same time immediately separating any identifying information of the participant from their scored data. One designated person would enter this identifying information from the cover sheet into a Demographics Database. You should have one, master, demographic table/file. This is the file that will link the participant’s ID, name, and demographic info to their data. This process prevents the group of people doing data entry from accessing the names of people in the study, thereby limiting the number of people who have access to identifiable data. All remaining pages of the packet will contain labels with the participant’s unique Packet ID, allowing you to enter the full packet’s data at a later point in time in a lab where formal data entry occurs. Another good idea is to record summary data (summary scores) for each assessment on the cover sheet. The benefit of this is that you do, at least, maintain some amount of data if the packet is completely lost, or in the worst-case scenario, if your main dataset of detailed score information somehow crashes. The downside is that it introduces an additional place for errors to occur. Remember, you should always leave the second page (or back page) of the cover sheet blank so that when you rip off the page and store it separately, you will not lose any test data that was collected! Tracking Data Collection. You will want to create a process for tracking everything that needs to happen in your lab and a way to make all of these data collection steps happen properly: where do received forms go, who enters it into the Demographics Database, what happens to the cover sheet, is it kept safe in a locked drawer, where does the packet go when it needs to be data entered, where does it go after data entry, who maintains the list of who is in the study/if they have been assessed, what should you do if you receive data on a student that did not give consent. A project manager should know where the assessment packets are at all times, keeping a paper trail that can be used to check off data as it comes and goes. DATA DOCUMENTATION Protocols. A protocol is a living document that tracks all decisions made about a project, who made the decision, and why the decision was made. This type of documentation is essential to the running of the project, the ability to make and follow consistent decisions and processes across the life of a project, as well as the ability to describe what was done during the project at a later date (e.g., grant reports; manuscripts). There are several types of decisions where documenting those decisions with a protocol would be useful: Variable naming rules Inclusionary Criteria: How decisions were made about who to include in the study Recruitment procedures and specific ways you might start the communication process with your participants When and how follow-up phone calls might be needed School requests that differ from school to school or are based on small specifics of classroom rules Training of project and lab staff (particularly useful for coding schemes) Source material for any measures or scales given (include citations; measurement properties) Data entry procedures for the lab (mentioned above) How to record missing data Data cleaning steps (see data cleaning section) What team members should do when in a situation where they don’t know what to do Any other items that may be specific to your study The Codebook. The Codebook is also referred to as a Data Dictionary, Data Documentation, Metadata, Data Manual, or Technical Documentation. It is the key that tells everyone what the data in the dataset means. It will be used by all members of your team, including the person performing the data analysis. A codebook also allows others who are not familiar with your project to completely understand your data, cite it, and reuse it responsibly well into the future. At a minimum, a Codebook should contain: All variable names Variable labels (could be the exact wording of the question on the assessment) Values of the data stored in the variable (which again, when applicable, should be coded consistently across the project). Other information you might want to include in the Codebook, per variable, are: Metadata that was implied during data entry but is not shown in the dataset. This might include the fact that it was collected in one specific state; which wave it was; or that one dataset included teacher’s responses, and another included a school psychologist’s response, yet those values were not stored on the dataset. Skipping patterns (the reason certain answers were skipped by the respondent, e.g. “If your response is No, then go to Question \#10.”) Per-project-unique codes or missing data codes and exactly what they mean. A description of a variable that was calculated during or after data entry, including the code used to get that number. One way to ensure a codebook is high quality is to make a copy of each survey or form administered and superimpose the variable names and values on top of each question. This is a great way to get started. Some chose to use XML or a searchable PDF, to stay in compliance with the Data Documentation Initiative (DDI), an international metadata standard for data documentation and exchange. Tools that can help create a codebook can be found here: https://ddialliance.org/training/getting-started-new-content/create-a-codebook. Producing a high-quality codebook and keeping it updated as projects progress takes a lot of time, but it is a critical step in ensuring that your information is shared effectively, both within your team and with other researchers. It allows all members of your team to explain results and to share data in open science. DATA ENTRY Data that have been collected via paper forms must be translated to a digital database. This translational step is called data entry. Choosing The Data Entry Program. Decide which software you will use for data entry. If you have a quick and simple study, you might want to pick a user-friendly database package like SPSS, EXCEL etc. There are some of the open-source programs like JASP and JAMOVI that are increasing in popularity as well. If your project is large, you may want to create a relational database or use a pre-existing program that allows for data entry such as REDCap, SAS, FileMaker, Microsoft Access, MYSQL, or Oracle. These programs will let you define variable characteristics, create “data entry screens,” incorporate data cleaning which minimizes errors in real time, do sum-to-total checks, incorporate range restrictions, allow you to link multiple datasets with a linking variable, and lock data at the record level. All methods have their respective pros and cons. However, using spreadsheets should be avoided whenever possible. It becomes difficult for the people performing data entry to keep track of what column they are in, and also one “bad sort” can ruin an entire data set. Spreadsheets should be reserved for very small studies where if one had to re-enter all the data, it would not be devastating. Creating The Data Entry Screens. A data entry screen is the interface between the hard copy of the data and the digital database. It is a user interface, designed to minimize the cognitive load on the data enterer and therefore minimize errors during data entry. When you create data entry screens, you will want to make them look as similar to the packets as possible. This will not only save time for data entry personnel but will help them stay focused on what they are entering, rather than focusing on where they are in the program and feeling lost between the paper and the screen. Within each form, the flow of the data entry should follow the logical order of each form. A paper form with 20 items (\#1-\#20) should allow the data enterer to type in the response to the questions in the order that they appear on the form. This is called “Tab order” and dramatically improves the data form entry process. If a project contains multiple assessments, the order of the assessments on your data entry screens should follow the order of the paper packet in the data entry person’s hand. When they have finished entering one assessment, and turn the page, your “next” button on the data entry screen should advance them to the next assessment they are looking at on paper. Stay in synch with them. This helps them save time and stay focused. The data entry screen provides an excellent opportunity to prevent data entry errors. Define range limits on your data where possible (for example, the score must be a value between 0 and 30; the birthday and/or assessment date must be between certain dates). Do not allow a required field to have a null value. Do not allow entry into certain fields if the criteria of the participant should not allow them to answer that specific question. After creating the data entry program, you should sit with the data entry team and watch them enter a few forms. You will find there might be misunderstandings about the data, or that there is something you can tweak on the screen to make the process easier for them. Test it with them before going “live.” After they enter a few forms, you will also want to check the data against the participant’s paper form to make sure all data was translated correctly; and then, run the data by the PI and the Data Analyst to make sure it is what they were expecting. You will want to make sure that your department/university has a file server. This allows multiple computers to access the same data files, and it is easy to set a process that backs up your data. And most importantly, MAKE SURE YOUR DATA IS BEING BACKED UP THROUGHOUT THE ENTIRE PROCESS! Calculated fields. The data entry team may be entering item level and sum score information that is calculated by a human, in the field. You will probably want to create a system-calculated field using a formula and compare your calculation to the assessor’s calculation. This code calculation happens AFTER data entry, on the back end. (You do not want it calculated on the data entry person’s screen, as they should not be looking at calculations and comparing them during this time. They should only be doing data entry.) If the two calculated scores do not match, then send it back to the data entry team to find the problem. Quality Assurance. Ensure that you have some kind of data entry quality checks. We recommend all data get entered twice. That is, have a primary and a secondary dataset that are supposed to contain the same information. Then these two datasets should be compared for discrepancies and any data that do not match should be flagged and investigated. Many data entry programs do this step for you; otherwise, discrepancies can be reconciled either in a third dataset or through hard coding (syntax). Longitudinal Considerations. Designing a data entry program for longitudinal databases can take on many forms. We recommend creating different files/sheets/tables for different waves of data. Use the univariate format – which means use the same variable names for the data collected over multiple time points. Do not add numbers to variables to designate “wave” or end them with an F or S to represent fall and spring. Those can be added later programmatically, and the univariate layout makes data cleaning easier. In the “wave” datasets, only enter what you collected at that wave (plus ID and date of collection). Do not add anything else. Maintain a separate file with demographic information for each subject (this is the only place that a subject name can appear). All other datasets should only use IDs. Beginning the Data Entry Process in the Lab. You’ll need to set up data entry stations or a lab where people can simply sit down and enter data. The data should stay at a station and should never follow a person if they need to walk away or go ask someone a question. The project manager must maintain some kind of master list that tells her/him what is expected at the end of each wave in terms of which participants were tested. This can be created from consent forms or school rosters and can be easily maintained in EXCEL. At all times, this person should have a master list of all students and know who is in the study. This master list might be from the Demographic Database and contain contact information on the family. When a packet arrives from the field, it should be immediately checked by someone for accuracy (missing data, miscalculations, missing labels). Checking this right away can catch administration errors that can greatly impact your data. Checking the protocols at the front end will reduce significant cleaning time at the back end. If the packet looks good, immediately log it into the Demographic Database. The database should only exist in one place. It will contain the student ID, packet ID, and any other sensitive information. The benefit of doing this is the safekeeping of your data. One trusted person looks at it, as you will want as few eyes as possible seeing any sensitive, identifying information. The process will also help your team keep track of which students still need to be tested at the schools (because you have already rostered; and you know who needs to be tested) or if a form has been misplaced. Many students miss kids/a classroom, and this will help you get more of your data on time. There should be an easy system in place, where each person, without effort or documentation, knows what needs to be entered. You might want to have file cabinets clearly labeled, for example, “Needs double entry.” The data entry team should never have to figure out which packets have been entered, and which have not, by pulling up a spreadsheet. This also will prevent simple errors like a packet being entered twice. Data entry should be as thoughtless as possible. No math should be done in the middle of data entry. Scores should have been calculated in the field, but if not, perhaps have a place in your filing cabinet system for “needs scores,” before “Needs to be entered.” People should only have access to their data entry program module. If your data entry program has other studies in it, they should not be able to see or access those. Once logged in to their study, they should only have data access to the functions that are available only to their role in the project (first entry, secondary, reviewer, etc.) and no one else’s roles. Make sure your system, as defined by your research grant, is in place and being followed. The PI and Data Analyst should develop Data Entry “Rules” for ambiguous cases. The data entry team should be well trained on this, and have a written, easy Rule Book at each data entry station that they can refer to while entering data. Cases might include what to do when: the form or question has no answer, there are blanks, the respondent did not follow directions, or the respondent circled two options when only one was allowed. The key, as always, is to be consistent! When the data entry team is completely finished with their entries, they will notify the Data Manager, who will extract the data from the data entry program. Data Collection With Electronic Systems. Instead of paper assessments and a team of people entering the responses from paper into a database, many researchers may want to use electronic web-based systems to collect data, such as Qualtrics and REDCap. We have some specifics tips and tricks. Online systems for delivering surveys to participants often name variables in a confusing way. In Qualtrics for example, variable names are often Q1, Q2 etc., but the numbers relate to the order in which the questions were made, not the order in which they appear on the survey. Further, the values assigned to categorical variables are also not intuitive and can even change per question. When creating surveys, most programs will have an option to change the names and default values for the created variables. Examine options to ensure these are in place before ever administering one survey. Ensuring the variables are named how you want them and that the values assigned are consistent will save you a lot of time later. If we have caught you too late, and the survey has already been administered, then make sure you refer to your documentation about the surveys to create code to change those names post hoc. Using code will help keep transformations transparent, and repeatable, when or if you need to make the same changes again. A second thing to think about is pulling the data from the server to your computer. Often it will look very different than what you had expected. Qualtrics, for example, adds many columns to the data that you had not programmed. These include longitudes and latitudes, IP addresses and other data that could potentially identify the respondent. It is a good idea to do a couple of practice data collections with your platform and inspect the datafile for these unexpected things before starting actual collection. You can then anticipate cleaning and possibly make changes to the survey to better suit your needs. CLEANING DATA Before you start. As you clean your data, remember to always keep a trail of everything you do. Always work in a copy of the dataset, maintaining the original dataset in its raw state. Always back up your data and documentation as you work. All data management code should be saved and annotated as you go. Explain what the code is doing, what you are looking for if running a query, what kind of a check you are running, or why you are doing this (for example: “Required by IRB to …. “). Anyone who looks at your code should be able to read your annotations at every level and understand what is happening and why. They should not have to read your programming code to figure out what you were doing or what your intentions were. Example: Your assessment has a rule that the assessor records the highest item number the child answered. So, in your annotation, you might note: /*This grabs the highest item number the assessor said the child answered (for example, Question \#85) and makes sure no questions were scored beyond that (if child answered questions higher than question 85, then you want to flag it) and that all previous questions before Q85 were indeed answered.*/ Second example: You need to remove certain rows from the dataset before analyzing the data. Your annotation should explain that you are indeed deleting a participant and why: /*Child was removed from study, due to moving schools before testing began/not having enough data... */ Not only will these annotations help the next person that might be looking at your data process, but these detailed notes will also help you when you revisit your data multiple times during the study. It prevents you from overlooking decisions made about the study, and it allows you to quickly run through all pre-established requirements if you need to run the file again for some reason in the future. (Example: two more participants were added to the data entry program after the project was “over,” and all data must be re-extracted, and code must be re-run). Structure. You will now create and manage the working (aka transactional) and master datasets. Transactional datasets are datasets that contain cleaned data and any derived data such as computed sum scores, transformed variables, and potentially norm-referenced scores if they were calculated via code. To begin, structure your data into the file layouts that work for your project. You may need to join tables or rearrange your data into long or wide datasets. Get the data files organized in a way that fits your research needs. Make sure you are only dealing with the rows of data that have been double-entered and verified. Data entry programs often maintain the first entry, second entry, and finalized entry. Make sure your working dataset only contains the finalized record for that participant. The participant ID, Study ID, and any identifying variables should be located at the front of the dataset, in the first few variables. Examples of some general checks we run include: The numbers of participants you have match the number of participants given to you by the project manager Every ID that is in the table should have been assessed (ties back to master list), to make sure you have no unexpected students and/or to make sure no one is missing Eyeballing your data – surprisingly, this is quite helpful Is the variable in the correct format (char or numeric)? Sum-to-total checks Sum scores on the summary sheet match the sum scores on the score sheets Range checks Duplicate ID entries, duplicate form entries Basal and Ceiling rules Validity checks Run descriptives on key variables/calculated scores Run frequency codes just to get a fresh look at your data. You might find a value that just seems off Run Consistency Checks that are particular to your study and based on your extensive knowledge of the study. Double checking all calculated fields like “Total Scores,” whether it was entered by a human or calculated in the data entry program Are the scores expected for that particular wave and timepoint present; and vice versa, do you have scores that were not expected for that participant/timepoint Have participants been assessed at every timepoint required of them by the protocol (pretest, posttest, delayed posttest, etc.) Do the means make sense across ages/grades/timepoints Do you need to find incomplete records? You are only looking at rows that have been double entered and verified Inconsistent responses per participant. Or perhaps, has the respondent selected the same response for every item? Check project-specific protocols. There will be a lot of these. This might include things like “no one was given a PPVT assessment in the spring.” So, make sure there is no data for that. Check that all project-specific requirements have been met. Did an assessor mark that this assessment “was not administered” yet have data for it. Go back and have the data entry team research it. Ensure that the variables and their values match those in the codebook, you do not want to have a “5” response in your study that is not explained in your codebook. Sidenote: We recommend running some of these code checks during the data entry process here and there. You never know what kind of issue might pop up, and it is better to fix it earlier in the process rather than later. Fixing Problems. Create a system for tracking issues you find within the data and their resolutions. When mistakes are found, often you will pull the protocols and conduct a “hard target” search. This means retrieving the original paper assessment to make the corrections. How you make the correction might depend on the extent of the problem and how it made it to the final dataset. For example, you might be able to have the data entry person correct the error on their end. If, however, you feel like the error should be corrected via hard code within your working dataset (e.g., a birthdate was incorrect and needs to be corrected), then you will begin the process of making a “transactional dataset.” This brings us to a very important principle. CHANGES TO DATA ALWAYS FLOW FORWARD. What does that mean? That means your primary datasets should never change. Any reconciliation occurs in a separate, “transactional” dataset. Always work from a copy, and keep extra copies of clean data in other locations. You should never rewrite the original files produced during data entry. Why do we recommend this? If something goes wrong or mistakes are made in the data cleaning process, you will always have the original datasets to fall back on. Using a transactional dataset and code to correct data ensures you can track all changes made from entry to final dataset. DATA FLOWS FORWARD, NOT BACKWARD. You should always be able to go back to the originally entered data and be able to see where values got changed and why they were changed. Within your transactional dataset, you might also calculate or add values like standard scores, percentile ranks, confidence bands, and normative scores. For assessments that convert raw scores to percentile scores, you might have had a person manually add a score during data entry using a table to match each student’s criteria (age/grade/score) to the standard score and then enter that into the system during data entry. If not, however, you might have decided to instead use lookup tables to create those values. With lookup tables, you create a separate dataset with all possible scores and their corresponding percentile/standard scores. You then join each participant to this table to retrieve their score, based on the assessment criteria. It is more work on the front end, but it is more error free and works out nicely when dealing with larger amounts of data. Some assessments already have published norming tables out there that you can use. All changes to data need to happen in one data file. You should not have multiple versions. If you need to add 800 corrections or new variables, go ahead and do it, but do it all in ONE dataset. Remember to document where your data is backed up and stored. Multiple versions of final datasets ensure their safety. Documentation should include how many copies you will keep and how you will synchronize them. It is a good practice to also include security issues such as passwords, encryption, power supply or disaster backup, and virus protections. Once you have made all data management changes, you will have one master dataset that will be ready for analysis. This dataset will hold everything and can be quite large. Analysis Dataset. Datasets that can be used for analysis can also be created. These datasets are typically a subset of the variables or subjects from the master dataset. These datasets should always be created from the master datasets, but the master datasets should never be changed. Once data analysis has been completed and the manuscript sent off for review, the analysis dataset needs to be “frozen.” That is, no additional changes should be made to it, and the datasets should no longer be reconstructed from the original master datasets. This ensures that you will always be able to replicate the results of the analyses reported in any paper. SUMMARY In summary, many potential errors can be avoided by carefully planning and discussing all the different portions of the data life cycle before a data collection begins. It is a great idea to: (1.) define your data management guidelines during the design stages of your research study, (2.) take action during the study to ensure those plans are continually being revisited, applied and discussed, and (3.) complete all documentation about your data and store it with your archived data so that it can be understood and reused by others. These actions will likely fill funding agency requirements and ensure more citations for your publications. If you have any tips you’d like to share with us, feel free to email treynolds@fcrr.org.},
	urldate = {2022-09-16},
	author = {LDbase},
}

@article{neild_sharing_2022,
	title = {Sharing {Study} {Data}: {A} {Guide} for {Education} {Researchers}. ({NCEE} 2022-004)},
	url = {https://ies.ed.gov/ncee/pubs/2022004/pdf/2022004.pdf},
	journal = {U.S. Department of Education, Institute of Education Sciences, National Center for Education Evaluation and Regional Assistance.},
	author = {Neild, R.C. and Robinson, D. and Agufa, J.},
	year = {2022},
}
